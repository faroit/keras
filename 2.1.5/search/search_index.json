{
    "docs": [
        {
            "location": "/",
            "text": "Keras: The Python Deep Learning library\n\n\n\n\nYou have just found Keras.\n\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of \nTensorFlow\n, \nCNTK\n, or \nTheano\n. It was developed with a focus on enabling fast experimentation. \nBeing able to go from idea to result with the least possible delay is key to doing good research.\n\n\nUse Keras if you need a deep learning library that:\n\n\n\n\nAllows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n\n\nSupports both convolutional networks and recurrent networks, as well as combinations of the two.\n\n\nRuns seamlessly on CPU and GPU.\n\n\n\n\nRead the documentation at \nKeras.io\n.\n\n\nKeras is compatible with: \nPython 2.7-3.6\n.\n\n\n\n\nGuiding principles\n\n\n\n\n\n\nUser friendliness.\n Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n\n\n\n\n\n\nModularity.\n A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n\n\n\n\n\n\nEasy extensibility.\n New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n\n\n\n\n\n\nWork with Python\n. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n\n\n\n\n\n\n\n\nGetting started: 30 seconds to Keras\n\n\nThe core data structure of Keras is a \nmodel\n, a way to organize layers. The simplest type of model is the \nSequential\n model, a linear stack of layers. For more complex architectures, you should use the \nKeras functional API\n, which allows to build arbitrary graphs of layers.\n\n\nHere is the \nSequential\n model:\n\n\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\n\n\n\nStacking layers is as easy as \n.add()\n:\n\n\nfrom keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))\n\n\n\n\nOnce your model looks good, configure its learning process with \n.compile()\n:\n\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\n\n\n\nIf you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n\n\n\n\nYou can now iterate on your training data in batches:\n\n\n# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n\n\n\n\nAlternatively, you can feed batches to your model manually:\n\n\nmodel.train_on_batch(x_batch, y_batch)\n\n\n\n\nEvaluate your performance in one line:\n\n\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n\n\n\n\nOr generate predictions on new data:\n\n\nclasses = model.predict(x_test, batch_size=128)\n\n\n\n\nBuilding a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n\n\nFor a more in-depth tutorial about Keras, you can check out:\n\n\n\n\nGetting started with the Sequential model\n\n\nGetting started with the functional API\n\n\n\n\nIn the \nexamples folder\n of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.\n\n\n\n\nInstallation\n\n\nBefore installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.\n\n\n\n\nTensorFlow installation instructions\n.\n\n\nTheano installation instructions\n.\n\n\nCNTK installation instructions\n.\n\n\n\n\nYou may also consider installing the following \noptional dependencies\n:\n\n\n\n\ncuDNN\n (recommended if you plan on running Keras on GPU).\n\n\nHDF5 and \nh5py\n (required if you plan on saving Keras models to disk).\n\n\ngraphviz\n and \npydot\n (used by \nvisualization utilities\n to plot model graphs).\n\n\n\n\nThen, you can install Keras itself. There are two ways to install Keras:\n\n\n\n\nInstall Keras from PyPI (recommended):\n\n\n\n\nsudo pip install keras\n\n\n\n\nIf you are using a virtualenv, you may want to avoid using sudo:\n\n\npip install keras\n\n\n\n\n\n\nAlternatively: install Keras from the GitHub source:\n\n\n\n\nFirst, clone Keras using \ngit\n:\n\n\ngit clone https://github.com/keras-team/keras.git\n\n\n\n\nThen, \ncd\n to the Keras folder and run the install command:\n\n\ncd keras\nsudo python setup.py install\n\n\n\n\n\n\nUsing a different backend than TensorFlow\n\n\nBy default, Keras will use TensorFlow as its tensor manipulation library. \nFollow these instructions\n to configure the Keras backend.\n\n\n\n\nSupport\n\n\nYou can ask questions and join the development discussion:\n\n\n\n\nOn the \nKeras Google group\n.\n\n\nOn the \nKeras Slack channel\n. Use \nthis link\n to request an invitation to the channel.\n\n\n\n\nYou can also post \nbug reports and feature requests\n (only) in \nGitHub issues\n. Make sure to read \nour guidelines\n first.\n\n\n\n\nWhy this name, Keras?\n\n\nKeras (\u03ba\u03ad\u03c1\u03b1\u03c2) means \nhorn\n in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the \nOdyssey\n, where dream spirits (\nOneiroi\n, singular \nOneiros\n) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive).\n\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n\n\n\n\"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"\n Homer, Odyssey 19. 562 ff (Shewring translation).",
            "title": "Home"
        },
        {
            "location": "/#keras-the-python-deep-learning-library",
            "text": "",
            "title": "Keras: The Python Deep Learning library"
        },
        {
            "location": "/#you-have-just-found-keras",
            "text": "Keras is a high-level neural networks API, written in Python and capable of running on top of  TensorFlow ,  CNTK , or  Theano . It was developed with a focus on enabling fast experimentation.  Being able to go from idea to result with the least possible delay is key to doing good research.  Use Keras if you need a deep learning library that:   Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).  Supports both convolutional networks and recurrent networks, as well as combinations of the two.  Runs seamlessly on CPU and GPU.   Read the documentation at  Keras.io .  Keras is compatible with:  Python 2.7-3.6 .",
            "title": "You have just found Keras."
        },
        {
            "location": "/#guiding-principles",
            "text": "User friendliness.  Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.    Modularity.  A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.    Easy extensibility.  New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.    Work with Python . No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.",
            "title": "Guiding principles"
        },
        {
            "location": "/#getting-started-30-seconds-to-keras",
            "text": "The core data structure of Keras is a  model , a way to organize layers. The simplest type of model is the  Sequential  model, a linear stack of layers. For more complex architectures, you should use the  Keras functional API , which allows to build arbitrary graphs of layers.  Here is the  Sequential  model:  from keras.models import Sequential\n\nmodel = Sequential()  Stacking layers is as easy as  .add() :  from keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))  Once your model looks good, configure its learning process with  .compile() :  model.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])  If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).  model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))  You can now iterate on your training data in batches:  # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)  Alternatively, you can feed batches to your model manually:  model.train_on_batch(x_batch, y_batch)  Evaluate your performance in one line:  loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)  Or generate predictions on new data:  classes = model.predict(x_test, batch_size=128)  Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?  For a more in-depth tutorial about Keras, you can check out:   Getting started with the Sequential model  Getting started with the functional API   In the  examples folder  of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.",
            "title": "Getting started: 30 seconds to Keras"
        },
        {
            "location": "/#installation",
            "text": "Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.   TensorFlow installation instructions .  Theano installation instructions .  CNTK installation instructions .   You may also consider installing the following  optional dependencies :   cuDNN  (recommended if you plan on running Keras on GPU).  HDF5 and  h5py  (required if you plan on saving Keras models to disk).  graphviz  and  pydot  (used by  visualization utilities  to plot model graphs).   Then, you can install Keras itself. There are two ways to install Keras:   Install Keras from PyPI (recommended):   sudo pip install keras  If you are using a virtualenv, you may want to avoid using sudo:  pip install keras   Alternatively: install Keras from the GitHub source:   First, clone Keras using  git :  git clone https://github.com/keras-team/keras.git  Then,  cd  to the Keras folder and run the install command:  cd keras\nsudo python setup.py install",
            "title": "Installation"
        },
        {
            "location": "/#using-a-different-backend-than-tensorflow",
            "text": "By default, Keras will use TensorFlow as its tensor manipulation library.  Follow these instructions  to configure the Keras backend.",
            "title": "Using a different backend than TensorFlow"
        },
        {
            "location": "/#support",
            "text": "You can ask questions and join the development discussion:   On the  Keras Google group .  On the  Keras Slack channel . Use  this link  to request an invitation to the channel.   You can also post  bug reports and feature requests  (only) in  GitHub issues . Make sure to read  our guidelines  first.",
            "title": "Support"
        },
        {
            "location": "/#why-this-name-keras",
            "text": "Keras (\u03ba\u03ad\u03c1\u03b1\u03c2) means  horn  in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the  Odyssey , where dream spirits ( Oneiroi , singular  Oneiros ) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive).  Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).   \"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"  Homer, Odyssey 19. 562 ff (Shewring translation).",
            "title": "Why this name, Keras?"
        },
        {
            "location": "/why-use-keras/",
            "text": "Why use Keras?\n\n\nThere are countless deep learning frameworks available today. Why use Keras rather than any other? Here are some of the areas in which Keras compares favorably to existing alternatives.\n\n\n\n\nKeras prioritizes developer experience\n\n\n\n\nKeras is an API designed for human beings, not machines. \nKeras follows best practices for reducing cognitive load\n: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n\n\nThis makes Keras easy to learn and easy to use. As a Keras user, you are more productive, allowing you to try more ideas than your competition, faster -- which in turn \nhelps you win machine learning competitions\n.\n\n\nThis ease of use does not come at the cost of reduced flexibility: because Keras integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as \ntf.keras\n, the Keras API integrates seamlessly with your TensorFlow workflows.\n\n\n\n\n\n\nKeras has broad adoption in the industry and the research community\n\n\nWith over 200,000 individual users as of November 2017, Keras has stronger adoption in both the industry and the research community than any other deep learning framework except TensorFlow itself (and Keras is commonly used in conjunction with TensorFlow).\n\n\nYou are already constantly interacting with features built with Keras -- it is in use at Netflix, Uber, Yelp, Instacart, Zocdoc, Square, and many others. It is especially popular among startups that place deep learning at the core of their products.\n\n\nKeras is also a favorite among deep learning researchers, coming in #2 in terms of mentions in scientific papers uploaded to the preprint server \narXiv.org\n:\n\n\n\n\nKeras has also been adopted by researchers at large scientific organizations, in particular CERN and NASA.\n\n\n\n\nKeras makes it easy to turn models into products\n\n\nYour Keras models can be easily deployed across a greater range of platforms than any other deep learning framework:\n\n\n\n\nOn iOS, via \nApple\u2019s CoreML\n (Keras support officially provided by Apple)\n\n\nOn Android, via the TensorFlow Android runtime. Example: \nNot Hotdog app\n\n\nIn the browser, via GPU-accelerated JavaScript runtimes such as \nKeras.js\n and \nWebDNN\n\n\nOn Google Cloud, via \nTensorFlow-Serving\n\n\nIn a Python webapp backend (such as a Flask app)\n\n\nOn the JVM, via \nDL4J model import provided by SkyMind\n\n\nOn Raspberry Pi\n\n\n\n\n\n\nKeras supports multiple backend engines and does not lock you into one ecosystem\n\n\nYour Keras models can be developed with a range of different \ndeep learning backends\n. Importantly, any Keras model that only leverages built-in layers will be portable across all these backends: you can train a model with one backend, and load it with another (e.g. for deployment). Available backends include:\n\n\n\n\nThe TensorFlow backend (from Google)\n\n\nThe CNTK backend (from Microsoft)\n\n\nThe Theano backend\n\n\n\n\nAmazon is also currently working on developing a MXNet backend for Keras.\n\n\nAs such, your Keras model can be trained on a number of different hardware platforms beyond CPUs:\n\n\n\n\nNVIDIA GPUs\n\n\nGoogle TPUs\n, via the TensorFlow backend and Google Cloud\n\n\nOpenCL-enabled GPUs, such as those from AMD, via \nthe PlaidML Keras backend\n\n\n\n\n\n\nKeras has strong multi-GPU support and distributed training support\n\n\n\n\nKeras has \nbuilt-in support for multi-GPU data parallelism\n\n\nHorovod\n, from Uber, has first-class support for Keras models\n\n\nKeras models \ncan be turned into TensorFlow Estimators\n and trained on \nclusters of GPUs on Google Cloud\n\n\nKeras can be run on Spark via \nDist-Keras\n (from CERN) and \nElephas\n\n\n\n\n\n\nKeras development is backed by key companies in the deep learning ecosystem\n\n\nKeras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as \ntf.keras\n. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is developing MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).",
            "title": "Why use Keras"
        },
        {
            "location": "/why-use-keras/#why-use-keras",
            "text": "There are countless deep learning frameworks available today. Why use Keras rather than any other? Here are some of the areas in which Keras compares favorably to existing alternatives.",
            "title": "Why use Keras?"
        },
        {
            "location": "/why-use-keras/#keras-prioritizes-developer-experience",
            "text": "Keras is an API designed for human beings, not machines.  Keras follows best practices for reducing cognitive load : it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.  This makes Keras easy to learn and easy to use. As a Keras user, you are more productive, allowing you to try more ideas than your competition, faster -- which in turn  helps you win machine learning competitions .  This ease of use does not come at the cost of reduced flexibility: because Keras integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as  tf.keras , the Keras API integrates seamlessly with your TensorFlow workflows.",
            "title": "Keras prioritizes developer experience"
        },
        {
            "location": "/why-use-keras/#keras-has-broad-adoption-in-the-industry-and-the-research-community",
            "text": "With over 200,000 individual users as of November 2017, Keras has stronger adoption in both the industry and the research community than any other deep learning framework except TensorFlow itself (and Keras is commonly used in conjunction with TensorFlow).  You are already constantly interacting with features built with Keras -- it is in use at Netflix, Uber, Yelp, Instacart, Zocdoc, Square, and many others. It is especially popular among startups that place deep learning at the core of their products.  Keras is also a favorite among deep learning researchers, coming in #2 in terms of mentions in scientific papers uploaded to the preprint server  arXiv.org :   Keras has also been adopted by researchers at large scientific organizations, in particular CERN and NASA.",
            "title": "Keras has broad adoption in the industry and the research community"
        },
        {
            "location": "/why-use-keras/#keras-makes-it-easy-to-turn-models-into-products",
            "text": "Your Keras models can be easily deployed across a greater range of platforms than any other deep learning framework:   On iOS, via  Apple\u2019s CoreML  (Keras support officially provided by Apple)  On Android, via the TensorFlow Android runtime. Example:  Not Hotdog app  In the browser, via GPU-accelerated JavaScript runtimes such as  Keras.js  and  WebDNN  On Google Cloud, via  TensorFlow-Serving  In a Python webapp backend (such as a Flask app)  On the JVM, via  DL4J model import provided by SkyMind  On Raspberry Pi",
            "title": "Keras makes it easy to turn models into products"
        },
        {
            "location": "/why-use-keras/#keras-supports-multiple-backend-engines-and-does-not-lock-you-into-one-ecosystem",
            "text": "Your Keras models can be developed with a range of different  deep learning backends . Importantly, any Keras model that only leverages built-in layers will be portable across all these backends: you can train a model with one backend, and load it with another (e.g. for deployment). Available backends include:   The TensorFlow backend (from Google)  The CNTK backend (from Microsoft)  The Theano backend   Amazon is also currently working on developing a MXNet backend for Keras.  As such, your Keras model can be trained on a number of different hardware platforms beyond CPUs:   NVIDIA GPUs  Google TPUs , via the TensorFlow backend and Google Cloud  OpenCL-enabled GPUs, such as those from AMD, via  the PlaidML Keras backend",
            "title": "Keras supports multiple backend engines and does not lock you into one ecosystem"
        },
        {
            "location": "/why-use-keras/#keras-has-strong-multi-gpu-support-and-distributed-training-support",
            "text": "Keras has  built-in support for multi-GPU data parallelism  Horovod , from Uber, has first-class support for Keras models  Keras models  can be turned into TensorFlow Estimators  and trained on  clusters of GPUs on Google Cloud  Keras can be run on Spark via  Dist-Keras  (from CERN) and  Elephas",
            "title": "Keras has strong multi-GPU support and distributed training support"
        },
        {
            "location": "/why-use-keras/#keras-development-is-backed-by-key-companies-in-the-deep-learning-ecosystem",
            "text": "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as  tf.keras . Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is developing MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).",
            "title": "Keras development is backed by key companies in the deep learning ecosystem"
        },
        {
            "location": "/getting-started/sequential-model-guide/",
            "text": "Getting started with the Keras Sequential model\n\n\nThe \nSequential\n model is a linear stack of layers.\n\n\nYou can create a \nSequential\n model by passing a list of layer instances to the constructor:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])\n\n\n\n\nYou can also simply add layers via the \n.add()\n method:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n\n\n\n\n\n\nSpecifying the input shape\n\n\nThe model needs to know what input shape it should expect. For this reason, the first layer in a \nSequential\n model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n\n\n\n\nPass an \ninput_shape\n argument to the first layer. This is a shape tuple (a tuple of integers or \nNone\n entries, where \nNone\n indicates that any positive integer may be expected). In \ninput_shape\n, the batch dimension is not included.\n\n\nSome 2D layers, such as \nDense\n, support the specification of their input shape via the argument \ninput_dim\n, and some 3D temporal layers support the arguments \ninput_dim\n and \ninput_length\n.\n\n\nIf you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a \nbatch_size\n argument to a layer. If you pass both \nbatch_size=32\n and \ninput_shape=(6, 8)\n to a layer, it will then expect every batch of inputs to have the batch shape \n(32, 6, 8)\n.\n\n\n\n\nAs such, the following snippets are strictly equivalent:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(784,)))\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\n\n\n\n\n\n\nCompilation\n\n\nBefore training a model, you need to configure the learning process, which is done via the \ncompile\n method. It receives three arguments:\n\n\n\n\nAn optimizer. This could be the string identifier of an existing optimizer (such as \nrmsprop\n or \nadagrad\n), or an instance of the \nOptimizer\n class. See: \noptimizers\n.\n\n\nA loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as \ncategorical_crossentropy\n or \nmse\n), or it can be an objective function. See: \nlosses\n.\n\n\nA list of metrics. For any classification problem you will want to set this to \nmetrics=['accuracy']\n. A metric could be the string identifier of an existing metric or a custom metric function.\n\n\n\n\n# For a multi-class classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# For a binary classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# For a mean squared error regression problem\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n\n# For custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred])\n\n\n\n\n\n\nTraining\n\n\nKeras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the \nfit\n function. \nRead its documentation here\n.\n\n\n# For a single-input model with 2 classes (binary classification):\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 100))\nlabels = np.random.randint(2, size=(1000, 1))\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel.fit(data, labels, epochs=10, batch_size=32)\n\n\n\n\n# For a single-input model with 10 classes (categorical classification):\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=100))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 100))\nlabels = np.random.randint(10, size=(1000, 1))\n\n# Convert labels to categorical one-hot encoding\none_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)\n\n\n\n\n\n\nExamples\n\n\nHere are a few examples to get you started!\n\n\nIn the \nexamples folder\n, you will also find example models for real datasets:\n\n\n\n\nCIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation\n\n\nIMDB movie review sentiment classification: LSTM over sequences of words\n\n\nReuters newswires topic classification: Multilayer Perceptron (MLP)\n\n\nMNIST handwritten digits classification: MLP & CNN\n\n\nCharacter-level text generation with LSTM\n\n\n\n\n...and more.\n\n\nMultilayer Perceptron (MLP) for multi-class softmax classification:\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nimport numpy as np\nx_train = np.random.random((1000, 20))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\nx_test = np.random.random((100, 20))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n\nmodel = Sequential()\n# Dense(64) is a fully-connected layer with 64 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 20-dimensional vectors.\nmodel.add(Dense(64, activation='relu', input_dim=20))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)\n\n\n\n\nMLP for binary classification:\n\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Generate dummy data\nx_train = np.random.random((1000, 20))\ny_train = np.random.randint(2, size=(1000, 1))\nx_test = np.random.random((100, 20))\ny_test = np.random.randint(2, size=(100, 1))\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)\n\n\n\n\nVGG-like convnet:\n\n\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nx_train = np.random.random((100, 100, 100, 3))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\nx_test = np.random.random((20, 100, 100, 3))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n\nmodel = Sequential()\n# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=32)\n\n\n\n\nSequence classification with LSTM:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, output_dim=256))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)\n\n\n\n\nSequence classification with 1D convolutions:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n\nmodel = Sequential()\nmodel.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))\nmodel.add(Conv1D(64, 3, activation='relu'))\nmodel.add(MaxPooling1D(3))\nmodel.add(Conv1D(128, 3, activation='relu'))\nmodel.add(Conv1D(128, 3, activation='relu'))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)\n\n\n\n\nStacked LSTM for sequence classification\n\n\nIn this model, we stack 3 LSTM layers on top of each other,\nmaking the model capable of learning higher-level temporal representations.\n\n\nThe first two LSTMs return their full output sequences, but the last one only returns\nthe last step in its output sequence, thus dropping the temporal dimension\n(i.e. converting the input sequence into a single vector).\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 10\n\n# expected input data shape: (batch_size, timesteps, data_dim)\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True,\n               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32))  # return a single vector of dimension 32\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# Generate dummy training data\nx_train = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, num_classes))\n\n# Generate dummy validation data\nx_val = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, num_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=64, epochs=5,\n          validation_data=(x_val, y_val))\n\n\n\n\nSame stacked LSTM model, rendered \"stateful\"\n\n\nA stateful recurrent model is one for which the internal states (memories) obtained after processing a batch\nof samples are reused as initial states for the samples of the next batch. This allows to process longer sequences\nwhile keeping computational complexity manageable.\n\n\nYou can read more about stateful RNNs in the FAQ.\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 10\nbatch_size = 32\n\n# Expected input batch shape: (batch_size, timesteps, data_dim)\n# Note that we have to provide the full batch_input_shape since the network is stateful.\n# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, stateful=True,\n               batch_input_shape=(batch_size, timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True, stateful=True))\nmodel.add(LSTM(32, stateful=True))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# Generate dummy training data\nx_train = np.random.random((batch_size * 10, timesteps, data_dim))\ny_train = np.random.random((batch_size * 10, num_classes))\n\n# Generate dummy validation data\nx_val = np.random.random((batch_size * 3, timesteps, data_dim))\ny_val = np.random.random((batch_size * 3, num_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size, epochs=5, shuffle=False,\n          validation_data=(x_val, y_val))",
            "title": "Guide to the Sequential model"
        },
        {
            "location": "/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model",
            "text": "The  Sequential  model is a linear stack of layers.  You can create a  Sequential  model by passing a list of layer instances to the constructor:  from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])  You can also simply add layers via the  .add()  method:  model = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))",
            "title": "Getting started with the Keras Sequential model"
        },
        {
            "location": "/getting-started/sequential-model-guide/#specifying-the-input-shape",
            "text": "The model needs to know what input shape it should expect. For this reason, the first layer in a  Sequential  model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:   Pass an  input_shape  argument to the first layer. This is a shape tuple (a tuple of integers or  None  entries, where  None  indicates that any positive integer may be expected). In  input_shape , the batch dimension is not included.  Some 2D layers, such as  Dense , support the specification of their input shape via the argument  input_dim , and some 3D temporal layers support the arguments  input_dim  and  input_length .  If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a  batch_size  argument to a layer. If you pass both  batch_size=32  and  input_shape=(6, 8)  to a layer, it will then expect every batch of inputs to have the batch shape  (32, 6, 8) .   As such, the following snippets are strictly equivalent:  model = Sequential()\nmodel.add(Dense(32, input_shape=(784,)))  model = Sequential()\nmodel.add(Dense(32, input_dim=784))",
            "title": "Specifying the input shape"
        },
        {
            "location": "/getting-started/sequential-model-guide/#compilation",
            "text": "Before training a model, you need to configure the learning process, which is done via the  compile  method. It receives three arguments:   An optimizer. This could be the string identifier of an existing optimizer (such as  rmsprop  or  adagrad ), or an instance of the  Optimizer  class. See:  optimizers .  A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as  categorical_crossentropy  or  mse ), or it can be an objective function. See:  losses .  A list of metrics. For any classification problem you will want to set this to  metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function.   # For a multi-class classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# For a binary classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# For a mean squared error regression problem\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n\n# For custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred])",
            "title": "Compilation"
        },
        {
            "location": "/getting-started/sequential-model-guide/#training",
            "text": "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit  function.  Read its documentation here .  # For a single-input model with 2 classes (binary classification):\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 100))\nlabels = np.random.randint(2, size=(1000, 1))\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel.fit(data, labels, epochs=10, batch_size=32)  # For a single-input model with 10 classes (categorical classification):\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=100))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 100))\nlabels = np.random.randint(10, size=(1000, 1))\n\n# Convert labels to categorical one-hot encoding\none_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)",
            "title": "Training"
        },
        {
            "location": "/getting-started/sequential-model-guide/#examples",
            "text": "Here are a few examples to get you started!  In the  examples folder , you will also find example models for real datasets:   CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation  IMDB movie review sentiment classification: LSTM over sequences of words  Reuters newswires topic classification: Multilayer Perceptron (MLP)  MNIST handwritten digits classification: MLP & CNN  Character-level text generation with LSTM   ...and more.",
            "title": "Examples"
        },
        {
            "location": "/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification",
            "text": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nimport numpy as np\nx_train = np.random.random((1000, 20))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\nx_test = np.random.random((100, 20))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n\nmodel = Sequential()\n# Dense(64) is a fully-connected layer with 64 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 20-dimensional vectors.\nmodel.add(Dense(64, activation='relu', input_dim=20))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)",
            "title": "Multilayer Perceptron (MLP) for multi-class softmax classification:"
        },
        {
            "location": "/getting-started/sequential-model-guide/#mlp-for-binary-classification",
            "text": "import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Generate dummy data\nx_train = np.random.random((1000, 20))\ny_train = np.random.randint(2, size=(1000, 1))\nx_test = np.random.random((100, 20))\ny_test = np.random.randint(2, size=(100, 1))\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)",
            "title": "MLP for binary classification:"
        },
        {
            "location": "/getting-started/sequential-model-guide/#vgg-like-convnet",
            "text": "import numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nx_train = np.random.random((100, 100, 100, 3))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\nx_test = np.random.random((20, 100, 100, 3))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n\nmodel = Sequential()\n# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=32)",
            "title": "VGG-like convnet:"
        },
        {
            "location": "/getting-started/sequential-model-guide/#sequence-classification-with-lstm",
            "text": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, output_dim=256))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)",
            "title": "Sequence classification with LSTM:"
        },
        {
            "location": "/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions",
            "text": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n\nmodel = Sequential()\nmodel.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))\nmodel.add(Conv1D(64, 3, activation='relu'))\nmodel.add(MaxPooling1D(3))\nmodel.add(Conv1D(128, 3, activation='relu'))\nmodel.add(Conv1D(128, 3, activation='relu'))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)",
            "title": "Sequence classification with 1D convolutions:"
        },
        {
            "location": "/getting-started/sequential-model-guide/#stacked-lstm-for-sequence-classification",
            "text": "In this model, we stack 3 LSTM layers on top of each other,\nmaking the model capable of learning higher-level temporal representations.  The first two LSTMs return their full output sequences, but the last one only returns\nthe last step in its output sequence, thus dropping the temporal dimension\n(i.e. converting the input sequence into a single vector).   from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 10\n\n# expected input data shape: (batch_size, timesteps, data_dim)\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True,\n               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32))  # return a single vector of dimension 32\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# Generate dummy training data\nx_train = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, num_classes))\n\n# Generate dummy validation data\nx_val = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, num_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=64, epochs=5,\n          validation_data=(x_val, y_val))",
            "title": "Stacked LSTM for sequence classification"
        },
        {
            "location": "/getting-started/sequential-model-guide/#same-stacked-lstm-model-rendered-stateful",
            "text": "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch\nof samples are reused as initial states for the samples of the next batch. This allows to process longer sequences\nwhile keeping computational complexity manageable.  You can read more about stateful RNNs in the FAQ.  from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 10\nbatch_size = 32\n\n# Expected input batch shape: (batch_size, timesteps, data_dim)\n# Note that we have to provide the full batch_input_shape since the network is stateful.\n# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, stateful=True,\n               batch_input_shape=(batch_size, timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True, stateful=True))\nmodel.add(LSTM(32, stateful=True))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# Generate dummy training data\nx_train = np.random.random((batch_size * 10, timesteps, data_dim))\ny_train = np.random.random((batch_size * 10, num_classes))\n\n# Generate dummy validation data\nx_val = np.random.random((batch_size * 3, timesteps, data_dim))\ny_val = np.random.random((batch_size * 3, num_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size, epochs=5, shuffle=False,\n          validation_data=(x_val, y_val))",
            "title": "Same stacked LSTM model, rendered \"stateful\""
        },
        {
            "location": "/getting-started/functional-api-guide/",
            "text": "Getting started with the Keras functional API\n\n\nThe Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n\n\nThis guide assumes that you are already familiar with the \nSequential\n model.\n\n\nLet's start with something simple.\n\n\n\n\nFirst example: a densely-connected network\n\n\nThe \nSequential\n model is probably a better choice to implement such a network, but it helps to start with something really simple.\n\n\n\n\nA layer instance is callable (on a tensor), and it returns a tensor\n\n\nInput tensor(s) and output tensor(s) can then be used to define a \nModel\n\n\nSuch a model can be trained just like Keras \nSequential\n models.\n\n\n\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# This returns a tensor\ninputs = Input(shape=(784,))\n\n# a layer instance is callable on a tensor, and returns a tensor\nx = Dense(64, activation='relu')(inputs)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# This creates a model that includes\n# the Input layer and three Dense layers\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training\n\n\n\n\n\n\nAll models are callable, just like layers\n\n\nWith the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the \narchitecture\n of the model, you are also reusing its weights.\n\n\nx = Input(shape=(784,))\n# This works, and returns the 10-way softmax we defined above.\ny = model(x)\n\n\n\n\nThis can allow, for instance, to quickly create models that can process \nsequences\n of inputs. You could turn an image classification model into a video classification model, in just one line.\n\n\nfrom keras.layers import TimeDistributed\n\n# Input tensor for sequences of 20 timesteps,\n# each containing a 784-dimensional vector\ninput_sequences = Input(shape=(20, 784))\n\n# This applies our previous model to every timestep in the input sequences.\n# the output of the previous model was a 10-way softmax,\n# so the output of the layer below will be a sequence of 20 vectors of size 10.\nprocessed_sequences = TimeDistributed(model)(input_sequences)\n\n\n\n\n\n\nMulti-input and multi-output models\n\n\nHere's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n\n\nLet's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n\n\nHere's what our model looks like:\n\n\n\n\nLet's implement it with the functional API.\n\n\nThe main input will receive the headline, as a sequence of integers (each integer encodes a word).\nThe integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.\n\n\nfrom keras.layers import Input, Embedding, LSTM, Dense\nfrom keras.models import Model\n\n# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n# Note that we can name any layer by passing it a \"name\" argument.\nmain_input = Input(shape=(100,), dtype='int32', name='main_input')\n\n# This embedding layer will encode the input sequence\n# into a sequence of dense 512-dimensional vectors.\nx = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n\n# A LSTM will transform the vector sequence into a single vector,\n# containing information about the entire sequence\nlstm_out = LSTM(32)(x)\n\n\n\n\nHere we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.\n\n\nauxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n\n\n\n\nAt this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:\n\n\nauxiliary_input = Input(shape=(5,), name='aux_input')\nx = keras.layers.concatenate([lstm_out, auxiliary_input])\n\n# We stack a deep densely-connected network on top\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# And finally we add the main logistic regression layer\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\n\n\n\n\nThis defines a model with two inputs and two outputs:\n\n\nmodel = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n\n\n\n\nWe compile the model and assign a weight of 0.2 to the auxiliary loss.\nTo specify different \nloss_weights\n or \nloss\n for each different output, you can use a list or a dictionary.\nHere we pass a single loss as the \nloss\n argument, so the same loss will be used on all outputs.\n\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',\n              loss_weights=[1., 0.2])\n\n\n\n\nWe can train the model by passing it lists of input arrays and target arrays:\n\n\nmodel.fit([headline_data, additional_data], [labels, labels],\n          epochs=50, batch_size=32)\n\n\n\n\nSince our inputs and outputs are named (we passed them a \"name\" argument),\nWe could also have compiled the model via:\n\n\nmodel.compile(optimizer='rmsprop',\n              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n              loss_weights={'main_output': 1., 'aux_output': 0.2})\n\n# And trained it via:\nmodel.fit({'main_input': headline_data, 'aux_input': additional_data},\n          {'main_output': labels, 'aux_output': labels},\n          epochs=50, batch_size=32)\n\n\n\n\n\n\nShared layers\n\n\nAnother good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n\n\nLet's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\n\n\nOne way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\n\n\nBecause the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\n\n\nLet's build this with the functional API. We will take as input for a tweet a binary matrix of shape \n(280, 256)\n, i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).\n\n\nimport keras\nfrom keras.layers import Input, LSTM, Dense\nfrom keras.models import Model\n\ntweet_a = Input(shape=(280, 256))\ntweet_b = Input(shape=(280, 256))\n\n\n\n\nTo share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:\n\n\n# This layer can take as input a matrix\n# and will return a vector of size 64\nshared_lstm = LSTM(64)\n\n# When we reuse the same layer instance\n# multiple times, the weights of the layer\n# are also being reused\n# (it is effectively *the same* layer)\nencoded_a = shared_lstm(tweet_a)\nencoded_b = shared_lstm(tweet_b)\n\n# We can then concatenate the two vectors:\nmerged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n\n# And add a logistic regression on top\npredictions = Dense(1, activation='sigmoid')(merged_vector)\n\n# We define a trainable model linking the\n# tweet inputs to the predictions\nmodel = Model(inputs=[tweet_a, tweet_b], outputs=predictions)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([data_a, data_b], labels, epochs=10)\n\n\n\n\nLet's pause to take a look at how to read the shared layer's output or output shape.\n\n\n\n\nThe concept of layer \"node\"\n\n\nWhenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...\n\n\nIn previous versions of Keras, you could obtain the output tensor of a layer instance via \nlayer.get_output()\n, or its output shape via \nlayer.output_shape\n. You still can (except \nget_output()\n has been replaced by the property \noutput\n). But what if a layer is connected to multiple inputs?\n\n\nAs long as a layer is only connected to one input, there is no confusion, and \n.output\n will return the one output of the layer:\n\n\na = Input(shape=(280, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\n\nassert lstm.output == encoded_a\n\n\n\n\nNot so if the layer has multiple inputs:\n\n\na = Input(shape=(280, 256))\nb = Input(shape=(280, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\nencoded_b = lstm(b)\n\nlstm.output\n\n\n\n\n>> AttributeError: Layer lstm_1 has multiple inbound nodes,\nhence the notion of \"layer output\" is ill-defined.\nUse `get_output_at(node_index)` instead.\n\n\n\n\nOkay then. The following works:\n\n\nassert lstm.get_output_at(0) == encoded_a\nassert lstm.get_output_at(1) == encoded_b\n\n\n\n\nSimple enough, right?\n\n\nThe same is true for the properties \ninput_shape\n and \noutput_shape\n: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by \nlayer.output_shape\n/\nlayer.input_shape\n. But if, for instance, you apply the same \nConv2D\n layer to an input of shape \n(32, 32, 3)\n, and then to an input of shape \n(64, 64, 3)\n, the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:\n\n\na = Input(shape=(32, 32, 3))\nb = Input(shape=(64, 64, 3))\n\nconv = Conv2D(16, (3, 3), padding='same')\nconved_a = conv(a)\n\n# Only one input so far, the following will work:\nassert conv.input_shape == (None, 32, 32, 3)\n\nconved_b = conv(b)\n# now the `.input_shape` property wouldn't work, but this does:\nassert conv.get_input_shape_at(0) == (None, 32, 32, 3)\nassert conv.get_input_shape_at(1) == (None, 64, 64, 3)\n\n\n\n\n\n\nMore examples\n\n\nCode examples are still the best way to get started, so here are a few more.\n\n\nInception module\n\n\nFor more information about the Inception architecture, see \nGoing Deeper with Convolutions\n.\n\n\nfrom keras.layers import Conv2D, MaxPooling2D, Input\n\ninput_img = Input(shape=(256, 256, 3))\n\ntower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\ntower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n\ntower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\ntower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n\ntower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\ntower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n\noutput = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n\n\n\n\nResidual connection on a convolution layer\n\n\nFor more information about residual networks, see \nDeep Residual Learning for Image Recognition\n.\n\n\nfrom keras.layers import Conv2D, Input\n\n# input tensor for a 3-channel 256x256 image\nx = Input(shape=(256, 256, 3))\n# 3x3 conv with 3 output channels (same as input channels)\ny = Conv2D(3, (3, 3), padding='same')(x)\n# this returns x + y.\nz = keras.layers.add([x, y])\n\n\n\n\nShared vision model\n\n\nThis model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.\n\n\nfrom keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\nfrom keras.models import Model\n\n# First, define the vision modules\ndigit_input = Input(shape=(27, 27, 1))\nx = Conv2D(64, (3, 3))(digit_input)\nx = Conv2D(64, (3, 3))(x)\nx = MaxPooling2D((2, 2))(x)\nout = Flatten()(x)\n\nvision_model = Model(digit_input, out)\n\n# Then define the tell-digits-apart model\ndigit_a = Input(shape=(27, 27, 1))\ndigit_b = Input(shape=(27, 27, 1))\n\n# The vision model will be shared, weights and all\nout_a = vision_model(digit_a)\nout_b = vision_model(digit_b)\n\nconcatenated = keras.layers.concatenate([out_a, out_b])\nout = Dense(1, activation='sigmoid')(concatenated)\n\nclassification_model = Model([digit_a, digit_b], out)\n\n\n\n\nVisual question answering model\n\n\nThis model can select the correct one-word answer when asked a natural-language question about a picture.\n\n\nIt works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.\n\n\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model, Sequential\n\n# First, let's define a vision model using a Sequential model.\n# This model will encode an image into a vector.\nvision_model = Sequential()\nvision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\nvision_model.add(Conv2D(64, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nvision_model.add(Conv2D(128, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nvision_model.add(Conv2D(256, (3, 3), activation='relu'))\nvision_model.add(Conv2D(256, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Flatten())\n\n# Now let's get a tensor with the output of our vision model:\nimage_input = Input(shape=(224, 224, 3))\nencoded_image = vision_model(image_input)\n\n# Next, let's define a language model to encode the question into a vector.\n# Each question will be at most 100 word long,\n# and we will index words as integers from 1 to 9999.\nquestion_input = Input(shape=(100,), dtype='int32')\nembedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)\nencoded_question = LSTM(256)(embedded_question)\n\n# Let's concatenate the question vector and the image vector:\nmerged = keras.layers.concatenate([encoded_question, encoded_image])\n\n# And let's train a logistic regression over 1000 words on top:\noutput = Dense(1000, activation='softmax')(merged)\n\n# This is our final model:\nvqa_model = Model(inputs=[image_input, question_input], outputs=output)\n\n# The next stage would be training this model on actual data.\n\n\n\n\nVideo question answering model\n\n\nNow that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -> \"football\").\n\n\nfrom keras.layers import TimeDistributed\n\nvideo_input = Input(shape=(100, 224, 224, 3))\n# This is our video encoded via the previously trained vision_model (weights are reused)\nencoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\nencoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n\n# This is a model-level representation of the question encoder, reusing the same weights as before:\nquestion_encoder = Model(inputs=question_input, outputs=encoded_question)\n\n# Let's use it to encode the question:\nvideo_question_input = Input(shape=(100,), dtype='int32')\nencoded_video_question = question_encoder(video_question_input)\n\n# And this is our video question answering model:\nmerged = keras.layers.concatenate([encoded_video, encoded_video_question])\noutput = Dense(1000, activation='softmax')(merged)\nvideo_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)",
            "title": "Guide to the Functional API"
        },
        {
            "location": "/getting-started/functional-api-guide/#getting-started-with-the-keras-functional-api",
            "text": "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.  This guide assumes that you are already familiar with the  Sequential  model.  Let's start with something simple.",
            "title": "Getting started with the Keras functional API"
        },
        {
            "location": "/getting-started/functional-api-guide/#first-example-a-densely-connected-network",
            "text": "The  Sequential  model is probably a better choice to implement such a network, but it helps to start with something really simple.   A layer instance is callable (on a tensor), and it returns a tensor  Input tensor(s) and output tensor(s) can then be used to define a  Model  Such a model can be trained just like Keras  Sequential  models.   from keras.layers import Input, Dense\nfrom keras.models import Model\n\n# This returns a tensor\ninputs = Input(shape=(784,))\n\n# a layer instance is callable on a tensor, and returns a tensor\nx = Dense(64, activation='relu')(inputs)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# This creates a model that includes\n# the Input layer and three Dense layers\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training",
            "title": "First example: a densely-connected network"
        },
        {
            "location": "/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers",
            "text": "With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the  architecture  of the model, you are also reusing its weights.  x = Input(shape=(784,))\n# This works, and returns the 10-way softmax we defined above.\ny = model(x)  This can allow, for instance, to quickly create models that can process  sequences  of inputs. You could turn an image classification model into a video classification model, in just one line.  from keras.layers import TimeDistributed\n\n# Input tensor for sequences of 20 timesteps,\n# each containing a 784-dimensional vector\ninput_sequences = Input(shape=(20, 784))\n\n# This applies our previous model to every timestep in the input sequences.\n# the output of the previous model was a 10-way softmax,\n# so the output of the layer below will be a sequence of 20 vectors of size 10.\nprocessed_sequences = TimeDistributed(model)(input_sequences)",
            "title": "All models are callable, just like layers"
        },
        {
            "location": "/getting-started/functional-api-guide/#multi-input-and-multi-output-models",
            "text": "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.  Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.  Here's what our model looks like:   Let's implement it with the functional API.  The main input will receive the headline, as a sequence of integers (each integer encodes a word).\nThe integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.  from keras.layers import Input, Embedding, LSTM, Dense\nfrom keras.models import Model\n\n# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n# Note that we can name any layer by passing it a \"name\" argument.\nmain_input = Input(shape=(100,), dtype='int32', name='main_input')\n\n# This embedding layer will encode the input sequence\n# into a sequence of dense 512-dimensional vectors.\nx = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n\n# A LSTM will transform the vector sequence into a single vector,\n# containing information about the entire sequence\nlstm_out = LSTM(32)(x)  Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.  auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)  At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:  auxiliary_input = Input(shape=(5,), name='aux_input')\nx = keras.layers.concatenate([lstm_out, auxiliary_input])\n\n# We stack a deep densely-connected network on top\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# And finally we add the main logistic regression layer\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)  This defines a model with two inputs and two outputs:  model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])  We compile the model and assign a weight of 0.2 to the auxiliary loss.\nTo specify different  loss_weights  or  loss  for each different output, you can use a list or a dictionary.\nHere we pass a single loss as the  loss  argument, so the same loss will be used on all outputs.  model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n              loss_weights=[1., 0.2])  We can train the model by passing it lists of input arrays and target arrays:  model.fit([headline_data, additional_data], [labels, labels],\n          epochs=50, batch_size=32)  Since our inputs and outputs are named (we passed them a \"name\" argument),\nWe could also have compiled the model via:  model.compile(optimizer='rmsprop',\n              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n              loss_weights={'main_output': 1., 'aux_output': 0.2})\n\n# And trained it via:\nmodel.fit({'main_input': headline_data, 'aux_input': additional_data},\n          {'main_output': labels, 'aux_output': labels},\n          epochs=50, batch_size=32)",
            "title": "Multi-input and multi-output models"
        },
        {
            "location": "/getting-started/functional-api-guide/#shared-layers",
            "text": "Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.  Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).  One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.  Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.  Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape  (280, 256) , i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).  import keras\nfrom keras.layers import Input, LSTM, Dense\nfrom keras.models import Model\n\ntweet_a = Input(shape=(280, 256))\ntweet_b = Input(shape=(280, 256))  To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:  # This layer can take as input a matrix\n# and will return a vector of size 64\nshared_lstm = LSTM(64)\n\n# When we reuse the same layer instance\n# multiple times, the weights of the layer\n# are also being reused\n# (it is effectively *the same* layer)\nencoded_a = shared_lstm(tweet_a)\nencoded_b = shared_lstm(tweet_b)\n\n# We can then concatenate the two vectors:\nmerged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n\n# And add a logistic regression on top\npredictions = Dense(1, activation='sigmoid')(merged_vector)\n\n# We define a trainable model linking the\n# tweet inputs to the predictions\nmodel = Model(inputs=[tweet_a, tweet_b], outputs=predictions)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([data_a, data_b], labels, epochs=10)  Let's pause to take a look at how to read the shared layer's output or output shape.",
            "title": "Shared layers"
        },
        {
            "location": "/getting-started/functional-api-guide/#the-concept-of-layer-node",
            "text": "Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...  In previous versions of Keras, you could obtain the output tensor of a layer instance via  layer.get_output() , or its output shape via  layer.output_shape . You still can (except  get_output()  has been replaced by the property  output ). But what if a layer is connected to multiple inputs?  As long as a layer is only connected to one input, there is no confusion, and  .output  will return the one output of the layer:  a = Input(shape=(280, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\n\nassert lstm.output == encoded_a  Not so if the layer has multiple inputs:  a = Input(shape=(280, 256))\nb = Input(shape=(280, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\nencoded_b = lstm(b)\n\nlstm.output  >> AttributeError: Layer lstm_1 has multiple inbound nodes,\nhence the notion of \"layer output\" is ill-defined.\nUse `get_output_at(node_index)` instead.  Okay then. The following works:  assert lstm.get_output_at(0) == encoded_a\nassert lstm.get_output_at(1) == encoded_b  Simple enough, right?  The same is true for the properties  input_shape  and  output_shape : as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by  layer.output_shape / layer.input_shape . But if, for instance, you apply the same  Conv2D  layer to an input of shape  (32, 32, 3) , and then to an input of shape  (64, 64, 3) , the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:  a = Input(shape=(32, 32, 3))\nb = Input(shape=(64, 64, 3))\n\nconv = Conv2D(16, (3, 3), padding='same')\nconved_a = conv(a)\n\n# Only one input so far, the following will work:\nassert conv.input_shape == (None, 32, 32, 3)\n\nconved_b = conv(b)\n# now the `.input_shape` property wouldn't work, but this does:\nassert conv.get_input_shape_at(0) == (None, 32, 32, 3)\nassert conv.get_input_shape_at(1) == (None, 64, 64, 3)",
            "title": "The concept of layer \"node\""
        },
        {
            "location": "/getting-started/functional-api-guide/#more-examples",
            "text": "Code examples are still the best way to get started, so here are a few more.",
            "title": "More examples"
        },
        {
            "location": "/getting-started/functional-api-guide/#inception-module",
            "text": "For more information about the Inception architecture, see  Going Deeper with Convolutions .  from keras.layers import Conv2D, MaxPooling2D, Input\n\ninput_img = Input(shape=(256, 256, 3))\n\ntower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\ntower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n\ntower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\ntower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n\ntower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\ntower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n\noutput = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)",
            "title": "Inception module"
        },
        {
            "location": "/getting-started/functional-api-guide/#residual-connection-on-a-convolution-layer",
            "text": "For more information about residual networks, see  Deep Residual Learning for Image Recognition .  from keras.layers import Conv2D, Input\n\n# input tensor for a 3-channel 256x256 image\nx = Input(shape=(256, 256, 3))\n# 3x3 conv with 3 output channels (same as input channels)\ny = Conv2D(3, (3, 3), padding='same')(x)\n# this returns x + y.\nz = keras.layers.add([x, y])",
            "title": "Residual connection on a convolution layer"
        },
        {
            "location": "/getting-started/functional-api-guide/#shared-vision-model",
            "text": "This model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.  from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\nfrom keras.models import Model\n\n# First, define the vision modules\ndigit_input = Input(shape=(27, 27, 1))\nx = Conv2D(64, (3, 3))(digit_input)\nx = Conv2D(64, (3, 3))(x)\nx = MaxPooling2D((2, 2))(x)\nout = Flatten()(x)\n\nvision_model = Model(digit_input, out)\n\n# Then define the tell-digits-apart model\ndigit_a = Input(shape=(27, 27, 1))\ndigit_b = Input(shape=(27, 27, 1))\n\n# The vision model will be shared, weights and all\nout_a = vision_model(digit_a)\nout_b = vision_model(digit_b)\n\nconcatenated = keras.layers.concatenate([out_a, out_b])\nout = Dense(1, activation='sigmoid')(concatenated)\n\nclassification_model = Model([digit_a, digit_b], out)",
            "title": "Shared vision model"
        },
        {
            "location": "/getting-started/functional-api-guide/#visual-question-answering-model",
            "text": "This model can select the correct one-word answer when asked a natural-language question about a picture.  It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.  from keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model, Sequential\n\n# First, let's define a vision model using a Sequential model.\n# This model will encode an image into a vector.\nvision_model = Sequential()\nvision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\nvision_model.add(Conv2D(64, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nvision_model.add(Conv2D(128, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nvision_model.add(Conv2D(256, (3, 3), activation='relu'))\nvision_model.add(Conv2D(256, (3, 3), activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Flatten())\n\n# Now let's get a tensor with the output of our vision model:\nimage_input = Input(shape=(224, 224, 3))\nencoded_image = vision_model(image_input)\n\n# Next, let's define a language model to encode the question into a vector.\n# Each question will be at most 100 word long,\n# and we will index words as integers from 1 to 9999.\nquestion_input = Input(shape=(100,), dtype='int32')\nembedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)\nencoded_question = LSTM(256)(embedded_question)\n\n# Let's concatenate the question vector and the image vector:\nmerged = keras.layers.concatenate([encoded_question, encoded_image])\n\n# And let's train a logistic regression over 1000 words on top:\noutput = Dense(1000, activation='softmax')(merged)\n\n# This is our final model:\nvqa_model = Model(inputs=[image_input, question_input], outputs=output)\n\n# The next stage would be training this model on actual data.",
            "title": "Visual question answering model"
        },
        {
            "location": "/getting-started/functional-api-guide/#video-question-answering-model",
            "text": "Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -> \"football\").  from keras.layers import TimeDistributed\n\nvideo_input = Input(shape=(100, 224, 224, 3))\n# This is our video encoded via the previously trained vision_model (weights are reused)\nencoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\nencoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n\n# This is a model-level representation of the question encoder, reusing the same weights as before:\nquestion_encoder = Model(inputs=question_input, outputs=encoded_question)\n\n# Let's use it to encode the question:\nvideo_question_input = Input(shape=(100,), dtype='int32')\nencoded_video_question = question_encoder(video_question_input)\n\n# And this is our video question answering model:\nmerged = keras.layers.concatenate([encoded_video, encoded_video_question])\noutput = Dense(1000, activation='softmax')(merged)\nvideo_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)",
            "title": "Video question answering model"
        },
        {
            "location": "/getting-started/faq/",
            "text": "Keras FAQ: Frequently Asked Keras Questions\n\n\n\n\nHow should I cite Keras?\n\n\nHow can I run Keras on GPU?\n\n\nHow can I run a Keras model on multiple GPUs?\n\n\nWhat does \"sample\", \"batch\", \"epoch\" mean?\n\n\nHow can I save a Keras model?\n\n\nWhy is the training loss much higher than the testing loss?\n\n\nHow can I obtain the output of an intermediate layer?\n\n\nHow can I use Keras with datasets that don't fit in memory?\n\n\nHow can I interrupt training when the validation loss isn't decreasing anymore?\n\n\nHow is the validation split computed?\n\n\nIs the data shuffled during training?\n\n\nHow can I record the training / validation loss / accuracy at each epoch?\n\n\nHow can I \"freeze\" layers?\n\n\nHow can I use stateful RNNs?\n\n\nHow can I remove a layer from a Sequential model?\n\n\nHow can I use pre-trained models in Keras?\n\n\nHow can I use HDF5 inputs with Keras?\n\n\nWhere is the Keras configuration file stored?\n\n\nHow can I obtain reproducible results using Keras during development?\n\n\n\n\n\n\nHow should I cite Keras?\n\n\nPlease cite Keras in your publications if it helps your research. Here is an example BibTeX entry:\n\n\n@misc{chollet2015keras,\n  title={Keras},\n  author={Chollet, Fran\\c{c}ois and others},\n  year={2015},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/keras-team/keras}},\n}\n\n\n\n\n\n\nHow can I run Keras on GPU?\n\n\nIf you are running on the \nTensorFlow\n or \nCNTK\n backends, your code will automatically run on GPU if any available GPU is detected.\n\n\nIf you are running on the \nTheano\n backend, you can use one of the following methods:\n\n\nMethod 1\n: use Theano flags.\n\n\nTHEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py\n\n\n\n\nThe name 'gpu' might have to be changed depending on your device's identifier (e.g. \ngpu0\n, \ngpu1\n, etc).\n\n\nMethod 2\n: set up your \n.theanorc\n: \nInstructions\n\n\nMethod 3\n: manually set \ntheano.config.device\n, \ntheano.config.floatX\n at the beginning of your code:\n\n\nimport theano\ntheano.config.device = 'gpu'\ntheano.config.floatX = 'float32'\n\n\n\n\n\n\nHow can I run a Keras model on multiple GPUs?\n\n\nWe recommend doing so using the \nTensorFlow\n backend. There are two ways to run a single model on multiple GPUs: \ndata parallelism\n and \ndevice parallelism\n.\n\n\nIn most cases, what you need is most likely data parallelism.\n\n\nData parallelism\n\n\nData parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data.\nKeras has a built-in utility, \nkeras.utils.multi_gpu_model\n, which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.\n\n\nFor more information, see the documentation for \nmulti_gpu_model\n. Here is a quick example:\n\n\nfrom keras.utils import multi_gpu_model\n\n# Replicates `model` on 8 GPUs.\n# This assumes that your machine has 8 available GPUs.\nparallel_model = multi_gpu_model(model, gpus=8)\nparallel_model.compile(loss='categorical_crossentropy',\n                       optimizer='rmsprop')\n\n# This `fit` call will be distributed on 8 GPUs.\n# Since the batch size is 256, each GPU will process 32 samples.\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n\n\n\n\nDevice parallelism\n\n\nDevice parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.\n\n\nThis can be achieved by using TensorFlow device scopes. Here is a quick example:\n\n\n# Model where a shared LSTM is used to encode two different sequences in parallel\ninput_a = keras.Input(shape=(140, 256))\ninput_b = keras.Input(shape=(140, 256))\n\nshared_lstm = keras.layers.LSTM(64)\n\n# Process the first sequence on one GPU\nwith tf.device_scope('/gpu:0'):\n    encoded_a = shared_lstm(tweet_a)\n# Process the next sequence on another GPU\nwith tf.device_scope('/gpu:1'):\n    encoded_b = shared_lstm(tweet_b)\n\n# Concatenate results on CPU\nwith tf.device_scope('/cpu:0'):\n    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],\n                                             axis=-1)\n\n\n\n\n\n\nWhat does \"sample\", \"batch\", \"epoch\" mean?\n\n\nBelow are some common definitions that are necessary to know and understand to correctly utilize Keras:\n\n\n\n\nSample\n: one element of a dataset.\n\n\nExample:\n one image is a \nsample\n in a convolutional network\n\n\nExample:\n one audio file is a \nsample\n for a speech recognition model\n\n\nBatch\n: a set of \nN\n samples. The samples in a \nbatch\n are processed independently, in parallel. If training, a batch results in only one update to the model.\n\n\nA \nbatch\n generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\n\n\nEpoch\n: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n\n\nWhen using \nevaluation_data\n or \nevaluation_split\n with the \nfit\n method of Keras models, evaluation will be run at the end of every \nepoch\n.\n\n\nWithin Keras, there is the ability to add \ncallbacks\n specifically designed to be run at the end of an \nepoch\n. Examples of these are learning rate changes and model checkpointing (saving).\n\n\n\n\n\n\nHow can I save a Keras model?\n\n\nSaving/loading whole models (architecture + weights + optimizer state)\n\n\nIt is not recommended to use pickle or cPickle to save a Keras model.\n\n\nYou can use \nmodel.save(filepath)\n to save a Keras model into a single HDF5 file which will contain:\n\n\n\n\nthe architecture of the model, allowing to re-create the model\n\n\nthe weights of the model\n\n\nthe training configuration (loss, optimizer)\n\n\nthe state of the optimizer, allowing to resume training exactly where you left off.\n\n\n\n\nYou can then use \nkeras.models.load_model(filepath)\n to reinstantiate your model.\n\nload_model\n will also take care of compiling the model using the saved training configuration\n(unless the model was never compiled in the first place).\n\n\nExample:\n\n\nfrom keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')\n\n\n\n\nSaving/loading only a model's architecture\n\n\nIf you only need to save the \narchitecture of a model\n, and not its weights or its training configuration, you can do:\n\n\n# save as JSON\njson_string = model.to_json()\n\n# save as YAML\nyaml_string = model.to_yaml()\n\n\n\n\nThe generated JSON / YAML files are human-readable and can be manually edited if needed.\n\n\nYou can then build a fresh model from this data:\n\n\n# model reconstruction from JSON:\nfrom keras.models import model_from_json\nmodel = model_from_json(json_string)\n\n# model reconstruction from YAML\nfrom keras.models import model_from_yaml\nmodel = model_from_yaml(yaml_string)\n\n\n\n\nSaving/loading only a model's weights\n\n\nIf you need to save the \nweights of a model\n, you can do so in HDF5 with the code below.\n\n\nNote that you will first need to install HDF5 and the Python library h5py, which do not come bundled with Keras.\n\n\nmodel.save_weights('my_model_weights.h5')\n\n\n\n\nAssuming you have code for instantiating your model, you can then load the weights you saved into a model with the \nsame\n architecture:\n\n\nmodel.load_weights('my_model_weights.h5')\n\n\n\n\nIf you need to load weights into a \ndifferent\n architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by \nlayer name\n:\n\n\nmodel.load_weights('my_model_weights.h5', by_name=True)\n\n\n\n\nFor example:\n\n\n\"\"\"\nAssuming the original model looks like this:\n    model = Sequential()\n    model.add(Dense(2, input_dim=3, name='dense_1'))\n    model.add(Dense(3, name='dense_2'))\n    ...\n    model.save_weights(fname)\n\"\"\"\n\n# new model\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=3, name='dense_1'))  # will be loaded\nmodel.add(Dense(10, name='new_dense'))  # will not be loaded\n\n# load weights from first model; will only affect the first layer, dense_1.\nmodel.load_weights(fname, by_name=True)\n\n\n\n\nHandling custom layers (or other custom objects) in saved models\n\n\nIf the model you want to load includes custom layers or other custom classes or functions, \nyou can pass them to the loading mechanism via the \ncustom_objects\n argument: \n\n\nfrom keras.models import load_model\n# Assuming your model includes instance of an \"AttentionLayer\" class\nmodel = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n\n\n\n\nAlternatively, you can use a \ncustom object scope\n:\n\n\nfrom keras.utils import CustomObjectScope\n\nwith CustomObjectScope({'AttentionLayer': AttentionLayer}):\n    model = load_model('my_model.h5')\n\n\n\n\nCustom objects handling works the same way for \nload_model\n, \nmodel_from_json\n, \nmodel_from_yaml\n:\n\n\nfrom keras.models import model_from_json\nmodel = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})\n\n\n\n\n\n\nWhy is the training loss much higher than the testing loss?\n\n\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\n\n\nBesides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n\n\n\n\nHow can I obtain the output of an intermediate layer?\n\n\nOne simple way is to create a new \nModel\n that will output the layers that you are interested in:\n\n\nfrom keras.models import Model\n\nmodel = ...  # create the original model\n\nlayer_name = 'my_layer'\nintermediate_layer_model = Model(inputs=model.input,\n                                 outputs=model.get_layer(layer_name).output)\nintermediate_output = intermediate_layer_model.predict(data)\n\n\n\n\nAlternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:\n\n\nfrom keras import backend as K\n\n# with a Sequential model\nget_3rd_layer_output = K.function([model.layers[0].input],\n                                  [model.layers[3].output])\nlayer_output = get_3rd_layer_output([x])[0]\n\n\n\n\nSimilarly, you could build a Theano and TensorFlow function directly.\n\n\nNote that if your model has a different behavior in training and testing phase (e.g. if it uses \nDropout\n, \nBatchNormalization\n, etc.), you will need\nto pass the learning phase flag to your function:\n\n\nget_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n                                  [model.layers[3].output])\n\n# output in test mode = 0\nlayer_output = get_3rd_layer_output([x, 0])[0]\n\n# output in train mode = 1\nlayer_output = get_3rd_layer_output([x, 1])[0]\n\n\n\n\n\n\nHow can I use Keras with datasets that don't fit in memory?\n\n\nYou can do batch training using \nmodel.train_on_batch(x, y)\n and \nmodel.test_on_batch(x, y)\n. See the \nmodels documentation\n.\n\n\nAlternatively, you can write a generator that yields batches of training data and use the method \nmodel.fit_generator(data_generator, steps_per_epoch, epochs)\n.\n\n\nYou can see batch training in action in our \nCIFAR10 example\n.\n\n\n\n\nHow can I interrupt training when the validation loss isn't decreasing anymore?\n\n\nYou can use an \nEarlyStopping\n callback:\n\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\nmodel.fit(x, y, validation_split=0.2, callbacks=[early_stopping])\n\n\n\n\nFind out more in the \ncallbacks documentation\n.\n\n\n\n\nHow is the validation split computed?\n\n\nIf you set the \nvalidation_split\n argument in \nmodel.fit\n to e.g. 0.1, then the validation data used will be the \nlast 10%\n of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the \nlast\n x% of samples in the input you passed.\n\n\nThe same validation set is used for all epochs (within a same call to \nfit\n).\n\n\n\n\nIs the data shuffled during training?\n\n\nYes, if the \nshuffle\n argument in \nmodel.fit\n is set to \nTrue\n (which is the default), the training data will be randomly shuffled at each epoch.\n\n\nValidation data is never shuffled.\n\n\n\n\nHow can I record the training / validation loss / accuracy at each epoch?\n\n\nThe \nmodel.fit\n method returns an \nHistory\n callback, which has a \nhistory\n attribute containing the lists of successive losses and other metrics.\n\n\nhist = model.fit(x, y, validation_split=0.2)\nprint(hist.history)\n\n\n\n\n\n\nHow can I \"freeze\" Keras layers?\n\n\nTo \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.\n\n\nYou can pass a \ntrainable\n argument (boolean) to a layer constructor to set a layer to be non-trainable:\n\n\nfrozen_layer = Dense(32, trainable=False)\n\n\n\n\nAdditionally, you can set the \ntrainable\n property of a layer to \nTrue\n or \nFalse\n after instantiation. For this to take effect, you will need to call \ncompile()\n on your model after modifying the \ntrainable\n property. Here's an example:\n\n\nx = Input(shape=(32,))\nlayer = Dense(32)\nlayer.trainable = False\ny = layer(x)\n\nfrozen_model = Model(x, y)\n# in the model below, the weights of `layer` will not be updated during training\nfrozen_model.compile(optimizer='rmsprop', loss='mse')\n\nlayer.trainable = True\ntrainable_model = Model(x, y)\n# with this model the weights of the layer will be updated during training\n# (which will also affect the above model since it uses the same layer instance)\ntrainable_model.compile(optimizer='rmsprop', loss='mse')\n\nfrozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\ntrainable_model.fit(data, labels)  # this updates the weights of `layer`\n\n\n\n\n\n\nHow can I use stateful RNNs?\n\n\nMaking a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n\n\nWhen using stateful RNNs, it is therefore assumed that:\n\n\n\n\nall batches have the same number of samples\n\n\nIf \nx1\n and \nx2\n are successive batches of samples, then \nx2[i]\n is the follow-up sequence to \nx1[i]\n, for every \ni\n.\n\n\n\n\nTo use statefulness in RNNs, you need to:\n\n\n\n\nexplicitly specify the batch size you are using, by passing a \nbatch_size\n argument to the first layer in your model. E.g. \nbatch_size=32\n for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.\n\n\nset \nstateful=True\n in your RNN layer(s).\n\n\nspecify \nshuffle=False\n when calling fit().\n\n\n\n\nTo reset the states accumulated:\n\n\n\n\nuse \nmodel.reset_states()\n to reset the states of all layers in the model\n\n\nuse \nlayer.reset_states()\n to reset the states of a specific stateful RNN layer\n\n\n\n\nExample:\n\n\n\nx  # this is our input data, of shape (32, 21, 16)\n# we will feed it to our model in sequences of length 10\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True))\nmodel.add(Dense(16, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# we train the network to predict the 11th timestep given the first 10:\nmodel.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16)))\n\n# the state of the network has changed. We can feed the follow-up sequences:\nmodel.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16)))\n\n# let's reset the states of the LSTM layer:\nmodel.reset_states()\n\n# another way to do it in this case:\nmodel.layers[0].reset_states()\n\n\n\n\nNotes that the methods \npredict\n, \nfit\n, \ntrain_on_batch\n, \npredict_classes\n, etc. will \nall\n update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.\n\n\n\n\nHow can I remove a layer from a Sequential model?\n\n\nYou can remove the last added layer in a Sequential model by calling \n.pop()\n:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=784))\nmodel.add(Dense(32, activation='relu'))\n\nprint(len(model.layers))  # \"2\"\n\nmodel.pop()\nprint(len(model.layers))  # \"1\"\n\n\n\n\n\n\nHow can I use pre-trained models in Keras?\n\n\nCode and pre-trained weights are available for the following image classification models:\n\n\n\n\nXception\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInception v3\n\n\nInception-ResNet v2\n\n\nMobileNet v1\n\n\n\n\nThey can be imported from the module \nkeras.applications\n:\n\n\nfrom keras.applications.xception import Xception\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.mobilenet import MobileNet\n\nmodel = VGG16(weights='imagenet', include_top=True)\n\n\n\n\nFor a few simple usage examples, see \nthe documentation for the Applications module\n.\n\n\nFor a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see \nthis blog post\n.\n\n\nThe VGG16 model is also the basis for several Keras example scripts:\n\n\n\n\nStyle transfer\n\n\nFeature visualization\n\n\nDeep dream\n\n\n\n\n\n\nHow can I use HDF5 inputs with Keras?\n\n\nYou can use the \nHDF5Matrix\n class from \nkeras.utils.io_utils\n. See \nthe HDF5Matrix documentation\n for details.\n\n\nYou can also directly use a HDF5 dataset:\n\n\nimport h5py\nwith h5py.File('input/file.hdf5', 'r') as f:\n    x_data = f['x_data']\n    model.predict(x_data)\n\n\n\n\n\n\nWhere is the Keras configuration file stored?\n\n\nThe default directory where all Keras data is stored is:\n\n\n$HOME/.keras/\n\n\n\n\nNote that Windows users should replace \n$HOME\n with \n%USERPROFILE%\n.\nIn case Keras cannot create the above directory (e.g. due to permission issues), \n/tmp/.keras/\n is used as a backup.\n\n\nThe Keras configuration file is a JSON file stored at \n$HOME/.keras/keras.json\n. The default configuration file looks like this:\n\n\n{\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}\n\n\n\n\nIt contains the following fields:\n\n\n\n\nThe image data format to be used as default by image processing layers and utilities (either \nchannels_last\n or \nchannels_first\n).\n\n\nThe \nepsilon\n numerical fuzz factor to be used to prevent division by zero in some operations.\n\n\nThe default float data type.\n\n\nThe default backend. See the \nbackend documentation\n.\n\n\n\n\nLikewise, cached dataset files, such as those downloaded with \nget_file()\n, are stored by default in \n$HOME/.keras/datasets/\n.\n\n\n\n\nHow can I obtain reproducible results using Keras during development?\n\n\nDuring development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample.  The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment.\n\n\nimport numpy as np\nimport tensorflow as tf\nimport random as rn\n\n# The below is necessary in Python 3.2.3 onwards to\n# have reproducible behavior for certain hash-based operations.\n# See these references for further details:\n# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\n\n# The below is necessary for starting Numpy generated random numbers\n# in a well-defined initial state.\n\nnp.random.seed(42)\n\n# The below is necessary for starting core Python generated random numbers\n# in a well-defined state.\n\nrn.seed(12345)\n\n# Force TensorFlow to use single thread.\n# Multiple threads are a potential source of\n# non-reproducible results.\n# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n\nfrom keras import backend as K\n\n# The below tf.set_random_seed() will make random number generation\n# in the TensorFlow backend have a well-defined initial state.\n# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n\ntf.set_random_seed(1234)\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\n# Rest of code follows ...",
            "title": "FAQ"
        },
        {
            "location": "/getting-started/faq/#keras-faq-frequently-asked-keras-questions",
            "text": "How should I cite Keras?  How can I run Keras on GPU?  How can I run a Keras model on multiple GPUs?  What does \"sample\", \"batch\", \"epoch\" mean?  How can I save a Keras model?  Why is the training loss much higher than the testing loss?  How can I obtain the output of an intermediate layer?  How can I use Keras with datasets that don't fit in memory?  How can I interrupt training when the validation loss isn't decreasing anymore?  How is the validation split computed?  Is the data shuffled during training?  How can I record the training / validation loss / accuracy at each epoch?  How can I \"freeze\" layers?  How can I use stateful RNNs?  How can I remove a layer from a Sequential model?  How can I use pre-trained models in Keras?  How can I use HDF5 inputs with Keras?  Where is the Keras configuration file stored?  How can I obtain reproducible results using Keras during development?",
            "title": "Keras FAQ: Frequently Asked Keras Questions"
        },
        {
            "location": "/getting-started/faq/#how-should-i-cite-keras",
            "text": "Please cite Keras in your publications if it helps your research. Here is an example BibTeX entry:  @misc{chollet2015keras,\n  title={Keras},\n  author={Chollet, Fran\\c{c}ois and others},\n  year={2015},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/keras-team/keras}},\n}",
            "title": "How should I cite Keras?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-run-keras-on-gpu",
            "text": "If you are running on the  TensorFlow  or  CNTK  backends, your code will automatically run on GPU if any available GPU is detected.  If you are running on the  Theano  backend, you can use one of the following methods:  Method 1 : use Theano flags.  THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py  The name 'gpu' might have to be changed depending on your device's identifier (e.g.  gpu0 ,  gpu1 , etc).  Method 2 : set up your  .theanorc :  Instructions  Method 3 : manually set  theano.config.device ,  theano.config.floatX  at the beginning of your code:  import theano\ntheano.config.device = 'gpu'\ntheano.config.floatX = 'float32'",
            "title": "How can I run Keras on GPU?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus",
            "text": "We recommend doing so using the  TensorFlow  backend. There are two ways to run a single model on multiple GPUs:  data parallelism  and  device parallelism .  In most cases, what you need is most likely data parallelism.",
            "title": "How can I run a Keras model on multiple GPUs?"
        },
        {
            "location": "/getting-started/faq/#data-parallelism",
            "text": "Data parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data.\nKeras has a built-in utility,  keras.utils.multi_gpu_model , which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.  For more information, see the documentation for  multi_gpu_model . Here is a quick example:  from keras.utils import multi_gpu_model\n\n# Replicates `model` on 8 GPUs.\n# This assumes that your machine has 8 available GPUs.\nparallel_model = multi_gpu_model(model, gpus=8)\nparallel_model.compile(loss='categorical_crossentropy',\n                       optimizer='rmsprop')\n\n# This `fit` call will be distributed on 8 GPUs.\n# Since the batch size is 256, each GPU will process 32 samples.\nparallel_model.fit(x, y, epochs=20, batch_size=256)",
            "title": "Data parallelism"
        },
        {
            "location": "/getting-started/faq/#device-parallelism",
            "text": "Device parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.  This can be achieved by using TensorFlow device scopes. Here is a quick example:  # Model where a shared LSTM is used to encode two different sequences in parallel\ninput_a = keras.Input(shape=(140, 256))\ninput_b = keras.Input(shape=(140, 256))\n\nshared_lstm = keras.layers.LSTM(64)\n\n# Process the first sequence on one GPU\nwith tf.device_scope('/gpu:0'):\n    encoded_a = shared_lstm(tweet_a)\n# Process the next sequence on another GPU\nwith tf.device_scope('/gpu:1'):\n    encoded_b = shared_lstm(tweet_b)\n\n# Concatenate results on CPU\nwith tf.device_scope('/cpu:0'):\n    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],\n                                             axis=-1)",
            "title": "Device parallelism"
        },
        {
            "location": "/getting-started/faq/#what-does-sample-batch-epoch-mean",
            "text": "Below are some common definitions that are necessary to know and understand to correctly utilize Keras:   Sample : one element of a dataset.  Example:  one image is a  sample  in a convolutional network  Example:  one audio file is a  sample  for a speech recognition model  Batch : a set of  N  samples. The samples in a  batch  are processed independently, in parallel. If training, a batch results in only one update to the model.  A  batch  generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).  Epoch : an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.  When using  evaluation_data  or  evaluation_split  with the  fit  method of Keras models, evaluation will be run at the end of every  epoch .  Within Keras, there is the ability to add  callbacks  specifically designed to be run at the end of an  epoch . Examples of these are learning rate changes and model checkpointing (saving).",
            "title": "What does \"sample\", \"batch\", \"epoch\" mean?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-save-a-keras-model",
            "text": "",
            "title": "How can I save a Keras model?"
        },
        {
            "location": "/getting-started/faq/#savingloading-whole-models-architecture-weights-optimizer-state",
            "text": "It is not recommended to use pickle or cPickle to save a Keras model.  You can use  model.save(filepath)  to save a Keras model into a single HDF5 file which will contain:   the architecture of the model, allowing to re-create the model  the weights of the model  the training configuration (loss, optimizer)  the state of the optimizer, allowing to resume training exactly where you left off.   You can then use  keras.models.load_model(filepath)  to reinstantiate your model. load_model  will also take care of compiling the model using the saved training configuration\n(unless the model was never compiled in the first place).  Example:  from keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')",
            "title": "Saving/loading whole models (architecture + weights + optimizer state)"
        },
        {
            "location": "/getting-started/faq/#savingloading-only-a-models-architecture",
            "text": "If you only need to save the  architecture of a model , and not its weights or its training configuration, you can do:  # save as JSON\njson_string = model.to_json()\n\n# save as YAML\nyaml_string = model.to_yaml()  The generated JSON / YAML files are human-readable and can be manually edited if needed.  You can then build a fresh model from this data:  # model reconstruction from JSON:\nfrom keras.models import model_from_json\nmodel = model_from_json(json_string)\n\n# model reconstruction from YAML\nfrom keras.models import model_from_yaml\nmodel = model_from_yaml(yaml_string)",
            "title": "Saving/loading only a model's architecture"
        },
        {
            "location": "/getting-started/faq/#savingloading-only-a-models-weights",
            "text": "If you need to save the  weights of a model , you can do so in HDF5 with the code below.  Note that you will first need to install HDF5 and the Python library h5py, which do not come bundled with Keras.  model.save_weights('my_model_weights.h5')  Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the  same  architecture:  model.load_weights('my_model_weights.h5')  If you need to load weights into a  different  architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by  layer name :  model.load_weights('my_model_weights.h5', by_name=True)  For example:  \"\"\"\nAssuming the original model looks like this:\n    model = Sequential()\n    model.add(Dense(2, input_dim=3, name='dense_1'))\n    model.add(Dense(3, name='dense_2'))\n    ...\n    model.save_weights(fname)\n\"\"\"\n\n# new model\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=3, name='dense_1'))  # will be loaded\nmodel.add(Dense(10, name='new_dense'))  # will not be loaded\n\n# load weights from first model; will only affect the first layer, dense_1.\nmodel.load_weights(fname, by_name=True)",
            "title": "Saving/loading only a model's weights"
        },
        {
            "location": "/getting-started/faq/#handling-custom-layers-or-other-custom-objects-in-saved-models",
            "text": "If the model you want to load includes custom layers or other custom classes or functions, \nyou can pass them to the loading mechanism via the  custom_objects  argument:   from keras.models import load_model\n# Assuming your model includes instance of an \"AttentionLayer\" class\nmodel = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer})  Alternatively, you can use a  custom object scope :  from keras.utils import CustomObjectScope\n\nwith CustomObjectScope({'AttentionLayer': AttentionLayer}):\n    model = load_model('my_model.h5')  Custom objects handling works the same way for  load_model ,  model_from_json ,  model_from_yaml :  from keras.models import model_from_json\nmodel = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})",
            "title": "Handling custom layers (or other custom objects) in saved models"
        },
        {
            "location": "/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss",
            "text": "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.  Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.",
            "title": "Why is the training loss much higher than the testing loss?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer",
            "text": "One simple way is to create a new  Model  that will output the layers that you are interested in:  from keras.models import Model\n\nmodel = ...  # create the original model\n\nlayer_name = 'my_layer'\nintermediate_layer_model = Model(inputs=model.input,\n                                 outputs=model.get_layer(layer_name).output)\nintermediate_output = intermediate_layer_model.predict(data)  Alternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:  from keras import backend as K\n\n# with a Sequential model\nget_3rd_layer_output = K.function([model.layers[0].input],\n                                  [model.layers[3].output])\nlayer_output = get_3rd_layer_output([x])[0]  Similarly, you could build a Theano and TensorFlow function directly.  Note that if your model has a different behavior in training and testing phase (e.g. if it uses  Dropout ,  BatchNormalization , etc.), you will need\nto pass the learning phase flag to your function:  get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n                                  [model.layers[3].output])\n\n# output in test mode = 0\nlayer_output = get_3rd_layer_output([x, 0])[0]\n\n# output in train mode = 1\nlayer_output = get_3rd_layer_output([x, 1])[0]",
            "title": "How can I obtain the output of an intermediate layer?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory",
            "text": "You can do batch training using  model.train_on_batch(x, y)  and  model.test_on_batch(x, y) . See the  models documentation .  Alternatively, you can write a generator that yields batches of training data and use the method  model.fit_generator(data_generator, steps_per_epoch, epochs) .  You can see batch training in action in our  CIFAR10 example .",
            "title": "How can I use Keras with datasets that don't fit in memory?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore",
            "text": "You can use an  EarlyStopping  callback:  from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\nmodel.fit(x, y, validation_split=0.2, callbacks=[early_stopping])  Find out more in the  callbacks documentation .",
            "title": "How can I interrupt training when the validation loss isn't decreasing anymore?"
        },
        {
            "location": "/getting-started/faq/#how-is-the-validation-split-computed",
            "text": "If you set the  validation_split  argument in  model.fit  to e.g. 0.1, then the validation data used will be the  last 10%  of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the  last  x% of samples in the input you passed.  The same validation set is used for all epochs (within a same call to  fit ).",
            "title": "How is the validation split computed?"
        },
        {
            "location": "/getting-started/faq/#is-the-data-shuffled-during-training",
            "text": "Yes, if the  shuffle  argument in  model.fit  is set to  True  (which is the default), the training data will be randomly shuffled at each epoch.  Validation data is never shuffled.",
            "title": "Is the data shuffled during training?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch",
            "text": "The  model.fit  method returns an  History  callback, which has a  history  attribute containing the lists of successive losses and other metrics.  hist = model.fit(x, y, validation_split=0.2)\nprint(hist.history)",
            "title": "How can I record the training / validation loss / accuracy at each epoch?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-freeze-keras-layers",
            "text": "To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.  You can pass a  trainable  argument (boolean) to a layer constructor to set a layer to be non-trainable:  frozen_layer = Dense(32, trainable=False)  Additionally, you can set the  trainable  property of a layer to  True  or  False  after instantiation. For this to take effect, you will need to call  compile()  on your model after modifying the  trainable  property. Here's an example:  x = Input(shape=(32,))\nlayer = Dense(32)\nlayer.trainable = False\ny = layer(x)\n\nfrozen_model = Model(x, y)\n# in the model below, the weights of `layer` will not be updated during training\nfrozen_model.compile(optimizer='rmsprop', loss='mse')\n\nlayer.trainable = True\ntrainable_model = Model(x, y)\n# with this model the weights of the layer will be updated during training\n# (which will also affect the above model since it uses the same layer instance)\ntrainable_model.compile(optimizer='rmsprop', loss='mse')\n\nfrozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\ntrainable_model.fit(data, labels)  # this updates the weights of `layer`",
            "title": "How can I \"freeze\" Keras layers?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-use-stateful-rnns",
            "text": "Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.  When using stateful RNNs, it is therefore assumed that:   all batches have the same number of samples  If  x1  and  x2  are successive batches of samples, then  x2[i]  is the follow-up sequence to  x1[i] , for every  i .   To use statefulness in RNNs, you need to:   explicitly specify the batch size you are using, by passing a  batch_size  argument to the first layer in your model. E.g.  batch_size=32  for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.  set  stateful=True  in your RNN layer(s).  specify  shuffle=False  when calling fit().   To reset the states accumulated:   use  model.reset_states()  to reset the states of all layers in the model  use  layer.reset_states()  to reset the states of a specific stateful RNN layer   Example:  \nx  # this is our input data, of shape (32, 21, 16)\n# we will feed it to our model in sequences of length 10\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True))\nmodel.add(Dense(16, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# we train the network to predict the 11th timestep given the first 10:\nmodel.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16)))\n\n# the state of the network has changed. We can feed the follow-up sequences:\nmodel.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16)))\n\n# let's reset the states of the LSTM layer:\nmodel.reset_states()\n\n# another way to do it in this case:\nmodel.layers[0].reset_states()  Notes that the methods  predict ,  fit ,  train_on_batch ,  predict_classes , etc. will  all  update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.",
            "title": "How can I use stateful RNNs?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-remove-a-layer-from-a-sequential-model",
            "text": "You can remove the last added layer in a Sequential model by calling  .pop() :  model = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=784))\nmodel.add(Dense(32, activation='relu'))\n\nprint(len(model.layers))  # \"2\"\n\nmodel.pop()\nprint(len(model.layers))  # \"1\"",
            "title": "How can I remove a layer from a Sequential model?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras",
            "text": "Code and pre-trained weights are available for the following image classification models:   Xception  VGG16  VGG19  ResNet50  Inception v3  Inception-ResNet v2  MobileNet v1   They can be imported from the module  keras.applications :  from keras.applications.xception import Xception\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.mobilenet import MobileNet\n\nmodel = VGG16(weights='imagenet', include_top=True)  For a few simple usage examples, see  the documentation for the Applications module .  For a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see  this blog post .  The VGG16 model is also the basis for several Keras example scripts:   Style transfer  Feature visualization  Deep dream",
            "title": "How can I use pre-trained models in Keras?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-use-hdf5-inputs-with-keras",
            "text": "You can use the  HDF5Matrix  class from  keras.utils.io_utils . See  the HDF5Matrix documentation  for details.  You can also directly use a HDF5 dataset:  import h5py\nwith h5py.File('input/file.hdf5', 'r') as f:\n    x_data = f['x_data']\n    model.predict(x_data)",
            "title": "How can I use HDF5 inputs with Keras?"
        },
        {
            "location": "/getting-started/faq/#where-is-the-keras-configuration-file-stored",
            "text": "The default directory where all Keras data is stored is:  $HOME/.keras/  Note that Windows users should replace  $HOME  with  %USERPROFILE% .\nIn case Keras cannot create the above directory (e.g. due to permission issues),  /tmp/.keras/  is used as a backup.  The Keras configuration file is a JSON file stored at  $HOME/.keras/keras.json . The default configuration file looks like this:  {\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}  It contains the following fields:   The image data format to be used as default by image processing layers and utilities (either  channels_last  or  channels_first ).  The  epsilon  numerical fuzz factor to be used to prevent division by zero in some operations.  The default float data type.  The default backend. See the  backend documentation .   Likewise, cached dataset files, such as those downloaded with  get_file() , are stored by default in  $HOME/.keras/datasets/ .",
            "title": "Where is the Keras configuration file stored?"
        },
        {
            "location": "/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development",
            "text": "During development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample.  The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment.  import numpy as np\nimport tensorflow as tf\nimport random as rn\n\n# The below is necessary in Python 3.2.3 onwards to\n# have reproducible behavior for certain hash-based operations.\n# See these references for further details:\n# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\n\n# The below is necessary for starting Numpy generated random numbers\n# in a well-defined initial state.\n\nnp.random.seed(42)\n\n# The below is necessary for starting core Python generated random numbers\n# in a well-defined state.\n\nrn.seed(12345)\n\n# Force TensorFlow to use single thread.\n# Multiple threads are a potential source of\n# non-reproducible results.\n# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n\nfrom keras import backend as K\n\n# The below tf.set_random_seed() will make random number generation\n# in the TensorFlow backend have a well-defined initial state.\n# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n\ntf.set_random_seed(1234)\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\n# Rest of code follows ...",
            "title": "How can I obtain reproducible results using Keras during development?"
        },
        {
            "location": "/models/about-keras-models/",
            "text": "About Keras models\n\n\nThere are two types of models available in Keras: \nthe Sequential model\n and \nthe Model class used with functional API\n.\n\n\nThese models have a number of methods in common:\n\n\n\n\nmodel.summary()\n: prints a summary representation of your model. Shortcut for \nutils.print_summary\n\n\nmodel.get_config()\n: returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via:\n\n\n\n\nconfig = model.get_config()\nmodel = Model.from_config(config)\n# or, for Sequential:\nmodel = Sequential.from_config(config)\n\n\n\n\n\n\nmodel.get_weights()\n: returns a list of all weight tensors in the model, as Numpy arrays.\n\n\nmodel.set_weights(weights)\n: sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by \nget_weights()\n.\n\n\nmodel.to_json()\n: returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via:\n\n\n\n\nfrom keras.models import model_from_json\n\njson_string = model.to_json()\nmodel = model_from_json(json_string)\n\n\n\n\n\n\nmodel.to_yaml()\n: returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via:\n\n\n\n\nfrom keras.models import model_from_yaml\n\nyaml_string = model.to_yaml()\nmodel = model_from_yaml(yaml_string)\n\n\n\n\n\n\nmodel.save_weights(filepath)\n: saves the weights of the model as a HDF5 file.\n\n\nmodel.load_weights(filepath, by_name=False)\n: loads the weights of the model from a HDF5 file (created by \nsave_weights\n). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use \nby_name=True\n to load only those layers with the same name.",
            "title": "About Keras models"
        },
        {
            "location": "/models/about-keras-models/#about-keras-models",
            "text": "There are two types of models available in Keras:  the Sequential model  and  the Model class used with functional API .  These models have a number of methods in common:   model.summary() : prints a summary representation of your model. Shortcut for  utils.print_summary  model.get_config() : returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via:   config = model.get_config()\nmodel = Model.from_config(config)\n# or, for Sequential:\nmodel = Sequential.from_config(config)   model.get_weights() : returns a list of all weight tensors in the model, as Numpy arrays.  model.set_weights(weights) : sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by  get_weights() .  model.to_json() : returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via:   from keras.models import model_from_json\n\njson_string = model.to_json()\nmodel = model_from_json(json_string)   model.to_yaml() : returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via:   from keras.models import model_from_yaml\n\nyaml_string = model.to_yaml()\nmodel = model_from_yaml(yaml_string)   model.save_weights(filepath) : saves the weights of the model as a HDF5 file.  model.load_weights(filepath, by_name=False) : loads the weights of the model from a HDF5 file (created by  save_weights ). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use  by_name=True  to load only those layers with the same name.",
            "title": "About Keras models"
        },
        {
            "location": "/models/sequential/",
            "text": "The Sequential model API\n\n\nTo get started, read \nthis guide to the Keras Sequential model\n.\n\n\nUseful attributes of Model\n\n\n\n\nmodel.layers\n is a list of the layers added to the model.\n\n\n\n\n\n\nSequential model methods\n\n\ncompile\n\n\ncompile(self, optimizer, loss, metrics=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n\n\n\n\nConfigures the model for training.\n\n\nArguments\n\n\n\n\noptimizer\n: String (name of optimizer) or optimizer object.\nSee \noptimizers\n.\n\n\nloss\n: String (name of objective function) or objective function.\nSee \nlosses\n.\nIf the model has multiple outputs, you can use a different loss\non each output by passing a dictionary or a list of losses.\nThe loss value that will be minimized by the model\nwill then be the sum of all individual losses.\n\n\nmetrics\n: List of metrics to be evaluated by the model\nduring training and testing.\nTypically you will use \nmetrics=['accuracy']\n.\nTo specify different metrics for different outputs of a\nmulti-output model, you could also pass a dictionary,\nsuch as \nmetrics={'output_a': 'accuracy'}\n.\n\n\nsample_weight_mode\n: If you need to do timestep-wise\nsample weighting (2D weights), set this to \n\"temporal\"\n.\n\nNone\n defaults to sample-wise weights (1D).\nIf the model has multiple outputs, you can use a different\n\nsample_weight_mode\n on each output by passing a\ndictionary or a list of modes.\n\n\nweighted_metrics\n: List of metrics to be evaluated and weighted\nby sample_weight or class_weight during training and testing.\n\n\ntarget_tensors\n: By default, Keras will create a placeholder for the\nmodel's target, which will be fed with the target data during\ntraining. If instead you would like to use your own\ntarget tensor (in turn, Keras will not expect external\nNumpy data for these targets at training time), you\ncan specify them via the \ntarget_tensors\n argument.\nIt should be a single tensor\n(for a single-output \nSequential\n model).\n\n\n**kwargs\n: When using the Theano/CNTK backends, these arguments\nare passed into \nK.function\n.\nWhen using the TensorFlow backend,\nthese arguments are passed into \ntf.Session.run\n.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid arguments for\n\noptimizer\n, \nloss\n, \nmetrics\n or \nsample_weight_mode\n.\n\n\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(500,)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n\n\n\nfit\n\n\nfit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n\n\n\n\nTrains the model for a fixed number of epochs (iterations on a dataset).\n\n\nArguments\n\n\n\n\nx\n: Numpy array of training data.\nIf the input layer in the model is named, you can also pass a\ndictionary mapping the input name to a Numpy array.\n\nx\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\ny\n: Numpy array of target (label) data.\nIf the output layer in the model is named, you can also pass a\ndictionary mapping the output name to a Numpy array.\n\ny\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\nbatch_size\n: Integer or \nNone\n.\nNumber of samples per gradient update.\nIf unspecified, it will default to 32.\n\n\nepochs\n: Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire \nx\n and \ny\n\ndata provided.\nNote that in conjunction with \ninitial_epoch\n,\n\nepochs\n is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by \nepochs\n, but merely until the epoch\nof index \nepochs\n is reached.\n\n\nverbose\n: 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n\ncallbacks\n: List of \nkeras.callbacks.Callback\n instances.\nList of callbacks to apply during training.\nSee \ncallbacks\n.\n\n\nvalidation_split\n: Float between 0 and 1.\nFraction of the training data to be used as validation data.\nThe model will set apart this fraction of the training data,\nwill not train on it, and will evaluate\nthe loss and any model metrics\non this data at the end of each epoch.\nThe validation data is selected from the last samples\nin the \nx\n and \ny\n data provided, before shuffling.\n\n\nvalidation_data\n: tuple \n(x_val, y_val)\n or tuple\n\n(x_val, y_val, val_sample_weights)\n on which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data.\nThis will override \nvalidation_split\n.\n\n\nshuffle\n: Boolean (whether to shuffle the training data\nbefore each epoch) or str (for 'batch').\n'batch' is a special option for dealing with the\nlimitations of HDF5 data; it shuffles in batch-sized chunks.\nHas no effect when \nsteps_per_epoch\n is not \nNone\n.\n\n\nclass_weight\n: Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only).\nThis can be useful to tell the model to\n\"pay more attention\" to samples from\nan under-represented class.\n\n\nsample_weight\n: Optional Numpy array of weights for\nthe training samples, used for weighting the loss function\n(during training only). You can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape\n\n(samples, sequence_length)\n,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\n\nsample_weight_mode=\"temporal\"\n in \ncompile()\n.\n\n\ninitial_epoch\n: Epoch at which to start training\n(useful for resuming a previous training run).\n\n\nsteps_per_epoch\n: Total number of steps (batches of samples)\nbefore declaring one epoch finished and starting the\nnext epoch. When training with input tensors such as\nTensorFlow data tensors, the default \nNone\n is equal to\nthe number of samples in your dataset divided by\nthe batch size, or 1 if that cannot be determined.\n\n\nvalidation_steps\n: Only relevant if \nsteps_per_epoch\n\nis specified. Total number of steps (batches of samples)\nto validate before stopping.\n\n\n\n\nReturns\n\n\nA \nHistory\n object. Its \nHistory.history\n attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).\n\n\nRaises\n\n\n\n\nRuntimeError\n: If the model was never compiled.\n\n\nValueError\n: In case of mismatch between the provided input data\nand what the model expects.\n\n\n\n\n\n\nevaluate\n\n\nevaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n\n\n\n\nComputes the loss on some input data, batch by batch.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).\n\nx\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\ny\n: labels, as a Numpy array.\n\ny\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\nbatch_size\n: Integer. If unspecified, it will default to 32.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\nsteps\n: Integer or \nNone\n.\nTotal number of steps (batches of samples)\nbefore declaring the evaluation round finished.\nIgnored with the default value of \nNone\n.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\nRaises\n\n\n\n\nRuntimeError\n: if the model was never compiled.\n\n\n\n\n\n\npredict\n\n\npredict(self, x, batch_size=None, verbose=0, steps=None)\n\n\n\n\nGenerates output predictions for the input samples.\n\n\nThe input samples are processed batch by batch.\n\n\nArguments\n\n\n\n\nx\n: the input data, as a Numpy array.\n\n\nbatch_size\n: Integer. If unspecified, it will default to 32.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\nsteps\n: Total number of steps (batches of samples)\nbefore declaring the prediction round finished.\nIgnored with the default value of \nNone\n.\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.\n\n\n\n\ntrain_on_batch\n\n\ntrain_on_batch(self, x, y, class_weight=None, sample_weight=None)\n\n\n\n\nSingle gradient update over one batch of samples.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nclass_weight\n: dictionary mapping classes to a weight value,\nused for scaling the loss function (during training only).\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\n\n\nReturns\n\n\nScalar training loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\nRaises\n\n\n\n\nRuntimeError\n: if the model was never compiled.\n\n\n\n\n\n\ntest_on_batch\n\n\ntest_on_batch(self, x, y, sample_weight=None)\n\n\n\n\nEvaluates the model over a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\nRaises\n\n\n\n\nRuntimeError\n: if the model was never compiled.\n\n\n\n\n\n\npredict_on_batch\n\n\npredict_on_batch(self, x)\n\n\n\n\nReturns predictions for a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.\n\n\n\n\nfit_generator\n\n\nfit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n\n\n\n\nFits the model on data generated batch-by-batch by a Python generator.\n\n\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.\n\n\nThe use of \nkeras.utils.Sequence\n guarantees the ordering\nand guarantees the single use of every input per epoch when\nusing \nuse_multiprocessing=True\n.\n\n\nArguments\n\n\n\n\ngenerator\n: A generator or an instance of \nSequence\n\n(\nkeras.utils.Sequence\n) object in order to avoid duplicate data\nwhen using multiprocessing.\nThe output of the generator must be either\n\n\na tuple \n(inputs, targets)\n\n\na tuple \n(inputs, targets, sample_weights)\n.\nThis tuple (a single output of the generator) makes a single\nbatch. Therefore, all arrays in this tuple must have the same\nlength (equal to the size of this batch). Different batches may\nhave different sizes. For example, the last batch of the epoch\nis commonly smaller than the others, if the size of the dataset\nis not divisible by the batch size.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when \nsteps_per_epoch\n\nbatches have been seen by the model.\n\n\nsteps_per_epoch\n: Total number of steps (batches of samples)\nto yield from \ngenerator\n before declaring one epoch\nfinished and starting the next epoch. It should typically\nbe equal to the number of samples of your dataset\ndivided by the batch size.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nepochs\n: Integer, total number of iterations on the data.\nNote that in conjunction with initial_epoch, the parameter\nepochs is to be understood as \"final epoch\". The model is\nnot trained for n steps given by epochs, but until the\nepoch epochs is reached.\n\n\nverbose\n: Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n\ncallbacks\n: List of \nkeras.callbacks.Callback\n instances.\nList of callbacks to apply during training.\nSee \ncallbacks\n.\n\n\nvalidation_data\n: This can be either\n\n\na generator for the validation data\n\n\na tuple \n(inputs, targets)\n\n\na tuple \n(inputs, targets, sample_weights)\n.\n\n\nvalidation_steps\n: Only relevant if \nvalidation_data\n\nis a generator. Total number of steps (batches of samples)\nto yield from \nvalidation_data\n generator before stopping\nat the end of every epoch. It should typically\nbe equal to the number of samples of your\nvalidation dataset divided by the batch size.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(validation_data)\n as a number of steps.\n\n\nclass_weight\n: Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only). This can be useful to tell the model to\n\"pay more attention\" to samples from an under-represented class.\n\n\nmax_queue_size\n: Integer. Maximum size for the generator queue.\nIf unspecified, \nmax_queue_size\n will default to 10.\n\n\nworkers\n: Integer. Maximum number of processes to spin up\nwhen using process-based threading.\nIf unspecified, \nworkers\n will default to 1. If 0, will\nexecute the generator on the main thread.\n\n\nuse_multiprocessing\n: Boolean.\nIf \nTrue\n, use process-based threading.\nIf unspecified, \nuse_multiprocessing\n will default to \nFalse\n.\nNote that because this implementation relies on multiprocessing,\nyou should not pass non-picklable arguments to the generator\nas they can't be passed easily to children processes.\n\n\nshuffle\n: Boolean (whether to shuffle the order of the batches at\nthe beginning of each epoch. Only used with instances\nof \nSequence\n (\nkeras.utils.Sequence\n).\nHas no effect when \nsteps_per_epoch\n is not \nNone\n.\n\n\ninitial_epoch\n: Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).\n\n\n\n\nReturns\n\n\nA \nHistory\n object.\n\n\nRaises\n\n\n\n\nRuntimeError\n: if the model was never compiled.\n\n\nValueError\n: In case the generator yields data in an invalid format.\n\n\n\n\nExample\n\n\ndef generate_arrays_from_file(path):\n    while True:\n        with open(path) as f:\n            for line in f:\n                # create Numpy arrays of input data\n                # and labels, from each line in the file\n                x, y = process_line(line)\n                yield (x, y)\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                    steps_per_epoch=1000, epochs=10)\n\n\n\n\n\n\nevaluate_generator\n\n\nevaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n\n\n\n\nEvaluates the model on a data generator.\n\n\nThe generator should return the same kind of data\nas accepted by \ntest_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: Generator yielding tuples (inputs, targets)\nor (inputs, targets, sample_weights)\n\n\nsteps\n: Total number of steps (batches of samples)\nto yield from \ngenerator\n before stopping.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nmax_queue_size\n: maximum size for the generator queue\n\n\nworkers\n: maximum number of processes to spin up\n\n\nuse_multiprocessing\n: if True, use process based threading.\nNote that because this implementation\nrelies on multiprocessing, you should not pass\nnon picklable arguments to the generator\nas they can't be passed easily to children processes.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\nRaises\n\n\n\n\nRuntimeError\n: if the model was never compiled.\n\n\n\n\n\n\npredict_generator\n\n\npredict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n\n\n\n\nGenerates predictions for the input samples from a data generator.\n\n\nThe generator should return the same kind of data as accepted by\n\npredict_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: generator yielding batches of input samples.\n\n\nsteps\n: Total number of steps (batches of samples)\nto yield from \ngenerator\n before stopping.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nmax_queue_size\n: maximum size for the generator queue\n\n\nworkers\n: maximum number of processes to spin up\n\n\nuse_multiprocessing\n: if True, use process based threading.\nNote that because this implementation\nrelies on multiprocessing, you should not pass\nnon picklable arguments to the generator\nas they can't be passed easily to children processes.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.\n\n\n\n\nget_layer\n\n\nget_layer(self, name=None, index=None)\n\n\n\n\nRetrieve a layer that is part of the model.\n\n\nReturns a layer based on either its name (unique)\nor its index in the graph. Indices are based on\norder of horizontal graph traversal (bottom-up).\n\n\nArguments\n\n\n\n\nname\n: string, name of layer.\n\n\nindex\n: integer, index of layer.\n\n\n\n\nReturns\n\n\nA layer instance.",
            "title": "Sequential"
        },
        {
            "location": "/models/sequential/#the-sequential-model-api",
            "text": "To get started, read  this guide to the Keras Sequential model .",
            "title": "The Sequential model API"
        },
        {
            "location": "/models/sequential/#useful-attributes-of-model",
            "text": "model.layers  is a list of the layers added to the model.",
            "title": "Useful attributes of Model"
        },
        {
            "location": "/models/sequential/#sequential-model-methods",
            "text": "",
            "title": "Sequential model methods"
        },
        {
            "location": "/models/sequential/#compile",
            "text": "compile(self, optimizer, loss, metrics=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)  Configures the model for training.  Arguments   optimizer : String (name of optimizer) or optimizer object.\nSee  optimizers .  loss : String (name of objective function) or objective function.\nSee  losses .\nIf the model has multiple outputs, you can use a different loss\non each output by passing a dictionary or a list of losses.\nThe loss value that will be minimized by the model\nwill then be the sum of all individual losses.  metrics : List of metrics to be evaluated by the model\nduring training and testing.\nTypically you will use  metrics=['accuracy'] .\nTo specify different metrics for different outputs of a\nmulti-output model, you could also pass a dictionary,\nsuch as  metrics={'output_a': 'accuracy'} .  sample_weight_mode : If you need to do timestep-wise\nsample weighting (2D weights), set this to  \"temporal\" . None  defaults to sample-wise weights (1D).\nIf the model has multiple outputs, you can use a different sample_weight_mode  on each output by passing a\ndictionary or a list of modes.  weighted_metrics : List of metrics to be evaluated and weighted\nby sample_weight or class_weight during training and testing.  target_tensors : By default, Keras will create a placeholder for the\nmodel's target, which will be fed with the target data during\ntraining. If instead you would like to use your own\ntarget tensor (in turn, Keras will not expect external\nNumpy data for these targets at training time), you\ncan specify them via the  target_tensors  argument.\nIt should be a single tensor\n(for a single-output  Sequential  model).  **kwargs : When using the Theano/CNTK backends, these arguments\nare passed into  K.function .\nWhen using the TensorFlow backend,\nthese arguments are passed into  tf.Session.run .   Raises   ValueError : In case of invalid arguments for optimizer ,  loss ,  metrics  or  sample_weight_mode .   Example  model = Sequential()\nmodel.add(Dense(32, input_shape=(500,)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])",
            "title": "compile"
        },
        {
            "location": "/models/sequential/#fit",
            "text": "fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)  Trains the model for a fixed number of epochs (iterations on a dataset).  Arguments   x : Numpy array of training data.\nIf the input layer in the model is named, you can also pass a\ndictionary mapping the input name to a Numpy array. x  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  y : Numpy array of target (label) data.\nIf the output layer in the model is named, you can also pass a\ndictionary mapping the output name to a Numpy array. y  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  batch_size : Integer or  None .\nNumber of samples per gradient update.\nIf unspecified, it will default to 32.  epochs : Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire  x  and  y \ndata provided.\nNote that in conjunction with  initial_epoch , epochs  is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by  epochs , but merely until the epoch\nof index  epochs  is reached.  verbose : 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.  callbacks : List of  keras.callbacks.Callback  instances.\nList of callbacks to apply during training.\nSee  callbacks .  validation_split : Float between 0 and 1.\nFraction of the training data to be used as validation data.\nThe model will set apart this fraction of the training data,\nwill not train on it, and will evaluate\nthe loss and any model metrics\non this data at the end of each epoch.\nThe validation data is selected from the last samples\nin the  x  and  y  data provided, before shuffling.  validation_data : tuple  (x_val, y_val)  or tuple (x_val, y_val, val_sample_weights)  on which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data.\nThis will override  validation_split .  shuffle : Boolean (whether to shuffle the training data\nbefore each epoch) or str (for 'batch').\n'batch' is a special option for dealing with the\nlimitations of HDF5 data; it shuffles in batch-sized chunks.\nHas no effect when  steps_per_epoch  is not  None .  class_weight : Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only).\nThis can be useful to tell the model to\n\"pay more attention\" to samples from\nan under-represented class.  sample_weight : Optional Numpy array of weights for\nthe training samples, used for weighting the loss function\n(during training only). You can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape (samples, sequence_length) ,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify sample_weight_mode=\"temporal\"  in  compile() .  initial_epoch : Epoch at which to start training\n(useful for resuming a previous training run).  steps_per_epoch : Total number of steps (batches of samples)\nbefore declaring one epoch finished and starting the\nnext epoch. When training with input tensors such as\nTensorFlow data tensors, the default  None  is equal to\nthe number of samples in your dataset divided by\nthe batch size, or 1 if that cannot be determined.  validation_steps : Only relevant if  steps_per_epoch \nis specified. Total number of steps (batches of samples)\nto validate before stopping.   Returns  A  History  object. Its  History.history  attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).  Raises   RuntimeError : If the model was never compiled.  ValueError : In case of mismatch between the provided input data\nand what the model expects.",
            "title": "fit"
        },
        {
            "location": "/models/sequential/#evaluate",
            "text": "evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)  Computes the loss on some input data, batch by batch.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs). x  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  y : labels, as a Numpy array. y  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  batch_size : Integer. If unspecified, it will default to 32.  verbose : verbosity mode, 0 or 1.  sample_weight : sample weights, as a Numpy array.  steps : Integer or  None .\nTotal number of steps (batches of samples)\nbefore declaring the evaluation round finished.\nIgnored with the default value of  None .   Returns  Scalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.  Raises   RuntimeError : if the model was never compiled.",
            "title": "evaluate"
        },
        {
            "location": "/models/sequential/#predict",
            "text": "predict(self, x, batch_size=None, verbose=0, steps=None)  Generates output predictions for the input samples.  The input samples are processed batch by batch.  Arguments   x : the input data, as a Numpy array.  batch_size : Integer. If unspecified, it will default to 32.  verbose : verbosity mode, 0 or 1.  steps : Total number of steps (batches of samples)\nbefore declaring the prediction round finished.\nIgnored with the default value of  None .   Returns  A Numpy array of predictions.",
            "title": "predict"
        },
        {
            "location": "/models/sequential/#train_on_batch",
            "text": "train_on_batch(self, x, y, class_weight=None, sample_weight=None)  Single gradient update over one batch of samples.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).  y : labels, as a Numpy array.  class_weight : dictionary mapping classes to a weight value,\nused for scaling the loss function (during training only).  sample_weight : sample weights, as a Numpy array.   Returns  Scalar training loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.  Raises   RuntimeError : if the model was never compiled.",
            "title": "train_on_batch"
        },
        {
            "location": "/models/sequential/#test_on_batch",
            "text": "test_on_batch(self, x, y, sample_weight=None)  Evaluates the model over a single batch of samples.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).  y : labels, as a Numpy array.  sample_weight : sample weights, as a Numpy array.   Returns  Scalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.  Raises   RuntimeError : if the model was never compiled.",
            "title": "test_on_batch"
        },
        {
            "location": "/models/sequential/#predict_on_batch",
            "text": "predict_on_batch(self, x)  Returns predictions for a single batch of samples.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n(if the model has multiple inputs).   Returns  A Numpy array of predictions.",
            "title": "predict_on_batch"
        },
        {
            "location": "/models/sequential/#fit_generator",
            "text": "fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)  Fits the model on data generated batch-by-batch by a Python generator.  The generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.  The use of  keras.utils.Sequence  guarantees the ordering\nand guarantees the single use of every input per epoch when\nusing  use_multiprocessing=True .  Arguments   generator : A generator or an instance of  Sequence \n( keras.utils.Sequence ) object in order to avoid duplicate data\nwhen using multiprocessing.\nThe output of the generator must be either  a tuple  (inputs, targets)  a tuple  (inputs, targets, sample_weights) .\nThis tuple (a single output of the generator) makes a single\nbatch. Therefore, all arrays in this tuple must have the same\nlength (equal to the size of this batch). Different batches may\nhave different sizes. For example, the last batch of the epoch\nis commonly smaller than the others, if the size of the dataset\nis not divisible by the batch size.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when  steps_per_epoch \nbatches have been seen by the model.  steps_per_epoch : Total number of steps (batches of samples)\nto yield from  generator  before declaring one epoch\nfinished and starting the next epoch. It should typically\nbe equal to the number of samples of your dataset\ndivided by the batch size.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  epochs : Integer, total number of iterations on the data.\nNote that in conjunction with initial_epoch, the parameter\nepochs is to be understood as \"final epoch\". The model is\nnot trained for n steps given by epochs, but until the\nepoch epochs is reached.  verbose : Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.  callbacks : List of  keras.callbacks.Callback  instances.\nList of callbacks to apply during training.\nSee  callbacks .  validation_data : This can be either  a generator for the validation data  a tuple  (inputs, targets)  a tuple  (inputs, targets, sample_weights) .  validation_steps : Only relevant if  validation_data \nis a generator. Total number of steps (batches of samples)\nto yield from  validation_data  generator before stopping\nat the end of every epoch. It should typically\nbe equal to the number of samples of your\nvalidation dataset divided by the batch size.\nOptional for  Sequence : if unspecified, will use\nthe  len(validation_data)  as a number of steps.  class_weight : Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only). This can be useful to tell the model to\n\"pay more attention\" to samples from an under-represented class.  max_queue_size : Integer. Maximum size for the generator queue.\nIf unspecified,  max_queue_size  will default to 10.  workers : Integer. Maximum number of processes to spin up\nwhen using process-based threading.\nIf unspecified,  workers  will default to 1. If 0, will\nexecute the generator on the main thread.  use_multiprocessing : Boolean.\nIf  True , use process-based threading.\nIf unspecified,  use_multiprocessing  will default to  False .\nNote that because this implementation relies on multiprocessing,\nyou should not pass non-picklable arguments to the generator\nas they can't be passed easily to children processes.  shuffle : Boolean (whether to shuffle the order of the batches at\nthe beginning of each epoch. Only used with instances\nof  Sequence  ( keras.utils.Sequence ).\nHas no effect when  steps_per_epoch  is not  None .  initial_epoch : Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).   Returns  A  History  object.  Raises   RuntimeError : if the model was never compiled.  ValueError : In case the generator yields data in an invalid format.   Example  def generate_arrays_from_file(path):\n    while True:\n        with open(path) as f:\n            for line in f:\n                # create Numpy arrays of input data\n                # and labels, from each line in the file\n                x, y = process_line(line)\n                yield (x, y)\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                    steps_per_epoch=1000, epochs=10)",
            "title": "fit_generator"
        },
        {
            "location": "/models/sequential/#evaluate_generator",
            "text": "evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)  Evaluates the model on a data generator.  The generator should return the same kind of data\nas accepted by  test_on_batch .  Arguments   generator : Generator yielding tuples (inputs, targets)\nor (inputs, targets, sample_weights)  steps : Total number of steps (batches of samples)\nto yield from  generator  before stopping.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  max_queue_size : maximum size for the generator queue  workers : maximum number of processes to spin up  use_multiprocessing : if True, use process based threading.\nNote that because this implementation\nrelies on multiprocessing, you should not pass\nnon picklable arguments to the generator\nas they can't be passed easily to children processes.   Returns  Scalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.  Raises   RuntimeError : if the model was never compiled.",
            "title": "evaluate_generator"
        },
        {
            "location": "/models/sequential/#predict_generator",
            "text": "predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)  Generates predictions for the input samples from a data generator.  The generator should return the same kind of data as accepted by predict_on_batch .  Arguments   generator : generator yielding batches of input samples.  steps : Total number of steps (batches of samples)\nto yield from  generator  before stopping.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  max_queue_size : maximum size for the generator queue  workers : maximum number of processes to spin up  use_multiprocessing : if True, use process based threading.\nNote that because this implementation\nrelies on multiprocessing, you should not pass\nnon picklable arguments to the generator\nas they can't be passed easily to children processes.  verbose : verbosity mode, 0 or 1.   Returns  A Numpy array of predictions.",
            "title": "predict_generator"
        },
        {
            "location": "/models/sequential/#get_layer",
            "text": "get_layer(self, name=None, index=None)  Retrieve a layer that is part of the model.  Returns a layer based on either its name (unique)\nor its index in the graph. Indices are based on\norder of horizontal graph traversal (bottom-up).  Arguments   name : string, name of layer.  index : integer, index of layer.   Returns  A layer instance.",
            "title": "get_layer"
        },
        {
            "location": "/models/model/",
            "text": "Model class API\n\n\nIn the functional API, given some input tensor(s) and output tensor(s), you can instantiate a \nModel\n via:\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\na = Input(shape=(32,))\nb = Dense(32)(a)\nmodel = Model(inputs=a, outputs=b)\n\n\n\n\nThis model will include all layers required in the computation of \nb\n given \na\n.\n\n\nIn the case of multi-input or multi-output models, you can use lists as well:\n\n\nmodel = Model(inputs=[a1, a2], outputs=[b1, b3, b3])\n\n\n\n\nFor a detailed introduction of what \nModel\n can do, read \nthis guide to the Keras functional API\n.\n\n\nUseful attributes of Model\n\n\n\n\nmodel.layers\n is a flattened list of the layers comprising the model graph.\n\n\nmodel.inputs\n is the list of input tensors.\n\n\nmodel.outputs\n is the list of output tensors.\n\n\n\n\nMethods\n\n\ncompile\n\n\ncompile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n\n\n\n\nConfigures the model for training.\n\n\nArguments\n\n\n\n\noptimizer\n: String (name of optimizer) or optimizer instance.\nSee \noptimizers\n.\n\n\nloss\n: String (name of objective function) or objective function.\nSee \nlosses\n.\nIf the model has multiple outputs, you can use a different loss\non each output by passing a dictionary or a list of losses.\nThe loss value that will be minimized by the model\nwill then be the sum of all individual losses.\n\n\nmetrics\n: List of metrics to be evaluated by the model\nduring training and testing.\nTypically you will use \nmetrics=['accuracy']\n.\nTo specify different metrics for different outputs of a\nmulti-output model, you could also pass a dictionary,\nsuch as \nmetrics={'output_a': 'accuracy'}\n.\n\n\nloss_weights\n: Optional list or dictionary specifying scalar\ncoefficients (Python floats) to weight the loss contributions\nof different model outputs.\nThe loss value that will be minimized by the model\nwill then be the \nweighted sum\n of all individual losses,\nweighted by the \nloss_weights\n coefficients.\nIf a list, it is expected to have a 1:1 mapping\nto the model's outputs. If a tensor, it is expected to map\noutput names (strings) to scalar coefficients.\n\n\nsample_weight_mode\n: If you need to do timestep-wise\nsample weighting (2D weights), set this to \n\"temporal\"\n.\n\nNone\n defaults to sample-wise weights (1D).\nIf the model has multiple outputs, you can use a different\n\nsample_weight_mode\n on each output by passing a\ndictionary or a list of modes.\n\n\nweighted_metrics\n: List of metrics to be evaluated and weighted\nby sample_weight or class_weight during training and testing.\n\n\ntarget_tensors\n: By default, Keras will create placeholders for the\nmodel's target, which will be fed with the target data during\ntraining. If instead you would like to use your own\ntarget tensors (in turn, Keras will not expect external\nNumpy data for these targets at training time), you\ncan specify them via the \ntarget_tensors\n argument. It can be\na single tensor (for a single-output model), a list of tensors,\nor a dict mapping output names to target tensors.\n\n\n**kwargs\n: When using the Theano/CNTK backends, these arguments\nare passed into \nK.function\n.\nWhen using the TensorFlow backend,\nthese arguments are passed into \ntf.Session.run\n.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid arguments for\n\noptimizer\n, \nloss\n, \nmetrics\n or \nsample_weight_mode\n.\n\n\n\n\n\n\nfit\n\n\nfit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n\n\n\n\nTrains the model for a fixed number of epochs (iterations on a dataset).\n\n\nArguments\n\n\n\n\nx\n: Numpy array of training data (if the model has a single input),\nor list of Numpy arrays (if the model has multiple inputs).\nIf input layers in the model are named, you can also pass a\ndictionary mapping input names to Numpy arrays.\n\nx\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\ny\n: Numpy array of target (label) data\n(if the model has a single output),\nor list of Numpy arrays (if the model has multiple outputs).\nIf output layers in the model are named, you can also pass a\ndictionary mapping output names to Numpy arrays.\n\ny\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\nbatch_size\n: Integer or \nNone\n.\nNumber of samples per gradient update.\nIf unspecified, \nbatch_size\n will default to 32.\n\n\nepochs\n: Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire \nx\n and \ny\n\ndata provided.\nNote that in conjunction with \ninitial_epoch\n,\n\nepochs\n is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by \nepochs\n, but merely until the epoch\nof index \nepochs\n is reached.\n\n\nverbose\n: Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n\ncallbacks\n: List of \nkeras.callbacks.Callback\n instances.\nList of callbacks to apply during training.\nSee \ncallbacks\n.\n\n\nvalidation_split\n: Float between 0 and 1.\nFraction of the training data to be used as validation data.\nThe model will set apart this fraction of the training data,\nwill not train on it, and will evaluate\nthe loss and any model metrics\non this data at the end of each epoch.\nThe validation data is selected from the last samples\nin the \nx\n and \ny\n data provided, before shuffling.\n\n\nvalidation_data\n: tuple \n(x_val, y_val)\n or tuple\n\n(x_val, y_val, val_sample_weights)\n on which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data.\n\nvalidation_data\n will override \nvalidation_split\n.\n\n\nshuffle\n: Boolean (whether to shuffle the training data\nbefore each epoch) or str (for 'batch').\n'batch' is a special option for dealing with the\nlimitations of HDF5 data; it shuffles in batch-sized chunks.\nHas no effect when \nsteps_per_epoch\n is not \nNone\n.\n\n\nclass_weight\n: Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only).\nThis can be useful to tell the model to\n\"pay more attention\" to samples from\nan under-represented class.\n\n\nsample_weight\n: Optional Numpy array of weights for\nthe training samples, used for weighting the loss function\n(during training only). You can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape\n\n(samples, sequence_length)\n,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\n\nsample_weight_mode=\"temporal\"\n in \ncompile()\n.\n\n\ninitial_epoch\n: Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).\n\n\nsteps_per_epoch\n: Integer or \nNone\n.\nTotal number of steps (batches of samples)\nbefore declaring one epoch finished and starting the\nnext epoch. When training with input tensors such as\nTensorFlow data tensors, the default \nNone\n is equal to\nthe number of samples in your dataset divided by\nthe batch size, or 1 if that cannot be determined.\n\n\nvalidation_steps\n: Only relevant if \nsteps_per_epoch\n\nis specified. Total number of steps (batches of samples)\nto validate before stopping.\n\n\n\n\nReturns\n\n\nA \nHistory\n object. Its \nHistory.history\n attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).\n\n\nRaises\n\n\n\n\nRuntimeError\n: If the model was never compiled.\n\n\nValueError\n: In case of mismatch between the provided input data\nand what the model expects.\n\n\n\n\n\n\nevaluate\n\n\nevaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n\n\n\n\nReturns the loss value & metrics values for the model in test mode.\n\n\nComputation is done in batches.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of test data (if the model has a single input),\nor list of Numpy arrays (if the model has multiple inputs).\nIf input layers in the model are named, you can also pass a\ndictionary mapping input names to Numpy arrays.\n\nx\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\ny\n: Numpy array of target (label) data\n(if the model has a single output),\nor list of Numpy arrays (if the model has multiple outputs).\nIf output layers in the model are named, you can also pass a\ndictionary mapping output names to Numpy arrays.\n\ny\n can be \nNone\n (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).\n\n\nbatch_size\n: Integer or \nNone\n.\nNumber of samples per evaluation step.\nIf unspecified, \nbatch_size\n will default to 32.\n\n\nverbose\n: 0 or 1. Verbosity mode.\n0 = silent, 1 = progress bar.\n\n\nsample_weight\n: Optional Numpy array of weights for\nthe test samples, used for weighting the loss function.\nYou can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape\n\n(samples, sequence_length)\n,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\n\nsample_weight_mode=\"temporal\"\n in \ncompile()\n.\n\n\nsteps\n: Integer or \nNone\n.\nTotal number of steps (batches of samples)\nbefore declaring the evaluation round finished.\nIgnored with the default value of \nNone\n.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict\n\n\npredict(self, x, batch_size=None, verbose=0, steps=None)\n\n\n\n\nGenerates output predictions for the input samples.\n\n\nComputation is done in batches.\n\n\nArguments\n\n\n\n\nx\n: The input data, as a Numpy array\n(or list of Numpy arrays if the model has multiple outputs).\n\n\nbatch_size\n: Integer. If unspecified, it will default to 32.\n\n\nverbose\n: Verbosity mode, 0 or 1.\n\n\nsteps\n: Total number of steps (batches of samples)\nbefore declaring the prediction round finished.\nIgnored with the default value of \nNone\n.\n\n\n\n\nReturns\n\n\nNumpy array(s) of predictions.\n\n\nRaises\n\n\n\n\nValueError\n: In case of mismatch between the provided\ninput data and the model's expectations,\nor in case a stateful model receives a number of samples\nthat is not a multiple of the batch size.\n\n\n\n\n\n\ntrain_on_batch\n\n\ntrain_on_batch(self, x, y, sample_weight=None, class_weight=None)\n\n\n\n\nRuns a single gradient update on a single batch of data.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of training data,\nor list of Numpy arrays if the model has multiple inputs.\nIf all inputs in the model are named,\nyou can also pass a dictionary\nmapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\nor list of Numpy arrays if the model has multiple outputs.\nIf all outputs in the model are named,\nyou can also pass a dictionary\nmapping output names to Numpy arrays.\n\n\nsample_weight\n: Optional array of the same length as x, containing\nweights to apply to the model's loss for each sample.\nIn the case of temporal data, you can pass a 2D array\nwith shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().\n\n\nclass_weight\n: Optional dictionary mapping\nclass indices (integers) to\na weight (float) to apply to the model's loss for the samples\nfrom this class during training.\nThis can be useful to tell the model to \"pay more attention\" to\nsamples from an under-represented class.\n\n\n\n\nReturns\n\n\nScalar training loss\n(if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\ntest_on_batch\n\n\ntest_on_batch(self, x, y, sample_weight=None)\n\n\n\n\nTest the model on a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of test data,\nor list of Numpy arrays if the model has multiple inputs.\nIf all inputs in the model are named,\nyou can also pass a dictionary\nmapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\nor list of Numpy arrays if the model has multiple outputs.\nIf all outputs in the model are named,\nyou can also pass a dictionary\nmapping output names to Numpy arrays.\n\n\nsample_weight\n: Optional array of the same length as x, containing\nweights to apply to the model's loss for each sample.\nIn the case of temporal data, you can pass a 2D array\nwith shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict_on_batch\n\n\npredict_on_batch(self, x)\n\n\n\n\nReturns predictions for a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: Input samples, as a Numpy array.\n\n\n\n\nReturns\n\n\nNumpy array(s) of predictions.\n\n\n\n\nfit_generator\n\n\nfit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n\n\n\n\nTrains the model on data generated batch-by-batch by a Python generator or an instance of \nSequence\n.\n\n\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.\n\n\nThe use of \nkeras.utils.Sequence\n guarantees the ordering\nand guarantees the single use of every input per epoch when\nusing \nuse_multiprocessing=True\n.\n\n\nArguments\n\n\n\n\ngenerator\n: A generator or an instance of \nSequence\n\n(\nkeras.utils.Sequence\n) object in order to avoid\nduplicate data when using multiprocessing.\nThe output of the generator must be either\n\n\na tuple \n(inputs, targets)\n\n\na tuple \n(inputs, targets, sample_weights)\n.\nThis tuple (a single output of the generator) makes a single\nbatch. Therefore, all arrays in this tuple must have the same\nlength (equal to the size of this batch). Different batches may\nhave different sizes. For example, the last batch of the epoch\nis commonly smaller than the others, if the size of the dataset\nis not divisible by the batch size.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when \nsteps_per_epoch\n\nbatches have been seen by the model.\n\n\nsteps_per_epoch\n: Integer.\nTotal number of steps (batches of samples)\nto yield from \ngenerator\n before declaring one epoch\nfinished and starting the next epoch. It should typically\nbe equal to the number of samples of your dataset\ndivided by the batch size.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nepochs\n: Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire data provided,\nas defined by \nsteps_per_epoch\n.\nNote that in conjunction with \ninitial_epoch\n,\n\nepochs\n is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by \nepochs\n, but merely until the epoch\nof index \nepochs\n is reached.\n\n\nverbose\n: Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n\ncallbacks\n: List of \nkeras.callbacks.Callback\n instances.\nList of callbacks to apply during training.\nSee \ncallbacks\n.\n\n\nvalidation_data\n: This can be either\n\n\na generator for the validation data\n\n\ntuple \n(x_val, y_val)\n\n\ntuple \n(x_val, y_val, val_sample_weights)\n\non which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data.\n\n\nvalidation_steps\n: Only relevant if \nvalidation_data\n\nis a generator. Total number of steps (batches of samples)\nto yield from \nvalidation_data\n generator before stopping\nat the end of every epoch. It should typically\nbe equal to the number of samples of your\nvalidation dataset divided by the batch size.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(validation_data)\n as a number of steps.\n\n\nclass_weight\n: Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only). This can be useful to tell the model to\n\"pay more attention\" to samples from an under-represented class.\n\n\nmax_queue_size\n: Integer. Maximum size for the generator queue.\nIf unspecified, \nmax_queue_size\n will default to 10.\n\n\nworkers\n: Integer. Maximum number of processes to spin up\nwhen using process-based threading.\nIf unspecified, \nworkers\n will default to 1. If 0, will\nexecute the generator on the main thread.\n\n\nuse_multiprocessing\n: Boolean.\nIf \nTrue\n, use process-based threading.\nIf unspecified, \nuse_multiprocessing\n will default to \nFalse\n.\nNote that because this implementation relies on multiprocessing,\nyou should not pass non-picklable arguments to the generator\nas they can't be passed easily to children processes.\n\n\nshuffle\n: Boolean. Whether to shuffle the order of the batches at\nthe beginning of each epoch. Only used with instances\nof \nSequence\n (\nkeras.utils.Sequence\n).\nHas no effect when \nsteps_per_epoch\n is not \nNone\n.\n\n\ninitial_epoch\n: Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).\n\n\n\n\nReturns\n\n\nA \nHistory\n object. Its \nHistory.history\n attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).\n\n\nRaises\n\n\n\n\nValueError\n: In case the generator yields data in an invalid format.\n\n\n\n\nExample\n\n\ndef generate_arrays_from_file(path):\n    while True:\n        with open(path) as f:\n            for line in f:\n                # create numpy arrays of input data\n                # and labels, from each line in the file\n                x1, x2, y = process_line(line)\n                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                    steps_per_epoch=10000, epochs=10)\n\n\n\n\n\n\nevaluate_generator\n\n\nevaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n\n\n\n\nEvaluates the model on a data generator.\n\n\nThe generator should return the same kind of data\nas accepted by \ntest_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: Generator yielding tuples (inputs, targets)\nor (inputs, targets, sample_weights)\nor an instance of Sequence (keras.utils.Sequence)\nobject in order to avoid duplicate data\nwhen using multiprocessing.\n\n\nsteps\n: Total number of steps (batches of samples)\nto yield from \ngenerator\n before stopping.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nmax_queue_size\n: maximum size for the generator queue\n\n\nworkers\n: Integer. Maximum number of processes to spin up\nwhen using process based threading.\nIf unspecified, \nworkers\n will default to 1. If 0, will\nexecute the generator on the main thread.\n\n\nuse_multiprocessing\n: if True, use process based threading.\nNote that because\nthis implementation relies on multiprocessing,\nyou should not pass\nnon picklable arguments to the generator\nas they can't be passed\neasily to children processes.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\nRaises\n\n\n\n\nValueError\n: In case the generator yields\ndata in an invalid format.\n\n\n\n\n\n\npredict_generator\n\n\npredict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n\n\n\n\nGenerates predictions for the input samples from a data generator.\n\n\nThe generator should return the same kind of data as accepted by\n\npredict_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: Generator yielding batches of input samples\nor an instance of Sequence (keras.utils.Sequence)\nobject in order to avoid duplicate data\nwhen using multiprocessing.\n\n\nsteps\n: Total number of steps (batches of samples)\nto yield from \ngenerator\n before stopping.\nOptional for \nSequence\n: if unspecified, will use\nthe \nlen(generator)\n as a number of steps.\n\n\nmax_queue_size\n: Maximum size for the generator queue.\n\n\nworkers\n: Integer. Maximum number of processes to spin up\nwhen using process based threading.\nIf unspecified, \nworkers\n will default to 1. If 0, will\nexecute the generator on the main thread.\n\n\nuse_multiprocessing\n: If \nTrue\n, use process based threading.\nNote that because\nthis implementation relies on multiprocessing,\nyou should not pass\nnon picklable arguments to the generator\nas they can't be passed\neasily to children processes.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nNumpy array(s) of predictions.\n\n\nRaises\n\n\n\n\nValueError\n: In case the generator yields\ndata in an invalid format.\n\n\n\n\n\n\nget_layer\n\n\nget_layer(self, name=None, index=None)\n\n\n\n\nRetrieves a layer based on either its name (unique) or index.\n\n\nIf \nname\n and \nindex\n are both provided, \nindex\n will take precedence.\n\n\nIndices are based on order of horizontal graph traversal (bottom-up).\n\n\nArguments\n\n\n\n\nname\n: String, name of layer.\n\n\nindex\n: Integer, index of layer.\n\n\n\n\nReturns\n\n\nA layer instance.\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid layer name or index.",
            "title": "Model (functional API)"
        },
        {
            "location": "/models/model/#model-class-api",
            "text": "In the functional API, given some input tensor(s) and output tensor(s), you can instantiate a  Model  via:  from keras.models import Model\nfrom keras.layers import Input, Dense\n\na = Input(shape=(32,))\nb = Dense(32)(a)\nmodel = Model(inputs=a, outputs=b)  This model will include all layers required in the computation of  b  given  a .  In the case of multi-input or multi-output models, you can use lists as well:  model = Model(inputs=[a1, a2], outputs=[b1, b3, b3])  For a detailed introduction of what  Model  can do, read  this guide to the Keras functional API .",
            "title": "Model class API"
        },
        {
            "location": "/models/model/#useful-attributes-of-model",
            "text": "model.layers  is a flattened list of the layers comprising the model graph.  model.inputs  is the list of input tensors.  model.outputs  is the list of output tensors.",
            "title": "Useful attributes of Model"
        },
        {
            "location": "/models/model/#methods",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/models/model/#compile",
            "text": "compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)  Configures the model for training.  Arguments   optimizer : String (name of optimizer) or optimizer instance.\nSee  optimizers .  loss : String (name of objective function) or objective function.\nSee  losses .\nIf the model has multiple outputs, you can use a different loss\non each output by passing a dictionary or a list of losses.\nThe loss value that will be minimized by the model\nwill then be the sum of all individual losses.  metrics : List of metrics to be evaluated by the model\nduring training and testing.\nTypically you will use  metrics=['accuracy'] .\nTo specify different metrics for different outputs of a\nmulti-output model, you could also pass a dictionary,\nsuch as  metrics={'output_a': 'accuracy'} .  loss_weights : Optional list or dictionary specifying scalar\ncoefficients (Python floats) to weight the loss contributions\nof different model outputs.\nThe loss value that will be minimized by the model\nwill then be the  weighted sum  of all individual losses,\nweighted by the  loss_weights  coefficients.\nIf a list, it is expected to have a 1:1 mapping\nto the model's outputs. If a tensor, it is expected to map\noutput names (strings) to scalar coefficients.  sample_weight_mode : If you need to do timestep-wise\nsample weighting (2D weights), set this to  \"temporal\" . None  defaults to sample-wise weights (1D).\nIf the model has multiple outputs, you can use a different sample_weight_mode  on each output by passing a\ndictionary or a list of modes.  weighted_metrics : List of metrics to be evaluated and weighted\nby sample_weight or class_weight during training and testing.  target_tensors : By default, Keras will create placeholders for the\nmodel's target, which will be fed with the target data during\ntraining. If instead you would like to use your own\ntarget tensors (in turn, Keras will not expect external\nNumpy data for these targets at training time), you\ncan specify them via the  target_tensors  argument. It can be\na single tensor (for a single-output model), a list of tensors,\nor a dict mapping output names to target tensors.  **kwargs : When using the Theano/CNTK backends, these arguments\nare passed into  K.function .\nWhen using the TensorFlow backend,\nthese arguments are passed into  tf.Session.run .   Raises   ValueError : In case of invalid arguments for optimizer ,  loss ,  metrics  or  sample_weight_mode .",
            "title": "compile"
        },
        {
            "location": "/models/model/#fit",
            "text": "fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)  Trains the model for a fixed number of epochs (iterations on a dataset).  Arguments   x : Numpy array of training data (if the model has a single input),\nor list of Numpy arrays (if the model has multiple inputs).\nIf input layers in the model are named, you can also pass a\ndictionary mapping input names to Numpy arrays. x  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  y : Numpy array of target (label) data\n(if the model has a single output),\nor list of Numpy arrays (if the model has multiple outputs).\nIf output layers in the model are named, you can also pass a\ndictionary mapping output names to Numpy arrays. y  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  batch_size : Integer or  None .\nNumber of samples per gradient update.\nIf unspecified,  batch_size  will default to 32.  epochs : Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire  x  and  y \ndata provided.\nNote that in conjunction with  initial_epoch , epochs  is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by  epochs , but merely until the epoch\nof index  epochs  is reached.  verbose : Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.  callbacks : List of  keras.callbacks.Callback  instances.\nList of callbacks to apply during training.\nSee  callbacks .  validation_split : Float between 0 and 1.\nFraction of the training data to be used as validation data.\nThe model will set apart this fraction of the training data,\nwill not train on it, and will evaluate\nthe loss and any model metrics\non this data at the end of each epoch.\nThe validation data is selected from the last samples\nin the  x  and  y  data provided, before shuffling.  validation_data : tuple  (x_val, y_val)  or tuple (x_val, y_val, val_sample_weights)  on which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data. validation_data  will override  validation_split .  shuffle : Boolean (whether to shuffle the training data\nbefore each epoch) or str (for 'batch').\n'batch' is a special option for dealing with the\nlimitations of HDF5 data; it shuffles in batch-sized chunks.\nHas no effect when  steps_per_epoch  is not  None .  class_weight : Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only).\nThis can be useful to tell the model to\n\"pay more attention\" to samples from\nan under-represented class.  sample_weight : Optional Numpy array of weights for\nthe training samples, used for weighting the loss function\n(during training only). You can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape (samples, sequence_length) ,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify sample_weight_mode=\"temporal\"  in  compile() .  initial_epoch : Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).  steps_per_epoch : Integer or  None .\nTotal number of steps (batches of samples)\nbefore declaring one epoch finished and starting the\nnext epoch. When training with input tensors such as\nTensorFlow data tensors, the default  None  is equal to\nthe number of samples in your dataset divided by\nthe batch size, or 1 if that cannot be determined.  validation_steps : Only relevant if  steps_per_epoch \nis specified. Total number of steps (batches of samples)\nto validate before stopping.   Returns  A  History  object. Its  History.history  attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).  Raises   RuntimeError : If the model was never compiled.  ValueError : In case of mismatch between the provided input data\nand what the model expects.",
            "title": "fit"
        },
        {
            "location": "/models/model/#evaluate",
            "text": "evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)  Returns the loss value & metrics values for the model in test mode.  Computation is done in batches.  Arguments   x : Numpy array of test data (if the model has a single input),\nor list of Numpy arrays (if the model has multiple inputs).\nIf input layers in the model are named, you can also pass a\ndictionary mapping input names to Numpy arrays. x  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  y : Numpy array of target (label) data\n(if the model has a single output),\nor list of Numpy arrays (if the model has multiple outputs).\nIf output layers in the model are named, you can also pass a\ndictionary mapping output names to Numpy arrays. y  can be  None  (default) if feeding from\nframework-native tensors (e.g. TensorFlow data tensors).  batch_size : Integer or  None .\nNumber of samples per evaluation step.\nIf unspecified,  batch_size  will default to 32.  verbose : 0 or 1. Verbosity mode.\n0 = silent, 1 = progress bar.  sample_weight : Optional Numpy array of weights for\nthe test samples, used for weighting the loss function.\nYou can either pass a flat (1D)\nNumpy array with the same length as the input samples\n(1:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape (samples, sequence_length) ,\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify sample_weight_mode=\"temporal\"  in  compile() .  steps : Integer or  None .\nTotal number of steps (batches of samples)\nbefore declaring the evaluation round finished.\nIgnored with the default value of  None .   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.",
            "title": "evaluate"
        },
        {
            "location": "/models/model/#predict",
            "text": "predict(self, x, batch_size=None, verbose=0, steps=None)  Generates output predictions for the input samples.  Computation is done in batches.  Arguments   x : The input data, as a Numpy array\n(or list of Numpy arrays if the model has multiple outputs).  batch_size : Integer. If unspecified, it will default to 32.  verbose : Verbosity mode, 0 or 1.  steps : Total number of steps (batches of samples)\nbefore declaring the prediction round finished.\nIgnored with the default value of  None .   Returns  Numpy array(s) of predictions.  Raises   ValueError : In case of mismatch between the provided\ninput data and the model's expectations,\nor in case a stateful model receives a number of samples\nthat is not a multiple of the batch size.",
            "title": "predict"
        },
        {
            "location": "/models/model/#train_on_batch",
            "text": "train_on_batch(self, x, y, sample_weight=None, class_weight=None)  Runs a single gradient update on a single batch of data.  Arguments   x : Numpy array of training data,\nor list of Numpy arrays if the model has multiple inputs.\nIf all inputs in the model are named,\nyou can also pass a dictionary\nmapping input names to Numpy arrays.  y : Numpy array of target data,\nor list of Numpy arrays if the model has multiple outputs.\nIf all outputs in the model are named,\nyou can also pass a dictionary\nmapping output names to Numpy arrays.  sample_weight : Optional array of the same length as x, containing\nweights to apply to the model's loss for each sample.\nIn the case of temporal data, you can pass a 2D array\nwith shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().  class_weight : Optional dictionary mapping\nclass indices (integers) to\na weight (float) to apply to the model's loss for the samples\nfrom this class during training.\nThis can be useful to tell the model to \"pay more attention\" to\nsamples from an under-represented class.   Returns  Scalar training loss\n(if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.",
            "title": "train_on_batch"
        },
        {
            "location": "/models/model/#test_on_batch",
            "text": "test_on_batch(self, x, y, sample_weight=None)  Test the model on a single batch of samples.  Arguments   x : Numpy array of test data,\nor list of Numpy arrays if the model has multiple inputs.\nIf all inputs in the model are named,\nyou can also pass a dictionary\nmapping input names to Numpy arrays.  y : Numpy array of target data,\nor list of Numpy arrays if the model has multiple outputs.\nIf all outputs in the model are named,\nyou can also pass a dictionary\nmapping output names to Numpy arrays.  sample_weight : Optional array of the same length as x, containing\nweights to apply to the model's loss for each sample.\nIn the case of temporal data, you can pass a 2D array\nwith shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.",
            "title": "test_on_batch"
        },
        {
            "location": "/models/model/#predict_on_batch",
            "text": "predict_on_batch(self, x)  Returns predictions for a single batch of samples.  Arguments   x : Input samples, as a Numpy array.   Returns  Numpy array(s) of predictions.",
            "title": "predict_on_batch"
        },
        {
            "location": "/models/model/#fit_generator",
            "text": "fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)  Trains the model on data generated batch-by-batch by a Python generator or an instance of  Sequence .  The generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.  The use of  keras.utils.Sequence  guarantees the ordering\nand guarantees the single use of every input per epoch when\nusing  use_multiprocessing=True .  Arguments   generator : A generator or an instance of  Sequence \n( keras.utils.Sequence ) object in order to avoid\nduplicate data when using multiprocessing.\nThe output of the generator must be either  a tuple  (inputs, targets)  a tuple  (inputs, targets, sample_weights) .\nThis tuple (a single output of the generator) makes a single\nbatch. Therefore, all arrays in this tuple must have the same\nlength (equal to the size of this batch). Different batches may\nhave different sizes. For example, the last batch of the epoch\nis commonly smaller than the others, if the size of the dataset\nis not divisible by the batch size.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when  steps_per_epoch \nbatches have been seen by the model.  steps_per_epoch : Integer.\nTotal number of steps (batches of samples)\nto yield from  generator  before declaring one epoch\nfinished and starting the next epoch. It should typically\nbe equal to the number of samples of your dataset\ndivided by the batch size.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  epochs : Integer. Number of epochs to train the model.\nAn epoch is an iteration over the entire data provided,\nas defined by  steps_per_epoch .\nNote that in conjunction with  initial_epoch , epochs  is to be understood as \"final epoch\".\nThe model is not trained for a number of iterations\ngiven by  epochs , but merely until the epoch\nof index  epochs  is reached.  verbose : Integer. 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = progress bar, 2 = one line per epoch.  callbacks : List of  keras.callbacks.Callback  instances.\nList of callbacks to apply during training.\nSee  callbacks .  validation_data : This can be either  a generator for the validation data  tuple  (x_val, y_val)  tuple  (x_val, y_val, val_sample_weights) \non which to evaluate\nthe loss and any model metrics at the end of each epoch.\nThe model will not be trained on this data.  validation_steps : Only relevant if  validation_data \nis a generator. Total number of steps (batches of samples)\nto yield from  validation_data  generator before stopping\nat the end of every epoch. It should typically\nbe equal to the number of samples of your\nvalidation dataset divided by the batch size.\nOptional for  Sequence : if unspecified, will use\nthe  len(validation_data)  as a number of steps.  class_weight : Optional dictionary mapping class indices (integers)\nto a weight (float) value, used for weighting the loss function\n(during training only). This can be useful to tell the model to\n\"pay more attention\" to samples from an under-represented class.  max_queue_size : Integer. Maximum size for the generator queue.\nIf unspecified,  max_queue_size  will default to 10.  workers : Integer. Maximum number of processes to spin up\nwhen using process-based threading.\nIf unspecified,  workers  will default to 1. If 0, will\nexecute the generator on the main thread.  use_multiprocessing : Boolean.\nIf  True , use process-based threading.\nIf unspecified,  use_multiprocessing  will default to  False .\nNote that because this implementation relies on multiprocessing,\nyou should not pass non-picklable arguments to the generator\nas they can't be passed easily to children processes.  shuffle : Boolean. Whether to shuffle the order of the batches at\nthe beginning of each epoch. Only used with instances\nof  Sequence  ( keras.utils.Sequence ).\nHas no effect when  steps_per_epoch  is not  None .  initial_epoch : Integer.\nEpoch at which to start training\n(useful for resuming a previous training run).   Returns  A  History  object. Its  History.history  attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).  Raises   ValueError : In case the generator yields data in an invalid format.   Example  def generate_arrays_from_file(path):\n    while True:\n        with open(path) as f:\n            for line in f:\n                # create numpy arrays of input data\n                # and labels, from each line in the file\n                x1, x2, y = process_line(line)\n                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                    steps_per_epoch=10000, epochs=10)",
            "title": "fit_generator"
        },
        {
            "location": "/models/model/#evaluate_generator",
            "text": "evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)  Evaluates the model on a data generator.  The generator should return the same kind of data\nas accepted by  test_on_batch .  Arguments   generator : Generator yielding tuples (inputs, targets)\nor (inputs, targets, sample_weights)\nor an instance of Sequence (keras.utils.Sequence)\nobject in order to avoid duplicate data\nwhen using multiprocessing.  steps : Total number of steps (batches of samples)\nto yield from  generator  before stopping.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  max_queue_size : maximum size for the generator queue  workers : Integer. Maximum number of processes to spin up\nwhen using process based threading.\nIf unspecified,  workers  will default to 1. If 0, will\nexecute the generator on the main thread.  use_multiprocessing : if True, use process based threading.\nNote that because\nthis implementation relies on multiprocessing,\nyou should not pass\nnon picklable arguments to the generator\nas they can't be passed\neasily to children processes.   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.  Raises   ValueError : In case the generator yields\ndata in an invalid format.",
            "title": "evaluate_generator"
        },
        {
            "location": "/models/model/#predict_generator",
            "text": "predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)  Generates predictions for the input samples from a data generator.  The generator should return the same kind of data as accepted by predict_on_batch .  Arguments   generator : Generator yielding batches of input samples\nor an instance of Sequence (keras.utils.Sequence)\nobject in order to avoid duplicate data\nwhen using multiprocessing.  steps : Total number of steps (batches of samples)\nto yield from  generator  before stopping.\nOptional for  Sequence : if unspecified, will use\nthe  len(generator)  as a number of steps.  max_queue_size : Maximum size for the generator queue.  workers : Integer. Maximum number of processes to spin up\nwhen using process based threading.\nIf unspecified,  workers  will default to 1. If 0, will\nexecute the generator on the main thread.  use_multiprocessing : If  True , use process based threading.\nNote that because\nthis implementation relies on multiprocessing,\nyou should not pass\nnon picklable arguments to the generator\nas they can't be passed\neasily to children processes.  verbose : verbosity mode, 0 or 1.   Returns  Numpy array(s) of predictions.  Raises   ValueError : In case the generator yields\ndata in an invalid format.",
            "title": "predict_generator"
        },
        {
            "location": "/models/model/#get_layer",
            "text": "get_layer(self, name=None, index=None)  Retrieves a layer based on either its name (unique) or index.  If  name  and  index  are both provided,  index  will take precedence.  Indices are based on order of horizontal graph traversal (bottom-up).  Arguments   name : String, name of layer.  index : Integer, index of layer.   Returns  A layer instance.  Raises   ValueError : In case of invalid layer name or index.",
            "title": "get_layer"
        },
        {
            "location": "/layers/about-keras-layers/",
            "text": "About Keras layers\n\n\nAll Keras layers have a number of methods in common:\n\n\n\n\nlayer.get_weights()\n: returns the weights of the layer as a list of Numpy arrays.\n\n\nlayer.set_weights(weights)\n: sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of \nget_weights\n).\n\n\nlayer.get_config()\n: returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via:\n\n\n\n\nlayer = Dense(32)\nconfig = layer.get_config()\nreconstructed_layer = Dense.from_config(config)\n\n\n\n\nOr:\n\n\nfrom keras import layers\n\nconfig = layer.get_config()\nlayer = layers.deserialize({'class_name': layer.__class__.__name__,\n                            'config': config})\n\n\n\n\nIf a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:\n\n\n\n\nlayer.input\n\n\nlayer.output\n\n\nlayer.input_shape\n\n\nlayer.output_shape\n\n\n\n\nIf the layer has multiple nodes (see: \nthe concept of layer node and shared layers\n), you can use the following methods:\n\n\n\n\nlayer.get_input_at(node_index)\n\n\nlayer.get_output_at(node_index)\n\n\nlayer.get_input_shape_at(node_index)\n\n\nlayer.get_output_shape_at(node_index)",
            "title": "About Keras layers"
        },
        {
            "location": "/layers/about-keras-layers/#about-keras-layers",
            "text": "All Keras layers have a number of methods in common:   layer.get_weights() : returns the weights of the layer as a list of Numpy arrays.  layer.set_weights(weights) : sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of  get_weights ).  layer.get_config() : returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via:   layer = Dense(32)\nconfig = layer.get_config()\nreconstructed_layer = Dense.from_config(config)  Or:  from keras import layers\n\nconfig = layer.get_config()\nlayer = layers.deserialize({'class_name': layer.__class__.__name__,\n                            'config': config})  If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:   layer.input  layer.output  layer.input_shape  layer.output_shape   If the layer has multiple nodes (see:  the concept of layer node and shared layers ), you can use the following methods:   layer.get_input_at(node_index)  layer.get_output_at(node_index)  layer.get_input_shape_at(node_index)  layer.get_output_shape_at(node_index)",
            "title": "About Keras layers"
        },
        {
            "location": "/layers/core/",
            "text": "[source]\n\n\nDense\n\n\nkeras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nJust your regular densely-connected NN layer.\n\n\nDense\n implements the operation:\n\noutput = activation(dot(input, kernel) + bias)\n\nwhere \nactivation\n is the element-wise activation function\npassed as the \nactivation\n argument, \nkernel\n is a weights matrix\ncreated by the layer, and \nbias\n is a bias vector created by the layer\n(only applicable if \nuse_bias\n is \nTrue\n).\n\n\n\n\nNote\n: if the input to the layer has a rank greater than 2, then\nit is flattened prior to the initial dot product with \nkernel\n.\n\n\n\n\nExample\n\n\n# as first layer in a sequential model:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(16,)))\n# now the model will take as input arrays of shape (*, 16)\n# and output arrays of shape (*, 32)\n\n# after the first layer, you don't need to specify\n# the size of the input anymore:\nmodel.add(Dense(32))\n\n\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\nnD tensor with shape: \n(batch_size, ..., input_dim)\n.\nThe most common situation would be\na 2D input with shape \n(batch_size, input_dim)\n.\n\n\nOutput shape\n\n\nnD tensor with shape: \n(batch_size, ..., units)\n.\nFor instance, for a 2D input with shape \n(batch_size, input_dim)\n,\nthe output would have shape \n(batch_size, units)\n.\n\n\n\n\n[source]\n\n\nActivation\n\n\nkeras.layers.Activation(activation)\n\n\n\n\nApplies an activation function to an output.\n\n\nArguments\n\n\n\n\nactivation\n: name of activation function to use\n(see: \nactivations\n),\nor alternatively, a Theano or TensorFlow operation.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nDropout\n\n\nkeras.layers.Dropout(rate, noise_shape=None, seed=None)\n\n\n\n\nApplies Dropout to the input.\n\n\nDropout consists in randomly setting\na fraction \nrate\n of input units to 0 at each update during training time,\nwhich helps prevent overfitting.\n\n\nArguments\n\n\n\n\nrate\n: float between 0 and 1. Fraction of the input units to drop.\n\n\nnoise_shape\n: 1D integer tensor representing the shape of the\nbinary dropout mask that will be multiplied with the input.\nFor instance, if your inputs have shape\n\n(batch_size, timesteps, features)\n and\nyou want the dropout mask to be the same for all timesteps,\nyou can use \nnoise_shape=(batch_size, 1, features)\n.\n\n\nseed\n: A Python integer to use as random seed.\n\n\n\n\nReferences\n\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n\n\n\n\n\n\n[source]\n\n\nFlatten\n\n\nkeras.layers.Flatten()\n\n\n\n\nFlattens the input. Does not affect the batch size.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Conv2D(64, 3, 3,\n                 border_mode='same',\n                 input_shape=(3, 32, 32)))\n# now: model.output_shape == (None, 64, 32, 32)\n\nmodel.add(Flatten())\n# now: model.output_shape == (None, 65536)\n\n\n\n\n\n\n[source]\n\n\nInput\n\n\nkeras.engine.topology.Input()\n\n\n\n\nInput()\n is used to instantiate a Keras tensor.\n\n\nA Keras tensor is a tensor object from the underlying backend\n(Theano, TensorFlow or CNTK), which we augment with certain\nattributes that allow us to build a Keras model\njust by knowing the inputs and outputs of the model.\n\n\nFor instance, if a, b and c are Keras tensors,\nit becomes possible to do:\n\nmodel = Model(input=[a, b], output=c)\n\n\nThe added Keras attributes are:\n- \n_keras_shape\n: Integer shape tuple propagated\nvia Keras-side shape inference.\n- \n_keras_history\n: Last layer applied to the tensor.\nthe entire layer graph is retrievable from that layer,\nrecursively.\n\n\nArguments\n\n\n\n\nshape\n: A shape tuple (integer), not including the batch size.\nFor instance, \nshape=(32,)\n indicates that the expected input\nwill be batches of 32-dimensional vectors.\n\n\nbatch_shape\n: A shape tuple (integer), including the batch size.\nFor instance, \nbatch_shape=(10, 32)\n indicates that\nthe expected input will be batches of 10 32-dimensional vectors.\n\nbatch_shape=(None, 32)\n indicates batches of an arbitrary number\nof 32-dimensional vectors.\n\n\nname\n: An optional name string for the layer.\nShould be unique in a model (do not reuse the same name twice).\nIt will be autogenerated if it isn't provided.\n\n\ndtype\n: The data type expected by the input, as a string\n(\nfloat32\n, \nfloat64\n, \nint32\n...)\n\n\nsparse\n: A boolean specifying whether the placeholder\nto be created is sparse.\n\n\ntensor\n: Optional existing tensor to wrap into the \nInput\n layer.\nIf set, the layer will not create a placeholder tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\nExample\n\n\n# this is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n\n\n\n\n\n\n[source]\n\n\nReshape\n\n\nkeras.layers.Reshape(target_shape)\n\n\n\n\nReshapes an output to a certain shape.\n\n\nArguments\n\n\n\n\ntarget_shape\n: target shape. Tuple of integers.\nDoes not include the batch axis.\n\n\n\n\nInput shape\n\n\nArbitrary, although all dimensions in the input shaped must be fixed.\nUse the keyword argument \ninput_shape\n\n(tuple of integers, does not include the batch axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\n(batch_size,) + target_shape\n\n\nExample\n\n\n# as first layer in a Sequential model\nmodel = Sequential()\nmodel.add(Reshape((3, 4), input_shape=(12,)))\n# now: model.output_shape == (None, 3, 4)\n# note: `None` is the batch dimension\n\n# as intermediate layer in a Sequential model\nmodel.add(Reshape((6, 2)))\n# now: model.output_shape == (None, 6, 2)\n\n# also supports shape inference using `-1` as dimension\nmodel.add(Reshape((-1, 2, 2)))\n# now: model.output_shape == (None, 3, 2, 2)\n\n\n\n\n\n\n[source]\n\n\nPermute\n\n\nkeras.layers.Permute(dims)\n\n\n\n\nPermutes the dimensions of the input according to a given pattern.\n\n\nUseful for e.g. connecting RNNs and convnets together.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Permute((2, 1), input_shape=(10, 64)))\n# now: model.output_shape == (None, 64, 10)\n# note: `None` is the batch dimension\n\n\n\n\nArguments\n\n\n\n\ndims\n: Tuple of integers. Permutation pattern, does not include the\nsamples dimension. Indexing starts at 1.\nFor instance, \n(2, 1)\n permutes the first and second dimension\nof the input.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame as the input shape, but with the dimensions re-ordered according\nto the specified pattern.\n\n\n\n\n[source]\n\n\nRepeatVector\n\n\nkeras.layers.RepeatVector(n)\n\n\n\n\nRepeats the input n times.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=32))\n# now: model.output_shape == (None, 32)\n# note: `None` is the batch dimension\n\nmodel.add(RepeatVector(3))\n# now: model.output_shape == (None, 3, 32)\n\n\n\n\nArguments\n\n\n\n\nn\n: integer, repetition factor.\n\n\n\n\nInput shape\n\n\n2D tensor of shape \n(num_samples, features)\n.\n\n\nOutput shape\n\n\n3D tensor of shape \n(num_samples, n, features)\n.\n\n\n\n\n[source]\n\n\nLambda\n\n\nkeras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)\n\n\n\n\nWraps arbitrary expression as a \nLayer\n object.\n\n\nExamples\n\n\n# add a x -> x^2 layer\nmodel.add(Lambda(lambda x: x ** 2))\n\n\n\n\n# add a layer that returns the concatenation\n# of the positive part of the input and\n# the opposite of the negative part\n\ndef antirectifier(x):\n    x -= K.mean(x, axis=1, keepdims=True)\n    x = K.l2_normalize(x, axis=1)\n    pos = K.relu(x)\n    neg = K.relu(-x)\n    return K.concatenate([pos, neg], axis=1)\n\ndef antirectifier_output_shape(input_shape):\n    shape = list(input_shape)\n    assert len(shape) == 2  # only valid for 2D tensors\n    shape[-1] *= 2\n    return tuple(shape)\n\nmodel.add(Lambda(antirectifier,\n                 output_shape=antirectifier_output_shape))\n\n\n\n\nArguments\n\n\n\n\nfunction\n: The function to be evaluated.\nTakes input tensor as first argument.\n\n\noutput_shape\n: Expected output shape from function.\nOnly relevant when using Theano.\nCan be a tuple or function.\nIf a tuple, it only specifies the first dimension onward;\nsample dimension is assumed either the same as the input:\n\noutput_shape = (input_shape[0], ) + output_shape\n\nor, the input is \nNone\n and\nthe sample dimension is also \nNone\n:\n\noutput_shape = (None, ) + output_shape\n\nIf a function, it specifies the entire shape as a function of the\ninput shape: \noutput_shape = f(input_shape)\n\n\narguments\n: optional dictionary of keyword arguments to be passed\nto the function.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument input_shape\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSpecified by \noutput_shape\n argument\n(or auto-inferred when using TensorFlow).\n\n\n\n\n[source]\n\n\nActivityRegularization\n\n\nkeras.layers.ActivityRegularization(l1=0.0, l2=0.0)\n\n\n\n\nLayer that applies an update to the cost function based input activity.\n\n\nArguments\n\n\n\n\nl1\n: L1 regularization factor (positive float).\n\n\nl2\n: L2 regularization factor (positive float).\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nMasking\n\n\nkeras.layers.Masking(mask_value=0.0)\n\n\n\n\nMasks a sequence by using a mask value to skip timesteps.\n\n\nFor each timestep in the input tensor (dimension #1 in the tensor),\nif all values in the input tensor at that timestep\nare equal to \nmask_value\n, then the timestep will be masked (skipped)\nin all downstream layers (as long as they support masking).\n\n\nIf any downstream layer does not support masking yet receives such\nan input mask, an exception will be raised.\n\n\nExample\n\n\nConsider a Numpy data array \nx\n of shape \n(samples, timesteps, features)\n,\nto be fed to an LSTM layer.\nYou want to mask timestep #3 and #5 because you lack data for\nthese timesteps. You can:\n\n\n\n\nset \nx[:, 3, :] = 0.\n and \nx[:, 5, :] = 0.\n\n\ninsert a \nMasking\n layer with \nmask_value=0.\n before the LSTM layer:\n\n\n\n\nmodel = Sequential()\nmodel.add(Masking(mask_value=0., input_shape=(timesteps, features)))\nmodel.add(LSTM(32))",
            "title": "Core Layers"
        },
        {
            "location": "/layers/core/#dense",
            "text": "keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  Just your regular densely-connected NN layer.  Dense  implements the operation: output = activation(dot(input, kernel) + bias) \nwhere  activation  is the element-wise activation function\npassed as the  activation  argument,  kernel  is a weights matrix\ncreated by the layer, and  bias  is a bias vector created by the layer\n(only applicable if  use_bias  is  True ).   Note : if the input to the layer has a rank greater than 2, then\nit is flattened prior to the initial dot product with  kernel .   Example  # as first layer in a sequential model:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(16,)))\n# now the model will take as input arrays of shape (*, 16)\n# and output arrays of shape (*, 32)\n\n# after the first layer, you don't need to specify\n# the size of the input anymore:\nmodel.add(Dense(32))  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  nD tensor with shape:  (batch_size, ..., input_dim) .\nThe most common situation would be\na 2D input with shape  (batch_size, input_dim) .  Output shape  nD tensor with shape:  (batch_size, ..., units) .\nFor instance, for a 2D input with shape  (batch_size, input_dim) ,\nthe output would have shape  (batch_size, units) .   [source]",
            "title": "Dense"
        },
        {
            "location": "/layers/core/#activation",
            "text": "keras.layers.Activation(activation)  Applies an activation function to an output.  Arguments   activation : name of activation function to use\n(see:  activations ),\nor alternatively, a Theano or TensorFlow operation.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]",
            "title": "Activation"
        },
        {
            "location": "/layers/core/#dropout",
            "text": "keras.layers.Dropout(rate, noise_shape=None, seed=None)  Applies Dropout to the input.  Dropout consists in randomly setting\na fraction  rate  of input units to 0 at each update during training time,\nwhich helps prevent overfitting.  Arguments   rate : float between 0 and 1. Fraction of the input units to drop.  noise_shape : 1D integer tensor representing the shape of the\nbinary dropout mask that will be multiplied with the input.\nFor instance, if your inputs have shape (batch_size, timesteps, features)  and\nyou want the dropout mask to be the same for all timesteps,\nyou can use  noise_shape=(batch_size, 1, features) .  seed : A Python integer to use as random seed.   References   Dropout: A Simple Way to Prevent Neural Networks from Overfitting    [source]",
            "title": "Dropout"
        },
        {
            "location": "/layers/core/#flatten",
            "text": "keras.layers.Flatten()  Flattens the input. Does not affect the batch size.  Example  model = Sequential()\nmodel.add(Conv2D(64, 3, 3,\n                 border_mode='same',\n                 input_shape=(3, 32, 32)))\n# now: model.output_shape == (None, 64, 32, 32)\n\nmodel.add(Flatten())\n# now: model.output_shape == (None, 65536)   [source]",
            "title": "Flatten"
        },
        {
            "location": "/layers/core/#input",
            "text": "keras.engine.topology.Input()  Input()  is used to instantiate a Keras tensor.  A Keras tensor is a tensor object from the underlying backend\n(Theano, TensorFlow or CNTK), which we augment with certain\nattributes that allow us to build a Keras model\njust by knowing the inputs and outputs of the model.  For instance, if a, b and c are Keras tensors,\nit becomes possible to do: model = Model(input=[a, b], output=c)  The added Keras attributes are:\n-  _keras_shape : Integer shape tuple propagated\nvia Keras-side shape inference.\n-  _keras_history : Last layer applied to the tensor.\nthe entire layer graph is retrievable from that layer,\nrecursively.  Arguments   shape : A shape tuple (integer), not including the batch size.\nFor instance,  shape=(32,)  indicates that the expected input\nwill be batches of 32-dimensional vectors.  batch_shape : A shape tuple (integer), including the batch size.\nFor instance,  batch_shape=(10, 32)  indicates that\nthe expected input will be batches of 10 32-dimensional vectors. batch_shape=(None, 32)  indicates batches of an arbitrary number\nof 32-dimensional vectors.  name : An optional name string for the layer.\nShould be unique in a model (do not reuse the same name twice).\nIt will be autogenerated if it isn't provided.  dtype : The data type expected by the input, as a string\n( float32 ,  float64 ,  int32 ...)  sparse : A boolean specifying whether the placeholder\nto be created is sparse.  tensor : Optional existing tensor to wrap into the  Input  layer.\nIf set, the layer will not create a placeholder tensor.   Returns  A tensor.  Example  # this is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)   [source]",
            "title": "Input"
        },
        {
            "location": "/layers/core/#reshape",
            "text": "keras.layers.Reshape(target_shape)  Reshapes an output to a certain shape.  Arguments   target_shape : target shape. Tuple of integers.\nDoes not include the batch axis.   Input shape  Arbitrary, although all dimensions in the input shaped must be fixed.\nUse the keyword argument  input_shape \n(tuple of integers, does not include the batch axis)\nwhen using this layer as the first layer in a model.  Output shape  (batch_size,) + target_shape  Example  # as first layer in a Sequential model\nmodel = Sequential()\nmodel.add(Reshape((3, 4), input_shape=(12,)))\n# now: model.output_shape == (None, 3, 4)\n# note: `None` is the batch dimension\n\n# as intermediate layer in a Sequential model\nmodel.add(Reshape((6, 2)))\n# now: model.output_shape == (None, 6, 2)\n\n# also supports shape inference using `-1` as dimension\nmodel.add(Reshape((-1, 2, 2)))\n# now: model.output_shape == (None, 3, 2, 2)   [source]",
            "title": "Reshape"
        },
        {
            "location": "/layers/core/#permute",
            "text": "keras.layers.Permute(dims)  Permutes the dimensions of the input according to a given pattern.  Useful for e.g. connecting RNNs and convnets together.  Example  model = Sequential()\nmodel.add(Permute((2, 1), input_shape=(10, 64)))\n# now: model.output_shape == (None, 64, 10)\n# note: `None` is the batch dimension  Arguments   dims : Tuple of integers. Permutation pattern, does not include the\nsamples dimension. Indexing starts at 1.\nFor instance,  (2, 1)  permutes the first and second dimension\nof the input.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same as the input shape, but with the dimensions re-ordered according\nto the specified pattern.   [source]",
            "title": "Permute"
        },
        {
            "location": "/layers/core/#repeatvector",
            "text": "keras.layers.RepeatVector(n)  Repeats the input n times.  Example  model = Sequential()\nmodel.add(Dense(32, input_dim=32))\n# now: model.output_shape == (None, 32)\n# note: `None` is the batch dimension\n\nmodel.add(RepeatVector(3))\n# now: model.output_shape == (None, 3, 32)  Arguments   n : integer, repetition factor.   Input shape  2D tensor of shape  (num_samples, features) .  Output shape  3D tensor of shape  (num_samples, n, features) .   [source]",
            "title": "RepeatVector"
        },
        {
            "location": "/layers/core/#lambda",
            "text": "keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)  Wraps arbitrary expression as a  Layer  object.  Examples  # add a x -> x^2 layer\nmodel.add(Lambda(lambda x: x ** 2))  # add a layer that returns the concatenation\n# of the positive part of the input and\n# the opposite of the negative part\n\ndef antirectifier(x):\n    x -= K.mean(x, axis=1, keepdims=True)\n    x = K.l2_normalize(x, axis=1)\n    pos = K.relu(x)\n    neg = K.relu(-x)\n    return K.concatenate([pos, neg], axis=1)\n\ndef antirectifier_output_shape(input_shape):\n    shape = list(input_shape)\n    assert len(shape) == 2  # only valid for 2D tensors\n    shape[-1] *= 2\n    return tuple(shape)\n\nmodel.add(Lambda(antirectifier,\n                 output_shape=antirectifier_output_shape))  Arguments   function : The function to be evaluated.\nTakes input tensor as first argument.  output_shape : Expected output shape from function.\nOnly relevant when using Theano.\nCan be a tuple or function.\nIf a tuple, it only specifies the first dimension onward;\nsample dimension is assumed either the same as the input: output_shape = (input_shape[0], ) + output_shape \nor, the input is  None  and\nthe sample dimension is also  None : output_shape = (None, ) + output_shape \nIf a function, it specifies the entire shape as a function of the\ninput shape:  output_shape = f(input_shape)  arguments : optional dictionary of keyword arguments to be passed\nto the function.   Input shape  Arbitrary. Use the keyword argument input_shape\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Specified by  output_shape  argument\n(or auto-inferred when using TensorFlow).   [source]",
            "title": "Lambda"
        },
        {
            "location": "/layers/core/#activityregularization",
            "text": "keras.layers.ActivityRegularization(l1=0.0, l2=0.0)  Layer that applies an update to the cost function based input activity.  Arguments   l1 : L1 regularization factor (positive float).  l2 : L2 regularization factor (positive float).   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]",
            "title": "ActivityRegularization"
        },
        {
            "location": "/layers/core/#masking",
            "text": "keras.layers.Masking(mask_value=0.0)  Masks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor),\nif all values in the input tensor at that timestep\nare equal to  mask_value , then the timestep will be masked (skipped)\nin all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such\nan input mask, an exception will be raised.  Example  Consider a Numpy data array  x  of shape  (samples, timesteps, features) ,\nto be fed to an LSTM layer.\nYou want to mask timestep #3 and #5 because you lack data for\nthese timesteps. You can:   set  x[:, 3, :] = 0.  and  x[:, 5, :] = 0.  insert a  Masking  layer with  mask_value=0.  before the LSTM layer:   model = Sequential()\nmodel.add(Masking(mask_value=0., input_shape=(timesteps, features)))\nmodel.add(LSTM(32))",
            "title": "Masking"
        },
        {
            "location": "/layers/convolutional/",
            "text": "[source]\n\n\nConv1D\n\n\nkeras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\n1D convolution layer (e.g. temporal convolution).\n\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf \nuse_bias\n is True, a bias vector is created and added to the outputs.\nFinally, if \nactivation\n is not \nNone\n,\nit is applied to the outputs as well.\n\n\nWhen using this layer as the first layer in a model,\nprovide an \ninput_shape\n argument\n(tuple of integers or \nNone\n, e.g.\n\n(10, 128)\n for sequences of 10 vectors of 128-dimensional vectors,\nor \n(None, 128)\n for variable-length sequences of 128-dimensional vectors.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of a single integer,\nspecifying the length of the 1D convolution window.\n\n\nstrides\n: An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: One of \n\"valid\"\n, \n\"causal\"\n or \n\"same\"\n (case-insensitive).\n\n\"valid\"\n means \"no padding\".\n\n\"same\"\n results in padding the input such that\nthe output has the same length as the original input.\n\n\"causal\"\n results in causal (dilated) convolutions, e.g. output[t]\ndoes not depend on input[t+1:]. Useful when modeling temporal data\nwhere the model should not violate the temporal order.\nSee \nWaveNet: A Generative Model for Raw Audio, section 2.1\n.\n\n\ndilation_rate\n: an integer or tuple/list of a single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any \ndilation_rate\n value != 1 is\nincompatible with specifying any \nstrides\n value != 1.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, input_dim)\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch_size, new_steps, filters)\n\n\nsteps\n value might have changed due to padding or strides.\n\n\n\n\n[source]\n\n\nConv2D\n\n\nkeras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\n2D convolution layer (e.g. spatial convolution over images).\n\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If \nuse_bias\n is True,\na bias vector is created and added to the outputs. Finally, if\n\nactivation\n is not \nNone\n, it is applied to the outputs as well.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(128, 128, 3)\n for 128x128 RGB pictures\nin \ndata_format=\"channels_last\"\n.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\nstrides\n: An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: one of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\ndilation_rate\n: an integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any \ndilation_rate\n value != 1 is\nincompatible with specifying any stride value != 1.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if data_format='channels_last'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, filters, new_rows, new_cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, filters)\n if data_format='channels_last'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nSeparableConv2D\n\n\nkeras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)\n\n\n\n\nDepthwise separable 2D convolution.\n\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The \ndepth_multiplier\n argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\nstrides\n: An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: one of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\ndepth_multiplier\n: The number of depthwise convolution output channels\nfor each input channel.\nThe total number of depthwise convolution output\nchannels will be equal to \nfilterss_in * depth_multiplier\n.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\ndepthwise_initializer\n: Initializer for the depthwise kernel matrix\n(see \ninitializers\n).\n\n\npointwise_initializer\n: Initializer for the pointwise kernel matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\ndepthwise_regularizer\n: Regularizer function applied to\nthe depthwise kernel matrix\n(see \nregularizer\n).\n\n\npointwise_regularizer\n: Regularizer function applied to\nthe pointwise kernel matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\ndepthwise_constraint\n: Constraint function applied to\nthe depthwise kernel matrix\n(see \nconstraints\n).\n\n\npointwise_constraint\n: Constraint function applied to\nthe pointwise kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(batch, channels, rows, cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(batch, rows, cols, channels)\n if data_format='channels_last'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(batch, filters, new_rows, new_cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(batch, new_rows, new_cols, filters)\n if data_format='channels_last'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nConv2DTranspose\n\n\nkeras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nTransposed convolution layer (sometimes called Deconvolution).\n\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(128, 128, 3)\n for 128x128 RGB pictures\nin \ndata_format=\"channels_last\"\n.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\nstrides\n: An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: one of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\ndilation_rate\n: an integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any \ndilation_rate\n value != 1 is\nincompatible with specifying any stride value != 1.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(batch, channels, rows, cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(batch, rows, cols, channels)\n if data_format='channels_last'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(batch, filters, new_rows, new_cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(batch, new_rows, new_cols, filters)\n if data_format='channels_last'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\nReferences\n\n\n\n\nA guide to convolution arithmetic for deep learning\n\n\nDeconvolutional Networks\n\n\n\n\n\n\n[source]\n\n\nConv3D\n\n\nkeras.layers.Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\n3D convolution layer (e.g. spatial convolution over volumes).\n\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If \nuse_bias\n is True,\na bias vector is created and added to the outputs. Finally, if\n\nactivation\n is not \nNone\n, it is applied to the outputs as well.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(128, 128, 128, 1)\n for 128x128x128 volumes\nwith a single channel,\nin \ndata_format=\"channels_last\"\n.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\nstrides\n: An integer or tuple/list of 3 integers,\nspecifying the strides of the convolution along each spatial dimension.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: one of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\ndilation_rate\n: an integer or tuple/list of 3 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any \ndilation_rate\n value != 1 is\nincompatible with specifying any stride value != 1.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, conv_dim1, conv_dim2, conv_dim3)\n if data_format='channels_first'\nor 5D tensor with shape:\n\n(samples, conv_dim1, conv_dim2, conv_dim3, channels)\n if data_format='channels_last'.\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)\n if data_format='channels_first'\nor 5D tensor with shape:\n\n(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)\n if data_format='channels_last'.\n\nnew_conv_dim1\n, \nnew_conv_dim2\n and \nnew_conv_dim3\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nCropping1D\n\n\nkeras.layers.Cropping1D(cropping=(1, 1))\n\n\n\n\nCropping layer for 1D input (e.g. temporal sequence).\n\n\nIt crops along the time dimension (axis 1).\n\n\nArguments\n\n\n\n\ncropping\n: int or tuple of int (length 2)\nHow many units should be trimmed off at the beginning and end of\nthe cropping dimension (axis 1).\nIf a single int is provided,\nthe same value will be used for both.\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(batch, axis_to_crop, features)\n\n\nOutput shape\n\n\n3D tensor with shape \n(batch, cropped_axis, features)\n\n\n\n\n[source]\n\n\nCropping2D\n\n\nkeras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None)\n\n\n\n\nCropping layer for 2D input (e.g. picture).\n\n\nIt crops along spatial dimensions, i.e. width and height.\n\n\nArguments\n\n\n\n\ncropping\n: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n\n\nIf int: the same symmetric cropping\nis applied to width and height.\n\n\nIf tuple of 2 ints:\ninterpreted as two different\nsymmetric cropping values for height and width:\n\n(symmetric_height_crop, symmetric_width_crop)\n.\n\n\nIf tuple of 2 tuples of 2 ints:\ninterpreted as\n\n((top_crop, bottom_crop), (left_crop, right_crop))\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, rows, cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, rows, cols)\n\n\nOutput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, cropped_rows, cropped_cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, cropped_rows, cropped_cols)\n\n\nExamples\n\n\n# Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)),\n                     input_shape=(28, 28, 3)))\n# now model.output_shape == (None, 24, 20, 3)\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Cropping2D(cropping=((2, 2), (2, 2))))\n# now model.output_shape == (None, 20, 16. 64)\n\n\n\n\n\n\n[source]\n\n\nCropping3D\n\n\nkeras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None)\n\n\n\n\nCropping layer for 3D data (e.g. spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\ncropping\n: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n\n\nIf int: the same symmetric cropping\nis applied to depth, height, and width.\n\n\nIf tuple of 3 ints:\ninterpreted as two different\nsymmetric cropping values for depth, height, and width:\n\n(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)\n.\n\n\nIf tuple of 3 tuples of 2 ints:\ninterpreted as\n\n((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)\n\n\nOutput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n\n\n\n\n[source]\n\n\nUpSampling1D\n\n\nkeras.layers.UpSampling1D(size=2)\n\n\n\n\nUpsampling layer for 1D inputs.\n\n\nRepeats each temporal step \nsize\n times along the time axis.\n\n\nArguments\n\n\n\n\nsize\n: integer. Upsampling factor.\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch, upsampled_steps, features)\n.\n\n\n\n\n[source]\n\n\nUpSampling2D\n\n\nkeras.layers.UpSampling2D(size=(2, 2), data_format=None)\n\n\n\n\nUpsampling layer for 2D inputs.\n\n\nRepeats the rows and columns of the data\nby size[0] and size[1] respectively.\n\n\nArguments\n\n\n\n\nsize\n: int, or tuple of 2 integers.\nThe upsampling factors for rows and columns.\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, rows, cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, rows, cols)\n\n\nOutput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, upsampled_rows, upsampled_cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, upsampled_rows, upsampled_cols)\n\n\n\n\n[source]\n\n\nUpSampling3D\n\n\nkeras.layers.UpSampling3D(size=(2, 2, 2), data_format=None)\n\n\n\n\nUpsampling layer for 3D inputs.\n\n\nRepeats the 1st, 2nd and 3rd dimensions\nof the data by size[0], size[1] and size[2] respectively.\n\n\nArguments\n\n\n\n\nsize\n: int, or tuple of 3 integers.\nThe upsampling factors for dim1, dim2 and dim3.\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, dim1, dim2, dim3, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, dim1, dim2, dim3)\n\n\nOutput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)\n\n\n\n\n[source]\n\n\nZeroPadding1D\n\n\nkeras.layers.ZeroPadding1D(padding=1)\n\n\n\n\nZero-padding layer for 1D input (e.g. temporal sequence).\n\n\nArguments\n\n\n\n\npadding\n: int, or tuple of int (length 2), or dictionary.\n\n\nIf int:\nHow many zeros to add at the beginning and end of\nthe padding dimension (axis 1).\n\n\nIf tuple of int (length 2):\nHow many zeros to add at the beginning and at the end of\nthe padding dimension (\n(left_pad, right_pad)\n).\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(batch, axis_to_pad, features)\n\n\nOutput shape\n\n\n3D tensor with shape \n(batch, padded_axis, features)\n\n\n\n\n[source]\n\n\nZeroPadding2D\n\n\nkeras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)\n\n\n\n\nZero-padding layer for 2D input (e.g. picture).\n\n\nThis layer can add rows and columns of zeros\nat the top, bottom, left and right side of an image tensor.\n\n\nArguments\n\n\n\n\npadding\n: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n\n\nIf int: the same symmetric padding\nis applied to width and height.\n\n\nIf tuple of 2 ints:\ninterpreted as two different\nsymmetric padding values for height and width:\n\n(symmetric_height_pad, symmetric_width_pad)\n.\n\n\nIf tuple of 2 tuples of 2 ints:\ninterpreted as\n\n((top_pad, bottom_pad), (left_pad, right_pad))\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, rows, cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, rows, cols)\n\n\nOutput shape\n\n\n4D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, padded_rows, padded_cols, channels)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, channels, padded_rows, padded_cols)\n\n\n\n\n[source]\n\n\nZeroPadding3D\n\n\nkeras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None)\n\n\n\n\nZero-padding layer for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npadding\n: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n\n\nIf int: the same symmetric padding\nis applied to width and height.\n\n\nIf tuple of 2 ints:\ninterpreted as two different\nsymmetric padding values for height and width:\n\n(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)\n.\n\n\nIf tuple of 2 tuples of 2 ints:\ninterpreted as\n\n((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n\n\nOutput shape\n\n\n5D tensor with shape:\n- If \ndata_format\n is \n\"channels_last\"\n:\n\n(batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth)\n\n- If \ndata_format\n is \n\"channels_first\"\n:\n\n(batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)",
            "title": "Convolutional Layers"
        },
        {
            "location": "/layers/convolutional/#conv1d",
            "text": "keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  1D convolution layer (e.g. temporal convolution).  This layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf  use_bias  is True, a bias vector is created and added to the outputs.\nFinally, if  activation  is not  None ,\nit is applied to the outputs as well.  When using this layer as the first layer in a model,\nprovide an  input_shape  argument\n(tuple of integers or  None , e.g. (10, 128)  for sequences of 10 vectors of 128-dimensional vectors,\nor  (None, 128)  for variable-length sequences of 128-dimensional vectors.  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of a single integer,\nspecifying the length of the 1D convolution window.  strides : An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : One of  \"valid\" ,  \"causal\"  or  \"same\"  (case-insensitive). \"valid\"  means \"no padding\". \"same\"  results in padding the input such that\nthe output has the same length as the original input. \"causal\"  results in causal (dilated) convolutions, e.g. output[t]\ndoes not depend on input[t+1:]. Useful when modeling temporal data\nwhere the model should not violate the temporal order.\nSee  WaveNet: A Generative Model for Raw Audio, section 2.1 .  dilation_rate : an integer or tuple/list of a single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any  dilation_rate  value != 1 is\nincompatible with specifying any  strides  value != 1.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  3D tensor with shape:  (batch_size, steps, input_dim)  Output shape  3D tensor with shape:  (batch_size, new_steps, filters)  steps  value might have changed due to padding or strides.   [source]",
            "title": "Conv1D"
        },
        {
            "location": "/layers/convolutional/#conv2d",
            "text": "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  2D convolution layer (e.g. spatial convolution over images).  This layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If  use_bias  is True,\na bias vector is created and added to the outputs. Finally, if activation  is not  None , it is applied to the outputs as well.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(128, 128, 3)  for 128x128 RGB pictures\nin  data_format=\"channels_last\" .  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.  strides : An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : one of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  dilation_rate : an integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any  dilation_rate  value != 1 is\nincompatible with specifying any stride value != 1.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if data_format='channels_first'\nor 4D tensor with shape: (samples, rows, cols, channels)  if data_format='channels_last'.  Output shape  4D tensor with shape: (samples, filters, new_rows, new_cols)  if data_format='channels_first'\nor 4D tensor with shape: (samples, new_rows, new_cols, filters)  if data_format='channels_last'. rows  and  cols  values might have changed due to padding.   [source]",
            "title": "Conv2D"
        },
        {
            "location": "/layers/convolutional/#separableconv2d",
            "text": "keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)  Depthwise separable 2D convolution.  Separable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The  depth_multiplier  argument controls how many\noutput channels are generated per input channel in the depthwise step.  Intuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.  strides : An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : one of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  depth_multiplier : The number of depthwise convolution output channels\nfor each input channel.\nThe total number of depthwise convolution output\nchannels will be equal to  filterss_in * depth_multiplier .  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  depthwise_initializer : Initializer for the depthwise kernel matrix\n(see  initializers ).  pointwise_initializer : Initializer for the pointwise kernel matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  depthwise_regularizer : Regularizer function applied to\nthe depthwise kernel matrix\n(see  regularizer ).  pointwise_regularizer : Regularizer function applied to\nthe pointwise kernel matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  depthwise_constraint : Constraint function applied to\nthe depthwise kernel matrix\n(see  constraints ).  pointwise_constraint : Constraint function applied to\nthe pointwise kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  4D tensor with shape: (batch, channels, rows, cols)  if data_format='channels_first'\nor 4D tensor with shape: (batch, rows, cols, channels)  if data_format='channels_last'.  Output shape  4D tensor with shape: (batch, filters, new_rows, new_cols)  if data_format='channels_first'\nor 4D tensor with shape: (batch, new_rows, new_cols, filters)  if data_format='channels_last'. rows  and  cols  values might have changed due to padding.   [source]",
            "title": "SeparableConv2D"
        },
        {
            "location": "/layers/convolutional/#conv2dtranspose",
            "text": "keras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  Transposed convolution layer (sometimes called Deconvolution).  The need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(128, 128, 3)  for 128x128 RGB pictures\nin  data_format=\"channels_last\" .  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.  strides : An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : one of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  dilation_rate : an integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any  dilation_rate  value != 1 is\nincompatible with specifying any stride value != 1.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  4D tensor with shape: (batch, channels, rows, cols)  if data_format='channels_first'\nor 4D tensor with shape: (batch, rows, cols, channels)  if data_format='channels_last'.  Output shape  4D tensor with shape: (batch, filters, new_rows, new_cols)  if data_format='channels_first'\nor 4D tensor with shape: (batch, new_rows, new_cols, filters)  if data_format='channels_last'. rows  and  cols  values might have changed due to padding.  References   A guide to convolution arithmetic for deep learning  Deconvolutional Networks    [source]",
            "title": "Conv2DTranspose"
        },
        {
            "location": "/layers/convolutional/#conv3d",
            "text": "keras.layers.Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  3D convolution layer (e.g. spatial convolution over volumes).  This layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If  use_bias  is True,\na bias vector is created and added to the outputs. Finally, if activation  is not  None , it is applied to the outputs as well.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(128, 128, 128, 1)  for 128x128x128 volumes\nwith a single channel,\nin  data_format=\"channels_last\" .  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.  strides : An integer or tuple/list of 3 integers,\nspecifying the strides of the convolution along each spatial dimension.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : one of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  dilation_rate : an integer or tuple/list of 3 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any  dilation_rate  value != 1 is\nincompatible with specifying any stride value != 1.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3)  if data_format='channels_first'\nor 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels)  if data_format='channels_last'.  Output shape  5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)  if data_format='channels_first'\nor 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)  if data_format='channels_last'. new_conv_dim1 ,  new_conv_dim2  and  new_conv_dim3  values might have changed due to padding.   [source]",
            "title": "Conv3D"
        },
        {
            "location": "/layers/convolutional/#cropping1d",
            "text": "keras.layers.Cropping1D(cropping=(1, 1))  Cropping layer for 1D input (e.g. temporal sequence).  It crops along the time dimension (axis 1).  Arguments   cropping : int or tuple of int (length 2)\nHow many units should be trimmed off at the beginning and end of\nthe cropping dimension (axis 1).\nIf a single int is provided,\nthe same value will be used for both.   Input shape  3D tensor with shape  (batch, axis_to_crop, features)  Output shape  3D tensor with shape  (batch, cropped_axis, features)   [source]",
            "title": "Cropping1D"
        },
        {
            "location": "/layers/convolutional/#cropping2d",
            "text": "keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None)  Cropping layer for 2D input (e.g. picture).  It crops along spatial dimensions, i.e. width and height.  Arguments   cropping : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.  If int: the same symmetric cropping\nis applied to width and height.  If tuple of 2 ints:\ninterpreted as two different\nsymmetric cropping values for height and width: (symmetric_height_crop, symmetric_width_crop) .  If tuple of 2 tuples of 2 ints:\ninterpreted as ((top_crop, bottom_crop), (left_crop, right_crop))  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, rows, cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, rows, cols)  Output shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, cropped_rows, cropped_cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, cropped_rows, cropped_cols)  Examples  # Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)),\n                     input_shape=(28, 28, 3)))\n# now model.output_shape == (None, 24, 20, 3)\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Cropping2D(cropping=((2, 2), (2, 2))))\n# now model.output_shape == (None, 20, 16. 64)   [source]",
            "title": "Cropping2D"
        },
        {
            "location": "/layers/convolutional/#cropping3d",
            "text": "keras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None)  Cropping layer for 3D data (e.g. spatial or spatio-temporal).  Arguments   cropping : int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.  If int: the same symmetric cropping\nis applied to depth, height, and width.  If tuple of 3 ints:\ninterpreted as two different\nsymmetric cropping values for depth, height, and width: (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop) .  If tuple of 3 tuples of 2 ints:\ninterpreted as ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth) \n- If  data_format  is  \"channels_first\" : (batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)  Output shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) \n- If  data_format  is  \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)   [source]",
            "title": "Cropping3D"
        },
        {
            "location": "/layers/convolutional/#upsampling1d",
            "text": "keras.layers.UpSampling1D(size=2)  Upsampling layer for 1D inputs.  Repeats each temporal step  size  times along the time axis.  Arguments   size : integer. Upsampling factor.   Input shape  3D tensor with shape:  (batch, steps, features) .  Output shape  3D tensor with shape:  (batch, upsampled_steps, features) .   [source]",
            "title": "UpSampling1D"
        },
        {
            "location": "/layers/convolutional/#upsampling2d",
            "text": "keras.layers.UpSampling2D(size=(2, 2), data_format=None)  Upsampling layer for 2D inputs.  Repeats the rows and columns of the data\nby size[0] and size[1] respectively.  Arguments   size : int, or tuple of 2 integers.\nThe upsampling factors for rows and columns.  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, rows, cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, rows, cols)  Output shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, upsampled_rows, upsampled_cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, upsampled_rows, upsampled_cols)   [source]",
            "title": "UpSampling2D"
        },
        {
            "location": "/layers/convolutional/#upsampling3d",
            "text": "keras.layers.UpSampling3D(size=(2, 2, 2), data_format=None)  Upsampling layer for 3D inputs.  Repeats the 1st, 2nd and 3rd dimensions\nof the data by size[0], size[1] and size[2] respectively.  Arguments   size : int, or tuple of 3 integers.\nThe upsampling factors for dim1, dim2 and dim3.  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, dim1, dim2, dim3, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, dim1, dim2, dim3)  Output shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)   [source]",
            "title": "UpSampling3D"
        },
        {
            "location": "/layers/convolutional/#zeropadding1d",
            "text": "keras.layers.ZeroPadding1D(padding=1)  Zero-padding layer for 1D input (e.g. temporal sequence).  Arguments   padding : int, or tuple of int (length 2), or dictionary.  If int:\nHow many zeros to add at the beginning and end of\nthe padding dimension (axis 1).  If tuple of int (length 2):\nHow many zeros to add at the beginning and at the end of\nthe padding dimension ( (left_pad, right_pad) ).   Input shape  3D tensor with shape  (batch, axis_to_pad, features)  Output shape  3D tensor with shape  (batch, padded_axis, features)   [source]",
            "title": "ZeroPadding1D"
        },
        {
            "location": "/layers/convolutional/#zeropadding2d",
            "text": "keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)  Zero-padding layer for 2D input (e.g. picture).  This layer can add rows and columns of zeros\nat the top, bottom, left and right side of an image tensor.  Arguments   padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.  If int: the same symmetric padding\nis applied to width and height.  If tuple of 2 ints:\ninterpreted as two different\nsymmetric padding values for height and width: (symmetric_height_pad, symmetric_width_pad) .  If tuple of 2 tuples of 2 ints:\ninterpreted as ((top_pad, bottom_pad), (left_pad, right_pad))  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, rows, cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, rows, cols)  Output shape  4D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, padded_rows, padded_cols, channels) \n- If  data_format  is  \"channels_first\" : (batch, channels, padded_rows, padded_cols)   [source]",
            "title": "ZeroPadding2D"
        },
        {
            "location": "/layers/convolutional/#zeropadding3d",
            "text": "keras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None)  Zero-padding layer for 3D data (spatial or spatio-temporal).  Arguments   padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.  If int: the same symmetric padding\nis applied to width and height.  If tuple of 2 ints:\ninterpreted as two different\nsymmetric padding values for height and width: (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad) .  If tuple of 2 tuples of 2 ints:\ninterpreted as ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) \n- If  data_format  is  \"channels_first\" : (batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)  Output shape  5D tensor with shape:\n- If  data_format  is  \"channels_last\" : (batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) \n- If  data_format  is  \"channels_first\" : (batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)",
            "title": "ZeroPadding3D"
        },
        {
            "location": "/layers/pooling/",
            "text": "[source]\n\n\nMaxPooling1D\n\n\nkeras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid')\n\n\n\n\nMax pooling operation for temporal data.\n\n\nArguments\n\n\n\n\npool_size\n: Integer, size of the max pooling windows.\n\n\nstrides\n: Integer, or None. Factor by which to downscale.\nE.g. 2 will halve the input.\nIf None, it will default to \npool_size\n.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch_size, downsampled_steps, features)\n.\n\n\n\n\n[source]\n\n\nMaxPooling2D\n\n\nkeras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n\n\n\n\nMax pooling operation for spatial data.\n\n\nArguments\n\n\n\n\npool_size\n: integer or tuple of 2 integers,\nfactors by which to downscale (vertical, horizontal).\n(2, 2) will halve the input in both spatial dimension.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.\n\n\nstrides\n: Integer, tuple of 2 integers, or None.\nStrides values.\nIf None, it will default to \npool_size\n.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, rows, cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, rows, cols)\n\n\n\n\nOutput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, pooled_rows, pooled_cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, pooled_rows, pooled_cols)\n\n\n\n\n\n\n[source]\n\n\nMaxPooling3D\n\n\nkeras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)\n\n\n\n\nMax pooling operation for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.\n\n\nstrides\n: tuple of 3 integers, or None. Strides values.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n5D tensor with shape:\n\n(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\n\nIf \ndata_format='channels_first'\n:\n5D tensor with shape:\n\n(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n\n\n\n\nOutput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n5D tensor with shape:\n\n(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)\n\n\nIf \ndata_format='channels_first'\n:\n5D tensor with shape:\n\n(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)\n\n\n\n\n\n\n[source]\n\n\nAveragePooling1D\n\n\nkeras.layers.AveragePooling1D(pool_size=2, strides=None, padding='valid')\n\n\n\n\nAverage pooling for temporal data.\n\n\nArguments\n\n\n\n\npool_size\n: Integer, size of the average pooling windows.\n\n\nstrides\n: Integer, or None. Factor by which to downscale.\nE.g. 2 will halve the input.\nIf None, it will default to \npool_size\n.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch_size, downsampled_steps, features)\n.\n\n\n\n\n[source]\n\n\nAveragePooling2D\n\n\nkeras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n\n\n\n\nAverage pooling operation for spatial data.\n\n\nArguments\n\n\n\n\npool_size\n: integer or tuple of 2 integers,\nfactors by which to downscale (vertical, horizontal).\n(2, 2) will halve the input in both spatial dimension.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.\n\n\nstrides\n: Integer, tuple of 2 integers, or None.\nStrides values.\nIf None, it will default to \npool_size\n.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, rows, cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, rows, cols)\n\n\n\n\nOutput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, pooled_rows, pooled_cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, pooled_rows, pooled_cols)\n\n\n\n\n\n\n[source]\n\n\nAveragePooling3D\n\n\nkeras.layers.AveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)\n\n\n\n\nAverage pooling operation for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.\n\n\nstrides\n: tuple of 3 integers, or None. Strides values.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\nwhile \nchannels_first\n corresponds to inputs with shape\n\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n5D tensor with shape:\n\n(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n\n\nIf \ndata_format='channels_first'\n:\n5D tensor with shape:\n\n(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n\n\n\n\nOutput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n5D tensor with shape:\n\n(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)\n\n\nIf \ndata_format='channels_first'\n:\n5D tensor with shape:\n\n(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)\n\n\n\n\n\n\n[source]\n\n\nGlobalMaxPooling1D\n\n\nkeras.layers.GlobalMaxPooling1D()\n\n\n\n\nGlobal max pooling operation for temporal data.\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, features)\n.\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(batch_size, features)\n\n\n\n\n[source]\n\n\nGlobalAveragePooling1D\n\n\nkeras.layers.GlobalAveragePooling1D()\n\n\n\n\nGlobal average pooling operation for temporal data.\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, features)\n.\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(batch_size, features)\n\n\n\n\n[source]\n\n\nGlobalMaxPooling2D\n\n\nkeras.layers.GlobalMaxPooling2D(data_format=None)\n\n\n\n\nGlobal max pooling operation for spatial data.\n\n\nArguments\n\n\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, rows, cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, rows, cols)\n\n\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(batch_size, channels)\n\n\n\n\n[source]\n\n\nGlobalAveragePooling2D\n\n\nkeras.layers.GlobalAveragePooling2D(data_format=None)\n\n\n\n\nGlobal average pooling operation for spatial data.\n\n\nArguments\n\n\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\n\n\nInput shape\n\n\n\n\nIf \ndata_format='channels_last'\n:\n4D tensor with shape:\n\n(batch_size, rows, cols, channels)\n\n\nIf \ndata_format='channels_first'\n:\n4D tensor with shape:\n\n(batch_size, channels, rows, cols)\n\n\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(batch_size, channels)",
            "title": "Pooling Layers"
        },
        {
            "location": "/layers/pooling/#maxpooling1d",
            "text": "keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid')  Max pooling operation for temporal data.  Arguments   pool_size : Integer, size of the max pooling windows.  strides : Integer, or None. Factor by which to downscale.\nE.g. 2 will halve the input.\nIf None, it will default to  pool_size .  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).   Input shape  3D tensor with shape:  (batch_size, steps, features) .  Output shape  3D tensor with shape:  (batch_size, downsampled_steps, features) .   [source]",
            "title": "MaxPooling1D"
        },
        {
            "location": "/layers/pooling/#maxpooling2d",
            "text": "keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)  Max pooling operation for spatial data.  Arguments   pool_size : integer or tuple of 2 integers,\nfactors by which to downscale (vertical, horizontal).\n(2, 2) will halve the input in both spatial dimension.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.  strides : Integer, tuple of 2 integers, or None.\nStrides values.\nIf None, it will default to  pool_size .  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, rows, cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, rows, cols)   Output shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols)    [source]",
            "title": "MaxPooling2D"
        },
        {
            "location": "/layers/pooling/#maxpooling3d",
            "text": "keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)  Max pooling operation for 3D data (spatial or spatio-temporal).  Arguments   pool_size : tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.  strides : tuple of 3 integers, or None. Strides values.  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)  If  data_format='channels_first' :\n5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)   Output shape   If  data_format='channels_last' :\n5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)  If  data_format='channels_first' :\n5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)    [source]",
            "title": "MaxPooling3D"
        },
        {
            "location": "/layers/pooling/#averagepooling1d",
            "text": "keras.layers.AveragePooling1D(pool_size=2, strides=None, padding='valid')  Average pooling for temporal data.  Arguments   pool_size : Integer, size of the average pooling windows.  strides : Integer, or None. Factor by which to downscale.\nE.g. 2 will halve the input.\nIf None, it will default to  pool_size .  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).   Input shape  3D tensor with shape:  (batch_size, steps, features) .  Output shape  3D tensor with shape:  (batch_size, downsampled_steps, features) .   [source]",
            "title": "AveragePooling1D"
        },
        {
            "location": "/layers/pooling/#averagepooling2d",
            "text": "keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)  Average pooling operation for spatial data.  Arguments   pool_size : integer or tuple of 2 integers,\nfactors by which to downscale (vertical, horizontal).\n(2, 2) will halve the input in both spatial dimension.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.  strides : Integer, tuple of 2 integers, or None.\nStrides values.\nIf None, it will default to  pool_size .  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, rows, cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, rows, cols)   Output shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols)    [source]",
            "title": "AveragePooling2D"
        },
        {
            "location": "/layers/pooling/#averagepooling3d",
            "text": "keras.layers.AveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)  Average pooling operation for 3D data (spatial or spatio-temporal).  Arguments   pool_size : tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.  strides : tuple of 3 integers, or None. Strides values.  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \nwhile  channels_first  corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)  If  data_format='channels_first' :\n5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)   Output shape   If  data_format='channels_last' :\n5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)  If  data_format='channels_first' :\n5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)    [source]",
            "title": "AveragePooling3D"
        },
        {
            "location": "/layers/pooling/#globalmaxpooling1d",
            "text": "keras.layers.GlobalMaxPooling1D()  Global max pooling operation for temporal data.  Input shape  3D tensor with shape:  (batch_size, steps, features) .  Output shape  2D tensor with shape: (batch_size, features)   [source]",
            "title": "GlobalMaxPooling1D"
        },
        {
            "location": "/layers/pooling/#globalaveragepooling1d",
            "text": "keras.layers.GlobalAveragePooling1D()  Global average pooling operation for temporal data.  Input shape  3D tensor with shape:  (batch_size, steps, features) .  Output shape  2D tensor with shape: (batch_size, features)   [source]",
            "title": "GlobalAveragePooling1D"
        },
        {
            "location": "/layers/pooling/#globalmaxpooling2d",
            "text": "keras.layers.GlobalMaxPooling2D(data_format=None)  Global max pooling operation for spatial data.  Arguments   data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, rows, cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, rows, cols)   Output shape  2D tensor with shape: (batch_size, channels)   [source]",
            "title": "GlobalMaxPooling2D"
        },
        {
            "location": "/layers/pooling/#globalaveragepooling2d",
            "text": "keras.layers.GlobalAveragePooling2D(data_format=None)  Global average pooling operation for spatial data.  Arguments   data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".   Input shape   If  data_format='channels_last' :\n4D tensor with shape: (batch_size, rows, cols, channels)  If  data_format='channels_first' :\n4D tensor with shape: (batch_size, channels, rows, cols)   Output shape  2D tensor with shape: (batch_size, channels)",
            "title": "GlobalAveragePooling2D"
        },
        {
            "location": "/layers/local/",
            "text": "[source]\n\n\nLocallyConnected1D\n\n\nkeras.layers.LocallyConnected1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nLocally-connected layer for 1D inputs.\n\n\nThe \nLocallyConnected1D\n layer works similarly to\nthe \nConv1D\n layer, except that weights are unshared,\nthat is, a different set of filters is applied at each different patch\nof the input.\n\n\nExample\n\n\n# apply a unshared weight convolution 1d of length 3 to a sequence with\n# 10 timesteps, with 64 output filters\nmodel = Sequential()\nmodel.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))\n# now model.output_shape == (None, 8, 64)\n# add a new conv1d on top\nmodel.add(LocallyConnected1D(32, 3))\n# now model.output_shape == (None, 6, 32)\n\n\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of a single integer,\nspecifying the length of the 1D convolution window.\n\n\nstrides\n: An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: Currently only supports \n\"valid\"\n (case-insensitive).\n\n\"same\"\n may be supported in the future.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(batch_size, steps, input_dim)\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch_size, new_steps, filters)\n\n\nsteps\n value might have changed due to padding or strides.\n\n\n\n\n[source]\n\n\nLocallyConnected2D\n\n\nkeras.layers.LocallyConnected2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nLocally-connected layer for 2D inputs.\n\n\nThe \nLocallyConnected2D\n layer works similarly\nto the \nConv2D\n layer, except that weights are unshared,\nthat is, a different set of filters is applied at each\ndifferent patch of the input.\n\n\nExamples\n\n\n# apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image\n# with `data_format=\"channels_last\"`:\nmodel = Sequential()\nmodel.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n# now model.output_shape == (None, 30, 30, 64)\n# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters\n\n# add a 3x3 unshared weights convolution on top, with 32 output filters:\nmodel.add(LocallyConnected2D(32, (3, 3)))\n# now model.output_shape == (None, 28, 28, 32)\n\n\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\nstrides\n: An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.\n\n\npadding\n: Currently only support \n\"valid\"\n (case-insensitive).\n\n\"same\"\n will be supported in future.\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, height, width, channels)\n while \nchannels_first\n\ncorresponds to inputs with shape\n\n(batch, channels, height, width)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if data_format='channels_last'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, filters, new_rows, new_cols)\n if data_format='channels_first'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, filters)\n if data_format='channels_last'.\n\nrows\n and \ncols\n values might have changed due to padding.",
            "title": "Locally-connected Layers"
        },
        {
            "location": "/layers/local/#locallyconnected1d",
            "text": "keras.layers.LocallyConnected1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  Locally-connected layer for 1D inputs.  The  LocallyConnected1D  layer works similarly to\nthe  Conv1D  layer, except that weights are unshared,\nthat is, a different set of filters is applied at each different patch\nof the input.  Example  # apply a unshared weight convolution 1d of length 3 to a sequence with\n# 10 timesteps, with 64 output filters\nmodel = Sequential()\nmodel.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))\n# now model.output_shape == (None, 8, 64)\n# add a new conv1d on top\nmodel.add(LocallyConnected1D(32, 3))\n# now model.output_shape == (None, 6, 32)  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of a single integer,\nspecifying the length of the 1D convolution window.  strides : An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : Currently only supports  \"valid\"  (case-insensitive). \"same\"  may be supported in the future.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  3D tensor with shape:  (batch_size, steps, input_dim)  Output shape  3D tensor with shape:  (batch_size, new_steps, filters)  steps  value might have changed due to padding or strides.   [source]",
            "title": "LocallyConnected1D"
        },
        {
            "location": "/layers/local/#locallyconnected2d",
            "text": "keras.layers.LocallyConnected2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  Locally-connected layer for 2D inputs.  The  LocallyConnected2D  layer works similarly\nto the  Conv2D  layer, except that weights are unshared,\nthat is, a different set of filters is applied at each\ndifferent patch of the input.  Examples  # apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image\n# with `data_format=\"channels_last\"`:\nmodel = Sequential()\nmodel.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n# now model.output_shape == (None, 30, 30, 64)\n# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters\n\n# add a 3x3 unshared weights convolution on top, with 32 output filters:\nmodel.add(LocallyConnected2D(32, (3, 3)))\n# now model.output_shape == (None, 28, 28, 32)  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).  kernel_size : An integer or tuple/list of 2 integers, specifying the\nwidth and height of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.  strides : An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the width and height.\nCan be a single integer to specify the same value for\nall spatial dimensions.  padding : Currently only support  \"valid\"  (case-insensitive). \"same\"  will be supported in future.  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, height, width, channels)  while  channels_first \ncorresponds to inputs with shape (batch, channels, height, width) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if data_format='channels_first'\nor 4D tensor with shape: (samples, rows, cols, channels)  if data_format='channels_last'.  Output shape  4D tensor with shape: (samples, filters, new_rows, new_cols)  if data_format='channels_first'\nor 4D tensor with shape: (samples, new_rows, new_cols, filters)  if data_format='channels_last'. rows  and  cols  values might have changed due to padding.",
            "title": "LocallyConnected2D"
        },
        {
            "location": "/layers/recurrent/",
            "text": "[source]\n\n\nRNN\n\n\nkeras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n\n\n\n\nBase class for recurrent layers.\n\n\nArguments\n\n\n\n\ncell\n: A RNN cell instance. A RNN cell is a class that has:\n\n\na \ncall(input_at_t, states_at_t)\n method, returning\n\n(output_at_t, states_at_t_plus_1)\n. The call method of the\ncell can also take the optional argument \nconstants\n, see\nsection \"Note on passing external constants\" below.\n\n\na \nstate_size\n attribute. This can be a single integer\n(single state) in which case it is\nthe size of the recurrent state\n(which should be the same as the size of the cell output).\nThis can also be a list/tuple of integers\n(one size per state). In this case, the first entry\n(\nstate_size[0]\n) should be the same as\nthe size of the cell output.\nIt is also possible for \ncell\n to be a list of RNN cell instances,\nin which cases the cells get stacked on after the other in the RNN,\nimplementing an efficient stacked RNN.\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\ngo_backwards\n: Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\nunroll\n: Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.\n\n\ninput_dim\n: dimensionality of the input (integer).\nThis argument (or alternatively,\nthe keyword argument \ninput_shape\n)\nis required when using this layer as the first layer in a model.\n\n\ninput_length\n: Length of input sequences, to be specified\nwhen it is constant.\nThis argument is required if you are going to connect\n\nFlatten\n then \nDense\n layers upstream\n(without it, the shape of the dense outputs cannot be computed).\nNote that if the recurrent layer is not the first layer\nin your model, you would need to specify the input length\nat the level of the first layer\n(e.g. via the \ninput_shape\n argument)\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(batch_size, timesteps, input_dim)\n.\n\n\nOutput shape\n\n\n\n\nif \nreturn_state\n: a list of tensors. The first tensor is\nthe output. The remaining tensors are the last states,\neach with shape \n(batch_size, units)\n.\n\n\nif \nreturn_sequences\n: 3D tensor with shape\n\n(batch_size, timesteps, units)\n.\n\n\nelse, 2D tensor with shape \n(batch_size, units)\n.\n\n\n\n\nMasking\n\n\nThis layer supports masking for input data with a variable number\nof timesteps. To introduce masks to your data,\nuse an \nEmbedding\n layer with the \nmask_zero\n parameter\nset to \nTrue\n.\n\n\nNote on using statefulness in RNNs\n\n\nYou can set RNN layers to be 'stateful', which means that the states\ncomputed for the samples in one batch will be reused as initial states\nfor the samples in the next batch. This assumes a one-to-one mapping\nbetween samples in different successive batches.\n\n\nTo enable statefulness:\n- specify \nstateful=True\n in the layer constructor.\n- specify a fixed batch size for your model, by passing\nif sequential model:\n\nbatch_input_shape=(...)\n to the first layer in your model.\nelse for functional model with 1 or more Input layers:\n\nbatch_shape=(...)\n to all the first layers in your model.\nThis is the expected shape of your inputs\n\nincluding the batch size\n.\nIt should be a tuple of integers, e.g. \n(32, 10, 100)\n.\n- specify \nshuffle=False\n when calling fit().\n\n\nTo reset the states of your model, call \n.reset_states()\n on either\na specific layer, or on your entire model.\n\n\nNote on specifying the initial state of RNNs\n\n\nYou can specify the initial state of RNN layers symbolically by\ncalling them with the keyword argument \ninitial_state\n. The value of\n\ninitial_state\n should be a tensor or list of tensors representing\nthe initial state of the RNN layer.\n\n\nYou can specify the initial state of RNN layers numerically by\ncalling \nreset_states\n with the keyword argument \nstates\n. The value of\n\nstates\n should be a numpy array or list of numpy arrays representing\nthe initial state of the RNN layer.\n\n\nNote on passing external constants to RNNs\n\n\nYou can pass \"external\" constants to the cell using the \nconstants\n\nkeyword argument of \nRNN.__call__\n (as well as \nRNN.call\n) method. This\nrequires that the \ncell.call\n method accepts the same keyword argument\n\nconstants\n. Such constants can be used to condition the cell\ntransformation on additional static inputs (not changing over time),\na.k.a. an attention mechanism.\n\n\nExamples\n\n\n# First, let's define a RNN Cell, as a layer subclass.\n\nclass MinimalRNNCell(keras.layers.Layer):\n\n    def __init__(self, units, **kwargs):\n        self.units = units\n        self.state_size = units\n        super(MinimalRNNCell, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      initializer='uniform',\n                                      name='kernel')\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            initializer='uniform',\n            name='recurrent_kernel')\n        self.built = True\n\n    def call(self, inputs, states):\n        prev_output = states[0]\n        h = K.dot(inputs, self.kernel)\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        return output, [output]\n\n# Let's use this cell in a RNN layer:\n\ncell = MinimalRNNCell(32)\nx = keras.Input((None, 5))\nlayer = RNN(cell)\ny = layer(x)\n\n# Here's how to use the cell to build a stacked RNN:\n\ncells = [MinimalRNNCell(32), MinimalRNNCell(64)]\nx = keras.Input((None, 5))\nlayer = RNN(cells)\ny = layer(x)\n\n\n\n\n\n\n[source]\n\n\nSimpleRNN\n\n\nkeras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n\n\n\n\nFully-connected RNN where the output is to be fed back to input.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\ngo_backwards\n: Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\nunroll\n: Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.\n\n\n\n\n\n\n[source]\n\n\nGRU\n\n\nkeras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False)\n\n\n\n\nGated Recurrent Unit - Cho et al. 2014.\n\n\nThere are two variants. The default one is based on 1406.1078v3 and\nhas reset gate applied to hidden state before matrix multiplication. The\nother one is based on original 1406.1078v1 and has the order reversed.\n\n\nThe second variant is compatible with CuDNNGRU (GPU-only) and allows\ninference on CPU. Thus it has separate biases for \nkernel\n and\n\nrecurrent_kernel\n. Use \n'reset_after'=True\n and\n\nrecurrent_activation='sigmoid'\n.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nrecurrent_activation\n: Activation function to use\nfor the recurrent step\n(see \nactivations\n).\n\n\nDefault\n: hard sigmoid (\nhard_sigmoid\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\nimplementation\n: Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\ngo_backwards\n: Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\nunroll\n: Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.\n\n\nreset_after\n: GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (CuDNN compatible).\n\n\n\n\nReferences\n\n\n\n\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n\n\nOn the Properties of Neural Machine Translation: Encoder-Decoder Approaches\n\n\nEmpirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n\n\n\n\n\n\n[source]\n\n\nLSTM\n\n\nkeras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n\n\n\n\nLong Short-Term Memory layer - Hochreiter 1997.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nrecurrent_activation\n: Activation function to use\nfor the recurrent step\n(see \nactivations\n).\n\n\nDefault\n: hard sigmoid (\nhard_sigmoid\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs.\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state.\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nunit_forget_bias\n: Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force \nbias_initializer=\"zeros\"\n.\nThis is recommended in \nJozefowicz et al.\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\nimplementation\n: Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\ngo_backwards\n: Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\nunroll\n: Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.\n\n\n\n\nReferences\n\n\n\n\nLong short-term memory\n (original 1997 paper)\n\n\nLearning to forget: Continual prediction with LSTM\n\n\nSupervised sequence labeling with recurrent neural networks\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n\n\n\n\n\n\n[source]\n\n\nConvLSTM2D\n\n\nkeras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0)\n\n\n\n\nConvolutional LSTM.\n\n\nIt is similar to an LSTM layer, but the input transformations\nand recurrent transformations are both convolutional.\n\n\nArguments\n\n\n\n\nfilters\n: Integer, the dimensionality of the output space\n(i.e. the number output of filters in the convolution).\n\n\nkernel_size\n: An integer or tuple/list of n integers, specifying the\ndimensions of the convolution window.\n\n\nstrides\n: An integer or tuple/list of n integers,\nspecifying the strides of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany \ndilation_rate\n value != 1.\n\n\npadding\n: One of \n\"valid\"\n or \n\"same\"\n (case-insensitive).\n\n\ndata_format\n: A string,\none of \nchannels_last\n (default) or \nchannels_first\n.\nThe ordering of the dimensions in the inputs.\n\nchannels_last\n corresponds to inputs with shape\n\n(batch, time, ..., channels)\n\nwhile \nchannels_first\n corresponds to\ninputs with shape \n(batch, time, channels, ...)\n.\nIt defaults to the \nimage_data_format\n value found in your\nKeras config file at \n~/.keras/keras.json\n.\nIf you never set it, then it will be \"channels_last\".\n\n\ndilation_rate\n: An integer or tuple/list of n integers, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any \ndilation_rate\n value != 1 is\nincompatible with specifying any \nstrides\n value != 1.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nrecurrent_activation\n: Activation function to use\nfor the recurrent step\n(see \nactivations\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs.\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state.\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nunit_forget_bias\n: Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nUse in combination with \nbias_initializer=\"zeros\"\n.\nThis is recommended in \nJozefowicz et al.\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\nin the output sequence, or the full sequence.\n\n\ngo_backwards\n: Boolean (default False).\nIf True, process the input sequence backwards.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\n\n\nInput shape\n\n\n\n\nif data_format='channels_first'\n5D tensor with shape:\n\n(samples,time, channels, rows, cols)\n\n\nif data_format='channels_last'\n5D tensor with shape:\n\n(samples,time, rows, cols, channels)\n\n\n\n\nOutput shape\n\n\n\n\nif \nreturn_sequences\n\n\nif data_format='channels_first'\n5D tensor with shape:\n\n(samples, time, filters, output_row, output_col)\n\n\nif data_format='channels_last'\n5D tensor with shape:\n\n(samples, time, output_row, output_col, filters)\n\n\nelse\n\n\nif data_format ='channels_first'\n4D tensor with shape:\n\n(samples, filters, output_row, output_col)\n\n\nif data_format='channels_last'\n4D tensor with shape:\n\n(samples, output_row, output_col, filters)\n\nwhere o_row and o_col depend on the shape of the filter and\nthe padding\n\n\n\n\nRaises\n\n\n\n\nValueError\n: in case of invalid constructor arguments.\n\n\n\n\nReferences\n\n\n\n\nConvolutional LSTM Network: A Machine Learning Approach for\nPrecipitation Nowcasting\n\nThe current implementation does not include the feedback loop on the\ncells output\n\n\n\n\n\n\n[source]\n\n\nSimpleRNNCell\n\n\nkeras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n\n\n\n\nCell class for SimpleRNN.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\n\n\n\n\n[source]\n\n\nGRUCell\n\n\nkeras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False)\n\n\n\n\nCell class for the GRU layer.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nrecurrent_activation\n: Activation function to use\nfor the recurrent step\n(see \nactivations\n).\n\n\nDefault\n: hard sigmoid (\nhard_sigmoid\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\nimplementation\n: Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.\n\n\nreset_after\n: GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (CuDNN compatible).\n\n\n\n\n\n\n[source]\n\n\nLSTMCell\n\n\nkeras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1)\n\n\n\n\nCell class for the LSTM layer.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\n\n\nDefault\n: hyperbolic tangent (\ntanh\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nrecurrent_activation\n: Activation function to use\nfor the recurrent step\n(see \nactivations\n).\n\n\nDefault\n: hard sigmoid (\nhard_sigmoid\n).\nIf you pass \nNone\n, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).x\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nunit_forget_bias\n: Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force \nbias_initializer=\"zeros\"\n.\nThis is recommended in \nJozefowicz et al.\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\ndropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.\n\n\nrecurrent_dropout\n: Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.\n\n\nimplementation\n: Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.\n\n\n\n\n\n\n[source]\n\n\nStackedRNNCells\n\n\nkeras.layers.StackedRNNCells(cells)\n\n\n\n\nWrapper allowing a stack of RNN cells to behave as a single cell.\n\n\nUsed to implement efficient stacked RNNs.\n\n\nArguments\n\n\n\n\ncells\n: List of RNN cell instances.\n\n\n\n\nExamples\n\n\ncells = [\n    keras.layers.LSTMCell(output_dim),\n    keras.layers.LSTMCell(output_dim),\n    keras.layers.LSTMCell(output_dim),\n]\n\ninputs = keras.Input((timesteps, input_dim))\nx = keras.layers.RNN(cells)(inputs)\n\n\n\n\n\n\n[source]\n\n\nCuDNNGRU\n\n\nkeras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)\n\n\n\n\nFast GRU implementation backed by \nCuDNN\n.\n\n\nCan only be run on GPU, with the TensorFlow backend.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs.\n(see \ninitializers\n).\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state.\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\nreturn_sequences\n: Boolean. Whether to return the last output.\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n\n\n\n\n\n\n[source]\n\n\nCuDNNLSTM\n\n\nkeras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)\n\n\n\n\nFast LSTM implementation backed by \nCuDNN\n.\n\n\nCan only be run on GPU, with the TensorFlow backend.\n\n\nArguments\n\n\n\n\nunits\n: Positive integer, dimensionality of the output space.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix,\nused for the linear transformation of the inputs.\n(see \ninitializers\n).\n\n\nunit_forget_bias\n: Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force \nbias_initializer=\"zeros\"\n.\nThis is recommended in \nJozefowicz et al.\n\n\nrecurrent_initializer\n: Initializer for the \nrecurrent_kernel\n\nweights matrix,\nused for the linear transformation of the recurrent state.\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nrecurrent_regularizer\n: Regularizer function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to\nthe \nkernel\n weights matrix\n(see \nconstraints\n).\n\n\nrecurrent_constraint\n: Constraint function applied to\nthe \nrecurrent_kernel\n weights matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\nreturn_sequences\n: Boolean. Whether to return the last output.\nin the output sequence, or the full sequence.\n\n\nreturn_state\n: Boolean. Whether to return the last state\nin addition to the output.\n\n\nstateful\n: Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.",
            "title": "Recurrent Layers"
        },
        {
            "location": "/layers/recurrent/#rnn",
            "text": "keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)  Base class for recurrent layers.  Arguments   cell : A RNN cell instance. A RNN cell is a class that has:  a  call(input_at_t, states_at_t)  method, returning (output_at_t, states_at_t_plus_1) . The call method of the\ncell can also take the optional argument  constants , see\nsection \"Note on passing external constants\" below.  a  state_size  attribute. This can be a single integer\n(single state) in which case it is\nthe size of the recurrent state\n(which should be the same as the size of the cell output).\nThis can also be a list/tuple of integers\n(one size per state). In this case, the first entry\n( state_size[0] ) should be the same as\nthe size of the cell output.\nIt is also possible for  cell  to be a list of RNN cell instances,\nin which cases the cells get stacked on after the other in the RNN,\nimplementing an efficient stacked RNN.  return_sequences : Boolean. Whether to return the last output\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  go_backwards : Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.  unroll : Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.  input_dim : dimensionality of the input (integer).\nThis argument (or alternatively,\nthe keyword argument  input_shape )\nis required when using this layer as the first layer in a model.  input_length : Length of input sequences, to be specified\nwhen it is constant.\nThis argument is required if you are going to connect Flatten  then  Dense  layers upstream\n(without it, the shape of the dense outputs cannot be computed).\nNote that if the recurrent layer is not the first layer\nin your model, you would need to specify the input length\nat the level of the first layer\n(e.g. via the  input_shape  argument)   Input shape  3D tensor with shape  (batch_size, timesteps, input_dim) .  Output shape   if  return_state : a list of tensors. The first tensor is\nthe output. The remaining tensors are the last states,\neach with shape  (batch_size, units) .  if  return_sequences : 3D tensor with shape (batch_size, timesteps, units) .  else, 2D tensor with shape  (batch_size, units) .   Masking  This layer supports masking for input data with a variable number\nof timesteps. To introduce masks to your data,\nuse an  Embedding  layer with the  mask_zero  parameter\nset to  True .  Note on using statefulness in RNNs  You can set RNN layers to be 'stateful', which means that the states\ncomputed for the samples in one batch will be reused as initial states\nfor the samples in the next batch. This assumes a one-to-one mapping\nbetween samples in different successive batches.  To enable statefulness:\n- specify  stateful=True  in the layer constructor.\n- specify a fixed batch size for your model, by passing\nif sequential model: batch_input_shape=(...)  to the first layer in your model.\nelse for functional model with 1 or more Input layers: batch_shape=(...)  to all the first layers in your model.\nThis is the expected shape of your inputs including the batch size .\nIt should be a tuple of integers, e.g.  (32, 10, 100) .\n- specify  shuffle=False  when calling fit().  To reset the states of your model, call  .reset_states()  on either\na specific layer, or on your entire model.  Note on specifying the initial state of RNNs  You can specify the initial state of RNN layers symbolically by\ncalling them with the keyword argument  initial_state . The value of initial_state  should be a tensor or list of tensors representing\nthe initial state of the RNN layer.  You can specify the initial state of RNN layers numerically by\ncalling  reset_states  with the keyword argument  states . The value of states  should be a numpy array or list of numpy arrays representing\nthe initial state of the RNN layer.  Note on passing external constants to RNNs  You can pass \"external\" constants to the cell using the  constants \nkeyword argument of  RNN.__call__  (as well as  RNN.call ) method. This\nrequires that the  cell.call  method accepts the same keyword argument constants . Such constants can be used to condition the cell\ntransformation on additional static inputs (not changing over time),\na.k.a. an attention mechanism.  Examples  # First, let's define a RNN Cell, as a layer subclass.\n\nclass MinimalRNNCell(keras.layers.Layer):\n\n    def __init__(self, units, **kwargs):\n        self.units = units\n        self.state_size = units\n        super(MinimalRNNCell, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      initializer='uniform',\n                                      name='kernel')\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            initializer='uniform',\n            name='recurrent_kernel')\n        self.built = True\n\n    def call(self, inputs, states):\n        prev_output = states[0]\n        h = K.dot(inputs, self.kernel)\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        return output, [output]\n\n# Let's use this cell in a RNN layer:\n\ncell = MinimalRNNCell(32)\nx = keras.Input((None, 5))\nlayer = RNN(cell)\ny = layer(x)\n\n# Here's how to use the cell to build a stacked RNN:\n\ncells = [MinimalRNNCell(32), MinimalRNNCell(64)]\nx = keras.Input((None, 5))\nlayer = RNN(cells)\ny = layer(x)   [source]",
            "title": "RNN"
        },
        {
            "location": "/layers/recurrent/#simplernn",
            "text": "keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)  Fully-connected RNN where the output is to be fed back to input.  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.  return_sequences : Boolean. Whether to return the last output\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  go_backwards : Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.  unroll : Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.    [source]",
            "title": "SimpleRNN"
        },
        {
            "location": "/layers/recurrent/#gru",
            "text": "keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False)  Gated Recurrent Unit - Cho et al. 2014.  There are two variants. The default one is based on 1406.1078v3 and\nhas reset gate applied to hidden state before matrix multiplication. The\nother one is based on original 1406.1078v1 and has the order reversed.  The second variant is compatible with CuDNNGRU (GPU-only) and allows\ninference on CPU. Thus it has separate biases for  kernel  and recurrent_kernel . Use  'reset_after'=True  and recurrent_activation='sigmoid' .  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  recurrent_activation : Activation function to use\nfor the recurrent step\n(see  activations ).  Default : hard sigmoid ( hard_sigmoid ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.  implementation : Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.  return_sequences : Boolean. Whether to return the last output\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  go_backwards : Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.  unroll : Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.  reset_after : GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (CuDNN compatible).   References   Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation  On the Properties of Neural Machine Translation: Encoder-Decoder Approaches  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks    [source]",
            "title": "GRU"
        },
        {
            "location": "/layers/recurrent/#lstm",
            "text": "keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)  Long Short-Term Memory layer - Hochreiter 1997.  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  recurrent_activation : Activation function to use\nfor the recurrent step\n(see  activations ).  Default : hard sigmoid ( hard_sigmoid ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs.\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state.\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  unit_forget_bias : Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force  bias_initializer=\"zeros\" .\nThis is recommended in  Jozefowicz et al.  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.  implementation : Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.  return_sequences : Boolean. Whether to return the last output\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  go_backwards : Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.  unroll : Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.   References   Long short-term memory  (original 1997 paper)  Learning to forget: Continual prediction with LSTM  Supervised sequence labeling with recurrent neural networks  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks    [source]",
            "title": "LSTM"
        },
        {
            "location": "/layers/recurrent/#convlstm2d",
            "text": "keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0)  Convolutional LSTM.  It is similar to an LSTM layer, but the input transformations\nand recurrent transformations are both convolutional.  Arguments   filters : Integer, the dimensionality of the output space\n(i.e. the number output of filters in the convolution).  kernel_size : An integer or tuple/list of n integers, specifying the\ndimensions of the convolution window.  strides : An integer or tuple/list of n integers,\nspecifying the strides of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany  dilation_rate  value != 1.  padding : One of  \"valid\"  or  \"same\"  (case-insensitive).  data_format : A string,\none of  channels_last  (default) or  channels_first .\nThe ordering of the dimensions in the inputs. channels_last  corresponds to inputs with shape (batch, time, ..., channels) \nwhile  channels_first  corresponds to\ninputs with shape  (batch, time, channels, ...) .\nIt defaults to the  image_data_format  value found in your\nKeras config file at  ~/.keras/keras.json .\nIf you never set it, then it will be \"channels_last\".  dilation_rate : An integer or tuple/list of n integers, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any  dilation_rate  value != 1 is\nincompatible with specifying any  strides  value != 1.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  recurrent_activation : Activation function to use\nfor the recurrent step\n(see  activations ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs.\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state.\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  unit_forget_bias : Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nUse in combination with  bias_initializer=\"zeros\" .\nThis is recommended in  Jozefowicz et al.  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  return_sequences : Boolean. Whether to return the last output\nin the output sequence, or the full sequence.  go_backwards : Boolean (default False).\nIf True, process the input sequence backwards.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.   Input shape   if data_format='channels_first'\n5D tensor with shape: (samples,time, channels, rows, cols)  if data_format='channels_last'\n5D tensor with shape: (samples,time, rows, cols, channels)   Output shape   if  return_sequences  if data_format='channels_first'\n5D tensor with shape: (samples, time, filters, output_row, output_col)  if data_format='channels_last'\n5D tensor with shape: (samples, time, output_row, output_col, filters)  else  if data_format ='channels_first'\n4D tensor with shape: (samples, filters, output_row, output_col)  if data_format='channels_last'\n4D tensor with shape: (samples, output_row, output_col, filters) \nwhere o_row and o_col depend on the shape of the filter and\nthe padding   Raises   ValueError : in case of invalid constructor arguments.   References   Convolutional LSTM Network: A Machine Learning Approach for\nPrecipitation Nowcasting \nThe current implementation does not include the feedback loop on the\ncells output    [source]",
            "title": "ConvLSTM2D"
        },
        {
            "location": "/layers/recurrent/#simplernncell",
            "text": "keras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)  Cell class for SimpleRNN.  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.    [source]",
            "title": "SimpleRNNCell"
        },
        {
            "location": "/layers/recurrent/#grucell",
            "text": "keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False)  Cell class for the GRU layer.  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  recurrent_activation : Activation function to use\nfor the recurrent step\n(see  activations ).  Default : hard sigmoid ( hard_sigmoid ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.  implementation : Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.  reset_after : GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (CuDNN compatible).    [source]",
            "title": "GRUCell"
        },
        {
            "location": "/layers/recurrent/#lstmcell",
            "text": "keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1)  Cell class for the LSTM layer.  Arguments   units : Positive integer, dimensionality of the output space.  activation : Activation function to use\n(see  activations ).  Default : hyperbolic tangent ( tanh ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  recurrent_activation : Activation function to use\nfor the recurrent step\n(see  activations ).  Default : hard sigmoid ( hard_sigmoid ).\nIf you pass  None , no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).x  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  unit_forget_bias : Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force  bias_initializer=\"zeros\" .\nThis is recommended in  Jozefowicz et al.  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.  recurrent_dropout : Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.  implementation : Implementation mode, either 1 or 2.\nMode 1 will structure its operations as a larger number of\nsmaller dot products and additions, whereas mode 2 will\nbatch them into fewer, larger operations. These modes will\nhave different performance profiles on different hardware and\nfor different applications.    [source]",
            "title": "LSTMCell"
        },
        {
            "location": "/layers/recurrent/#stackedrnncells",
            "text": "keras.layers.StackedRNNCells(cells)  Wrapper allowing a stack of RNN cells to behave as a single cell.  Used to implement efficient stacked RNNs.  Arguments   cells : List of RNN cell instances.   Examples  cells = [\n    keras.layers.LSTMCell(output_dim),\n    keras.layers.LSTMCell(output_dim),\n    keras.layers.LSTMCell(output_dim),\n]\n\ninputs = keras.Input((timesteps, input_dim))\nx = keras.layers.RNN(cells)(inputs)   [source]",
            "title": "StackedRNNCells"
        },
        {
            "location": "/layers/recurrent/#cudnngru",
            "text": "keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)  Fast GRU implementation backed by  CuDNN .  Can only be run on GPU, with the TensorFlow backend.  Arguments   units : Positive integer, dimensionality of the output space.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs.\n(see  initializers ).  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state.\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  return_sequences : Boolean. Whether to return the last output.\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.    [source]",
            "title": "CuDNNGRU"
        },
        {
            "location": "/layers/recurrent/#cudnnlstm",
            "text": "keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)  Fast LSTM implementation backed by  CuDNN .  Can only be run on GPU, with the TensorFlow backend.  Arguments   units : Positive integer, dimensionality of the output space.  kernel_initializer : Initializer for the  kernel  weights matrix,\nused for the linear transformation of the inputs.\n(see  initializers ).  unit_forget_bias : Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force  bias_initializer=\"zeros\" .\nThis is recommended in  Jozefowicz et al.  recurrent_initializer : Initializer for the  recurrent_kernel \nweights matrix,\nused for the linear transformation of the recurrent state.\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  recurrent_regularizer : Regularizer function applied to\nthe  recurrent_kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to\nthe  kernel  weights matrix\n(see  constraints ).  recurrent_constraint : Constraint function applied to\nthe  recurrent_kernel  weights matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).  return_sequences : Boolean. Whether to return the last output.\nin the output sequence, or the full sequence.  return_state : Boolean. Whether to return the last state\nin addition to the output.  stateful : Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.",
            "title": "CuDNNLSTM"
        },
        {
            "location": "/layers/embeddings/",
            "text": "[source]\n\n\nEmbedding\n\n\nkeras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n\n\n\n\nTurns positive integers (indexes) into dense vectors of fixed size.\neg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\n\nThis layer can only be used as the first layer in a model.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Embedding(1000, 64, input_length=10))\n# the model will take as input an integer matrix of size (batch, input_length).\n# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n\ninput_array = np.random.randint(1000, size=(32, 10))\n\nmodel.compile('rmsprop', 'mse')\noutput_array = model.predict(input_array)\nassert output_array.shape == (32, 10, 64)\n\n\n\n\nArguments\n\n\n\n\ninput_dim\n: int > 0. Size of the vocabulary,\ni.e. maximum integer index + 1.\n\n\noutput_dim\n: int >= 0. Dimension of the dense embedding.\n\n\nembeddings_initializer\n: Initializer for the \nembeddings\n matrix\n(see \ninitializers\n).\n\n\nembeddings_regularizer\n: Regularizer function applied to\nthe \nembeddings\n matrix\n(see \nregularizer\n).\n\n\nembeddings_constraint\n: Constraint function applied to\nthe \nembeddings\n matrix\n(see \nconstraints\n).\n\n\nmask_zero\n: Whether or not the input value 0 is a special \"padding\"\nvalue that should be masked out.\nThis is useful when using \nrecurrent layers\n\nwhich may take variable length input.\nIf this is \nTrue\n then all subsequent layers\nin the model need to support masking or an exception will be raised.\nIf mask_zero is set to True, as a consequence, index 0 cannot be\nused in the vocabulary (input_dim should equal size of\nvocabulary + 1).\n\n\ninput_length\n: Length of input sequences, when it is constant.\nThis argument is required if you are going to connect\n\nFlatten\n then \nDense\n layers upstream\n(without it, the shape of the dense outputs cannot be computed).\n\n\n\n\nInput shape\n\n\n2D tensor with shape: \n(batch_size, sequence_length)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(batch_size, sequence_length, output_dim)\n.\n\n\nReferences\n\n\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
            "title": "Embedding Layers"
        },
        {
            "location": "/layers/embeddings/#embedding",
            "text": "keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)  Turns positive integers (indexes) into dense vectors of fixed size.\neg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]  This layer can only be used as the first layer in a model.  Example  model = Sequential()\nmodel.add(Embedding(1000, 64, input_length=10))\n# the model will take as input an integer matrix of size (batch, input_length).\n# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n\ninput_array = np.random.randint(1000, size=(32, 10))\n\nmodel.compile('rmsprop', 'mse')\noutput_array = model.predict(input_array)\nassert output_array.shape == (32, 10, 64)  Arguments   input_dim : int > 0. Size of the vocabulary,\ni.e. maximum integer index + 1.  output_dim : int >= 0. Dimension of the dense embedding.  embeddings_initializer : Initializer for the  embeddings  matrix\n(see  initializers ).  embeddings_regularizer : Regularizer function applied to\nthe  embeddings  matrix\n(see  regularizer ).  embeddings_constraint : Constraint function applied to\nthe  embeddings  matrix\n(see  constraints ).  mask_zero : Whether or not the input value 0 is a special \"padding\"\nvalue that should be masked out.\nThis is useful when using  recurrent layers \nwhich may take variable length input.\nIf this is  True  then all subsequent layers\nin the model need to support masking or an exception will be raised.\nIf mask_zero is set to True, as a consequence, index 0 cannot be\nused in the vocabulary (input_dim should equal size of\nvocabulary + 1).  input_length : Length of input sequences, when it is constant.\nThis argument is required if you are going to connect Flatten  then  Dense  layers upstream\n(without it, the shape of the dense outputs cannot be computed).   Input shape  2D tensor with shape:  (batch_size, sequence_length) .  Output shape  3D tensor with shape:  (batch_size, sequence_length, output_dim) .  References   A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
            "title": "Embedding"
        },
        {
            "location": "/layers/merge/",
            "text": "[source]\n\n\nAdd\n\n\nkeras.layers.Add()\n\n\n\n\nLayer that adds a list of inputs.\n\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).\n\n\nExamples\n\n\nimport keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nadded = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])\n\nout = keras.layers.Dense(4)(added)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n\n\n\n\n\n[source]\n\n\nSubtract\n\n\nkeras.layers.Subtract()\n\n\n\n\nLayer that subtracts two inputs.\n\n\nIt takes as input a list of tensors of size 2,\nboth of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\nalso of the same shape.\n\n\nExamples\n\n\nimport keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\n# Equivalent to subtracted = keras.layers.subtract([x1, x2])\nsubtracted = keras.layers.Subtract()([x1, x2])\n\nout = keras.layers.Dense(4)(subtracted)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n\n\n\n\n\n[source]\n\n\nMultiply\n\n\nkeras.layers.Multiply()\n\n\n\n\nLayer that multiplies (element-wise) a list of inputs.\n\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).\n\n\n\n\n[source]\n\n\nAverage\n\n\nkeras.layers.Average()\n\n\n\n\nLayer that averages a list of inputs.\n\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).\n\n\n\n\n[source]\n\n\nMaximum\n\n\nkeras.layers.Maximum()\n\n\n\n\nLayer that computes the maximum (element-wise) a list of inputs.\n\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).\n\n\n\n\n[source]\n\n\nConcatenate\n\n\nkeras.layers.Concatenate(axis=-1)\n\n\n\n\nLayer that concatenates a list of inputs.\n\n\nIt takes as input a list of tensors,\nall of the same shape except for the concatenation axis,\nand returns a single tensor, the concatenation of all inputs.\n\n\nArguments\n\n\n\n\naxis\n: Axis along which to concatenate.\n\n\n**kwargs\n: standard layer keyword arguments.\n\n\n\n\n\n\n[source]\n\n\nDot\n\n\nkeras.layers.Dot(axes, normalize=False)\n\n\n\n\nLayer that computes a dot product between samples in two tensors.\n\n\nE.g. if applied to two tensors \na\n and \nb\n of shape \n(batch_size, n)\n,\nthe output will be a tensor of shape \n(batch_size, 1)\n\nwhere each entry \ni\n will be the dot product between\n\na[i]\n and \nb[i]\n.\n\n\nArguments\n\n\n\n\naxes\n: Integer or tuple of integers,\naxis or axes along which to take the dot product.\n\n\nnormalize\n: Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\n\n\nadd\n\n\nkeras.layers.add(inputs)\n\n\n\n\nFunctional interface to the \nAdd\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the sum of the inputs.\n\n\nExamples\n\n\nimport keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nadded = keras.layers.add([x1, x2])\n\nout = keras.layers.Dense(4)(added)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n\n\n\n\n\nsubtract\n\n\nkeras.layers.subtract(inputs)\n\n\n\n\nFunctional interface to the \nSubtract\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (exactly 2).\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the difference of the inputs.\n\n\nExamples\n\n\nimport keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nsubtracted = keras.layers.subtract([x1, x2])\n\nout = keras.layers.Dense(4)(subtracted)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n\n\n\n\n\nmultiply\n\n\nkeras.layers.multiply(inputs)\n\n\n\n\nFunctional interface to the \nMultiply\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the element-wise product of the inputs.\n\n\n\n\naverage\n\n\nkeras.layers.average(inputs)\n\n\n\n\nFunctional interface to the \nAverage\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the average of the inputs.\n\n\n\n\nmaximum\n\n\nkeras.layers.maximum(inputs)\n\n\n\n\nFunctional interface to the \nMaximum\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the element-wise maximum of the inputs.\n\n\n\n\nconcatenate\n\n\nkeras.layers.concatenate(inputs, axis=-1)\n\n\n\n\nFunctional interface to the \nConcatenate\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\naxis\n: Concatenation axis.\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the concatenation of the inputs alongside axis \naxis\n.\n\n\n\n\ndot\n\n\nkeras.layers.dot(inputs, axes, normalize=False)\n\n\n\n\nFunctional interface to the \nDot\n layer.\n\n\nArguments\n\n\n\n\ninputs\n: A list of input tensors (at least 2).\n\n\naxes\n: Integer or tuple of integers,\naxis or axes along which to take the dot product.\n\n\nnormalize\n: Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.\n\n\n**kwargs\n: Standard layer keyword arguments.\n\n\n\n\nReturns\n\n\nA tensor, the dot product of the samples from the inputs.",
            "title": "Merge Layers"
        },
        {
            "location": "/layers/merge/#add",
            "text": "keras.layers.Add()  Layer that adds a list of inputs.  It takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).  Examples  import keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nadded = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])\n\nout = keras.layers.Dense(4)(added)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)   [source]",
            "title": "Add"
        },
        {
            "location": "/layers/merge/#subtract",
            "text": "keras.layers.Subtract()  Layer that subtracts two inputs.  It takes as input a list of tensors of size 2,\nboth of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\nalso of the same shape.  Examples  import keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\n# Equivalent to subtracted = keras.layers.subtract([x1, x2])\nsubtracted = keras.layers.Subtract()([x1, x2])\n\nout = keras.layers.Dense(4)(subtracted)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)   [source]",
            "title": "Subtract"
        },
        {
            "location": "/layers/merge/#multiply",
            "text": "keras.layers.Multiply()  Layer that multiplies (element-wise) a list of inputs.  It takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).   [source]",
            "title": "Multiply"
        },
        {
            "location": "/layers/merge/#average",
            "text": "keras.layers.Average()  Layer that averages a list of inputs.  It takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).   [source]",
            "title": "Average"
        },
        {
            "location": "/layers/merge/#maximum",
            "text": "keras.layers.Maximum()  Layer that computes the maximum (element-wise) a list of inputs.  It takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).   [source]",
            "title": "Maximum"
        },
        {
            "location": "/layers/merge/#concatenate",
            "text": "keras.layers.Concatenate(axis=-1)  Layer that concatenates a list of inputs.  It takes as input a list of tensors,\nall of the same shape except for the concatenation axis,\nand returns a single tensor, the concatenation of all inputs.  Arguments   axis : Axis along which to concatenate.  **kwargs : standard layer keyword arguments.    [source]",
            "title": "Concatenate"
        },
        {
            "location": "/layers/merge/#dot",
            "text": "keras.layers.Dot(axes, normalize=False)  Layer that computes a dot product between samples in two tensors.  E.g. if applied to two tensors  a  and  b  of shape  (batch_size, n) ,\nthe output will be a tensor of shape  (batch_size, 1) \nwhere each entry  i  will be the dot product between a[i]  and  b[i] .  Arguments   axes : Integer or tuple of integers,\naxis or axes along which to take the dot product.  normalize : Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.  **kwargs : Standard layer keyword arguments.",
            "title": "Dot"
        },
        {
            "location": "/layers/merge/#add_1",
            "text": "keras.layers.add(inputs)  Functional interface to the  Add  layer.  Arguments   inputs : A list of input tensors (at least 2).  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the sum of the inputs.  Examples  import keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nadded = keras.layers.add([x1, x2])\n\nout = keras.layers.Dense(4)(added)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)",
            "title": "add"
        },
        {
            "location": "/layers/merge/#subtract_1",
            "text": "keras.layers.subtract(inputs)  Functional interface to the  Subtract  layer.  Arguments   inputs : A list of input tensors (exactly 2).  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the difference of the inputs.  Examples  import keras\n\ninput1 = keras.layers.Input(shape=(16,))\nx1 = keras.layers.Dense(8, activation='relu')(input1)\ninput2 = keras.layers.Input(shape=(32,))\nx2 = keras.layers.Dense(8, activation='relu')(input2)\nsubtracted = keras.layers.subtract([x1, x2])\n\nout = keras.layers.Dense(4)(subtracted)\nmodel = keras.models.Model(inputs=[input1, input2], outputs=out)",
            "title": "subtract"
        },
        {
            "location": "/layers/merge/#multiply_1",
            "text": "keras.layers.multiply(inputs)  Functional interface to the  Multiply  layer.  Arguments   inputs : A list of input tensors (at least 2).  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the element-wise product of the inputs.",
            "title": "multiply"
        },
        {
            "location": "/layers/merge/#average_1",
            "text": "keras.layers.average(inputs)  Functional interface to the  Average  layer.  Arguments   inputs : A list of input tensors (at least 2).  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the average of the inputs.",
            "title": "average"
        },
        {
            "location": "/layers/merge/#maximum_1",
            "text": "keras.layers.maximum(inputs)  Functional interface to the  Maximum  layer.  Arguments   inputs : A list of input tensors (at least 2).  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the element-wise maximum of the inputs.",
            "title": "maximum"
        },
        {
            "location": "/layers/merge/#concatenate_1",
            "text": "keras.layers.concatenate(inputs, axis=-1)  Functional interface to the  Concatenate  layer.  Arguments   inputs : A list of input tensors (at least 2).  axis : Concatenation axis.  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the concatenation of the inputs alongside axis  axis .",
            "title": "concatenate"
        },
        {
            "location": "/layers/merge/#dot_1",
            "text": "keras.layers.dot(inputs, axes, normalize=False)  Functional interface to the  Dot  layer.  Arguments   inputs : A list of input tensors (at least 2).  axes : Integer or tuple of integers,\naxis or axes along which to take the dot product.  normalize : Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.  **kwargs : Standard layer keyword arguments.   Returns  A tensor, the dot product of the samples from the inputs.",
            "title": "dot"
        },
        {
            "location": "/layers/advanced-activations/",
            "text": "[source]\n\n\nLeakyReLU\n\n\nkeras.layers.LeakyReLU(alpha=0.3)\n\n\n\n\nLeaky version of a Rectified Linear Unit.\n\n\nIt allows a small gradient when the unit is not active:\n\nf(x) = alpha * x for x < 0\n,\n\nf(x) = x for x >= 0\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha\n: float >= 0. Negative slope coefficient.\n\n\n\n\nReferences\n\n\n\n\nRectifier Nonlinearities Improve Neural Network Acoustic Models\n\n\n\n\n\n\n[source]\n\n\nPReLU\n\n\nkeras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n\n\n\n\nParametric Rectified Linear Unit.\n\n\nIt follows:\n\nf(x) = alpha * x for x < 0\n,\n\nf(x) = x for x >= 0\n,\nwhere \nalpha\n is a learned array with the same shape as x.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha_initializer\n: initializer function for the weights.\n\n\nalpha_regularizer\n: regularizer for the weights.\n\n\nalpha_constraint\n: constraint for the weights.\n\n\nshared_axes\n: the axes along which to share learnable\nparameters for the activation function.\nFor example, if the incoming feature maps\nare from a 2D convolution\nwith output shape \n(batch, height, width, channels)\n,\nand you wish to share parameters across space\nso that each filter only has one set of parameters,\nset \nshared_axes=[1, 2]\n.\n\n\n\n\nReferences\n\n\n\n\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n\n\n\n\n\n\n[source]\n\n\nELU\n\n\nkeras.layers.ELU(alpha=1.0)\n\n\n\n\nExponential Linear Unit.\n\n\nIt follows:\n\nf(x) =  alpha * (exp(x) - 1.) for x < 0\n,\n\nf(x) = x for x >= 0\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha\n: scale for the negative factor.\n\n\n\n\nReferences\n\n\n\n\nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n\n\n\n\n\n\n[source]\n\n\nThresholdedReLU\n\n\nkeras.layers.ThresholdedReLU(theta=1.0)\n\n\n\n\nThresholded Rectified Linear Unit.\n\n\nIt follows:\n\nf(x) = x for x > theta\n,\n\nf(x) = 0 otherwise\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\ntheta\n: float >= 0. Threshold location of activation.\n\n\n\n\nReferences\n\n\n\n\nZero-Bias Autoencoders and the Benefits of Co-Adapting Features\n\n\n\n\n\n\n[source]\n\n\nSoftmax\n\n\nkeras.layers.Softmax(axis=-1)\n\n\n\n\nSoftmax activation function.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\naxis\n: Integer, axis along which the softmax normalization is applied.",
            "title": "Advanced Activations Layers"
        },
        {
            "location": "/layers/advanced-activations/#leakyrelu",
            "text": "keras.layers.LeakyReLU(alpha=0.3)  Leaky version of a Rectified Linear Unit.  It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha : float >= 0. Negative slope coefficient.   References   Rectifier Nonlinearities Improve Neural Network Acoustic Models    [source]",
            "title": "LeakyReLU"
        },
        {
            "location": "/layers/advanced-activations/#prelu",
            "text": "keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)  Parametric Rectified Linear Unit.  It follows: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 ,\nwhere  alpha  is a learned array with the same shape as x.  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha_initializer : initializer function for the weights.  alpha_regularizer : regularizer for the weights.  alpha_constraint : constraint for the weights.  shared_axes : the axes along which to share learnable\nparameters for the activation function.\nFor example, if the incoming feature maps\nare from a 2D convolution\nwith output shape  (batch, height, width, channels) ,\nand you wish to share parameters across space\nso that each filter only has one set of parameters,\nset  shared_axes=[1, 2] .   References   Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification    [source]",
            "title": "PReLU"
        },
        {
            "location": "/layers/advanced-activations/#elu",
            "text": "keras.layers.ELU(alpha=1.0)  Exponential Linear Unit.  It follows: f(x) =  alpha * (exp(x) - 1.) for x < 0 , f(x) = x for x >= 0 .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha : scale for the negative factor.   References   Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)    [source]",
            "title": "ELU"
        },
        {
            "location": "/layers/advanced-activations/#thresholdedrelu",
            "text": "keras.layers.ThresholdedReLU(theta=1.0)  Thresholded Rectified Linear Unit.  It follows: f(x) = x for x > theta , f(x) = 0 otherwise .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   theta : float >= 0. Threshold location of activation.   References   Zero-Bias Autoencoders and the Benefits of Co-Adapting Features    [source]",
            "title": "ThresholdedReLU"
        },
        {
            "location": "/layers/advanced-activations/#softmax",
            "text": "keras.layers.Softmax(axis=-1)  Softmax activation function.  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   axis : Integer, axis along which the softmax normalization is applied.",
            "title": "Softmax"
        },
        {
            "location": "/layers/normalization/",
            "text": "[source]\n\n\nBatchNormalization\n\n\nkeras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n\n\n\n\nBatch normalization layer (Ioffe and Szegedy, 2014).\n\n\nNormalize the activations of the previous layer at each batch,\ni.e. applies a transformation that maintains the mean activation\nclose to 0 and the activation standard deviation close to 1.\n\n\nArguments\n\n\n\n\naxis\n: Integer, the axis that should be normalized\n(typically the features axis).\nFor instance, after a \nConv2D\n layer with\n\ndata_format=\"channels_first\"\n,\nset \naxis=1\n in \nBatchNormalization\n.\n\n\nmomentum\n: Momentum for the moving mean and the moving variance.\n\n\nepsilon\n: Small float added to variance to avoid dividing by zero.\n\n\ncenter\n: If True, add offset of \nbeta\n to normalized tensor.\nIf False, \nbeta\n is ignored.\n\n\nscale\n: If True, multiply by \ngamma\n.\nIf False, \ngamma\n is not used.\nWhen the next layer is linear (also e.g. \nnn.relu\n),\nthis can be disabled since the scaling\nwill be done by the next layer.\n\n\nbeta_initializer\n: Initializer for the beta weight.\n\n\ngamma_initializer\n: Initializer for the gamma weight.\n\n\nmoving_mean_initializer\n: Initializer for the moving mean.\n\n\nmoving_variance_initializer\n: Initializer for the moving variance.\n\n\nbeta_regularizer\n: Optional regularizer for the beta weight.\n\n\ngamma_regularizer\n: Optional regularizer for the gamma weight.\n\n\nbeta_constraint\n: Optional constraint for the beta weight.\n\n\ngamma_constraint\n: Optional constraint for the gamma weight.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\nReferences\n\n\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "title": "Normalization Layers"
        },
        {
            "location": "/layers/normalization/#batchnormalization",
            "text": "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)  Batch normalization layer (Ioffe and Szegedy, 2014).  Normalize the activations of the previous layer at each batch,\ni.e. applies a transformation that maintains the mean activation\nclose to 0 and the activation standard deviation close to 1.  Arguments   axis : Integer, the axis that should be normalized\n(typically the features axis).\nFor instance, after a  Conv2D  layer with data_format=\"channels_first\" ,\nset  axis=1  in  BatchNormalization .  momentum : Momentum for the moving mean and the moving variance.  epsilon : Small float added to variance to avoid dividing by zero.  center : If True, add offset of  beta  to normalized tensor.\nIf False,  beta  is ignored.  scale : If True, multiply by  gamma .\nIf False,  gamma  is not used.\nWhen the next layer is linear (also e.g.  nn.relu ),\nthis can be disabled since the scaling\nwill be done by the next layer.  beta_initializer : Initializer for the beta weight.  gamma_initializer : Initializer for the gamma weight.  moving_mean_initializer : Initializer for the moving mean.  moving_variance_initializer : Initializer for the moving variance.  beta_regularizer : Optional regularizer for the beta weight.  gamma_regularizer : Optional regularizer for the gamma weight.  beta_constraint : Optional constraint for the beta weight.  gamma_constraint : Optional constraint for the gamma weight.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.  References   Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "title": "BatchNormalization"
        },
        {
            "location": "/layers/noise/",
            "text": "[source]\n\n\nGaussianNoise\n\n\nkeras.layers.GaussianNoise(stddev)\n\n\n\n\nApply additive zero-centered Gaussian noise.\n\n\nThis is useful to mitigate overfitting\n(you could see it as a form of random data augmentation).\nGaussian Noise (GS) is a natural choice as corruption process\nfor real valued inputs.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nArguments\n\n\n\n\nstddev\n: float, standard deviation of the noise distribution.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nGaussianDropout\n\n\nkeras.layers.GaussianDropout(rate)\n\n\n\n\nApply multiplicative 1-centered Gaussian noise.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nArguments\n\n\n\n\nrate\n: float, drop probability (as with \nDropout\n).\nThe multiplicative noise will have\nstandard deviation \nsqrt(rate / (1 - rate))\n.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\nReferences\n\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014\n\n\n\n\n\n\n[source]\n\n\nAlphaDropout\n\n\nkeras.layers.AlphaDropout(rate, noise_shape=None, seed=None)\n\n\n\n\nApplies Alpha Dropout to the input.\n\n\nAlpha Dropout is a \nDropout\n that keeps mean and variance of inputs\nto their original values, in order to ensure the self-normalizing property\neven after this dropout.\nAlpha Dropout fits well to Scaled Exponential Linear Units\nby randomly setting activations to the negative saturation value.\n\n\nArguments\n\n\n\n\nrate\n: float, drop probability (as with \nDropout\n).\nThe multiplicative noise will have\nstandard deviation \nsqrt(rate / (1 - rate))\n.\n\n\nseed\n: A Python integer to use as random seed.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\nReferences\n\n\n\n\nSelf-Normalizing Neural Networks",
            "title": "Noise layers"
        },
        {
            "location": "/layers/noise/#gaussiannoise",
            "text": "keras.layers.GaussianNoise(stddev)  Apply additive zero-centered Gaussian noise.  This is useful to mitigate overfitting\n(you could see it as a form of random data augmentation).\nGaussian Noise (GS) is a natural choice as corruption process\nfor real valued inputs.  As it is a regularization layer, it is only active at training time.  Arguments   stddev : float, standard deviation of the noise distribution.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]",
            "title": "GaussianNoise"
        },
        {
            "location": "/layers/noise/#gaussiandropout",
            "text": "keras.layers.GaussianDropout(rate)  Apply multiplicative 1-centered Gaussian noise.  As it is a regularization layer, it is only active at training time.  Arguments   rate : float, drop probability (as with  Dropout ).\nThe multiplicative noise will have\nstandard deviation  sqrt(rate / (1 - rate)) .   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.  References   Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014    [source]",
            "title": "GaussianDropout"
        },
        {
            "location": "/layers/noise/#alphadropout",
            "text": "keras.layers.AlphaDropout(rate, noise_shape=None, seed=None)  Applies Alpha Dropout to the input.  Alpha Dropout is a  Dropout  that keeps mean and variance of inputs\nto their original values, in order to ensure the self-normalizing property\neven after this dropout.\nAlpha Dropout fits well to Scaled Exponential Linear Units\nby randomly setting activations to the negative saturation value.  Arguments   rate : float, drop probability (as with  Dropout ).\nThe multiplicative noise will have\nstandard deviation  sqrt(rate / (1 - rate)) .  seed : A Python integer to use as random seed.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.  References   Self-Normalizing Neural Networks",
            "title": "AlphaDropout"
        },
        {
            "location": "/layers/wrappers/",
            "text": "[source]\n\n\nTimeDistributed\n\n\nkeras.layers.TimeDistributed(layer)\n\n\n\n\nThis wrapper applies a layer to every temporal slice of an input.\n\n\nThe input should be at least 3D, and the dimension of index one\nwill be considered to be the temporal dimension.\n\n\nConsider a batch of 32 samples,\nwhere each sample is a sequence of 10 vectors of 16 dimensions.\nThe batch input shape of the layer is then \n(32, 10, 16)\n,\nand the \ninput_shape\n, not including the samples dimension, is \n(10, 16)\n.\n\n\nYou can then use \nTimeDistributed\n to apply a \nDense\n layer\nto each of the 10 timesteps, independently:\n\n\n# as the first layer in a model\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n# now model.output_shape == (None, 10, 8)\n\n\n\n\nThe output will then have shape \n(32, 10, 8)\n.\n\n\nIn subsequent layers, there is no need for the \ninput_shape\n:\n\n\nmodel.add(TimeDistributed(Dense(32)))\n# now model.output_shape == (None, 10, 32)\n\n\n\n\nThe output will then have shape \n(32, 10, 32)\n.\n\n\nTimeDistributed\n can be used with arbitrary layers, not just \nDense\n,\nfor instance with a \nConv2D\n layer:\n\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv2D(64, (3, 3)),\n                          input_shape=(10, 299, 299, 3)))\n\n\n\n\nArguments\n\n\n\n\nlayer\n: a layer instance.\n\n\n\n\n\n\n[source]\n\n\nBidirectional\n\n\nkeras.layers.Bidirectional(layer, merge_mode='concat', weights=None)\n\n\n\n\nBidirectional wrapper for RNNs.\n\n\nArguments\n\n\n\n\nlayer\n: \nRecurrent\n instance.\n\n\nmerge_mode\n: Mode by which outputs of the\nforward and backward RNNs will be combined.\nOne of {'sum', 'mul', 'concat', 'ave', None}.\nIf None, the outputs will not be combined,\nthey will be returned as a list.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid \nmerge_mode\n argument.\n\n\n\n\nExamples\n\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(10, return_sequences=True),\n                        input_shape=(5, 10)))\nmodel.add(Bidirectional(LSTM(10)))\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')",
            "title": "Layer wrappers"
        },
        {
            "location": "/layers/wrappers/#timedistributed",
            "text": "keras.layers.TimeDistributed(layer)  This wrapper applies a layer to every temporal slice of an input.  The input should be at least 3D, and the dimension of index one\nwill be considered to be the temporal dimension.  Consider a batch of 32 samples,\nwhere each sample is a sequence of 10 vectors of 16 dimensions.\nThe batch input shape of the layer is then  (32, 10, 16) ,\nand the  input_shape , not including the samples dimension, is  (10, 16) .  You can then use  TimeDistributed  to apply a  Dense  layer\nto each of the 10 timesteps, independently:  # as the first layer in a model\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n# now model.output_shape == (None, 10, 8)  The output will then have shape  (32, 10, 8) .  In subsequent layers, there is no need for the  input_shape :  model.add(TimeDistributed(Dense(32)))\n# now model.output_shape == (None, 10, 32)  The output will then have shape  (32, 10, 32) .  TimeDistributed  can be used with arbitrary layers, not just  Dense ,\nfor instance with a  Conv2D  layer:  model = Sequential()\nmodel.add(TimeDistributed(Conv2D(64, (3, 3)),\n                          input_shape=(10, 299, 299, 3)))  Arguments   layer : a layer instance.    [source]",
            "title": "TimeDistributed"
        },
        {
            "location": "/layers/wrappers/#bidirectional",
            "text": "keras.layers.Bidirectional(layer, merge_mode='concat', weights=None)  Bidirectional wrapper for RNNs.  Arguments   layer :  Recurrent  instance.  merge_mode : Mode by which outputs of the\nforward and backward RNNs will be combined.\nOne of {'sum', 'mul', 'concat', 'ave', None}.\nIf None, the outputs will not be combined,\nthey will be returned as a list.   Raises   ValueError : In case of invalid  merge_mode  argument.   Examples  model = Sequential()\nmodel.add(Bidirectional(LSTM(10, return_sequences=True),\n                        input_shape=(5, 10)))\nmodel.add(Bidirectional(LSTM(10)))\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')",
            "title": "Bidirectional"
        },
        {
            "location": "/layers/writing-your-own-keras-layers/",
            "text": "Writing your own Keras layers\n\n\nFor simple, stateless custom operations, you are probably better off using \nlayers.core.Lambda\n layers. But for any custom operation that has trainable weights, you should implement your own layer.\n\n\nHere is the skeleton of a Keras layer, \nas of Keras 2.0\n (if you have an older version, please upgrade). There are only three methods you need to implement:\n\n\n\n\nbuild(input_shape)\n: this is where you will define your weights. This method must set \nself.built = True\n, which can be done by calling \nsuper([Layer], self).build()\n.\n\n\ncall(x)\n: this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to \ncall\n: the input tensor.\n\n\ncompute_output_shape(input_shape)\n: in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference.\n\n\n\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nimport numpy as np\n\nclass MyLayer(Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=(input_shape[1], self.output_dim),\n                                      initializer='uniform',\n                                      trainable=True)\n        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n\n    def call(self, x):\n        return K.dot(x, self.kernel)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)\n\n\n\n\nThe existing Keras layers provide examples of how to implement almost anything. Never hesitate to read the source code!",
            "title": "Writing your own Keras layers"
        },
        {
            "location": "/layers/writing-your-own-keras-layers/#writing-your-own-keras-layers",
            "text": "For simple, stateless custom operations, you are probably better off using  layers.core.Lambda  layers. But for any custom operation that has trainable weights, you should implement your own layer.  Here is the skeleton of a Keras layer,  as of Keras 2.0  (if you have an older version, please upgrade). There are only three methods you need to implement:   build(input_shape) : this is where you will define your weights. This method must set  self.built = True , which can be done by calling  super([Layer], self).build() .  call(x) : this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to  call : the input tensor.  compute_output_shape(input_shape) : in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference.   from keras import backend as K\nfrom keras.engine.topology import Layer\nimport numpy as np\n\nclass MyLayer(Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=(input_shape[1], self.output_dim),\n                                      initializer='uniform',\n                                      trainable=True)\n        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n\n    def call(self, x):\n        return K.dot(x, self.kernel)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)  The existing Keras layers provide examples of how to implement almost anything. Never hesitate to read the source code!",
            "title": "Writing your own Keras layers"
        },
        {
            "location": "/preprocessing/sequence/",
            "text": "[source]\n\n\nTimeseriesGenerator\n\n\nkeras.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128)\n\n\n\n\nUtility class for generating batches of temporal data.\n\n\nThis class takes in a sequence of data-points gathered at\nequal intervals, along with time series parameters such as\nstride, length of history, etc., to produce batches for\ntraining/validation.\n\n\nArguments\n\n\n\n\ndata\n: Indexable generator (such as list or Numpy array)\ncontaining consecutive data points (timesteps).\nThe data should be at 2D, and axis 0 is expected\nto be the time dimension.\n\n\ntargets\n: Targets corresponding to timesteps in \ndata\n.\nIt should have same length as \ndata\n.\n\n\nlength\n: Length of the output sequences (in number of timesteps).\n\n\nsampling_rate\n: Period between successive individual timesteps\nwithin sequences. For rate \nr\n, timesteps\n\ndata[i]\n, \ndata[i-r]\n, ... \ndata[i - length]\n\nare used for create a sample sequence.\n\n\nstride\n: Period between successive output sequences.\nFor stride \ns\n, consecutive output samples would\nbe centered around \ndata[i]\n, \ndata[i+s]\n, \ndata[i+2*s]\n, etc.\nstart_index, end_index: Data points earlier than \nstart_index\n\nor later than \nend_index\n will not be used in the output sequences.\nThis is seful to reserve part of the data for test or validation.\n\n\nshuffle\n: Whether to shuffle output samples,\nor instead draw them in chronological order.\n\n\nreverse\n: Boolean: if \ntrue\n, timesteps in each output sample will be\nin reverse chronological order.\n\n\nbatch_size\n: Number of timeseries samples in each batch\n(except maybe the last one).\n\n\n\n\nReturns\n\n\nA \nSequence\n instance.\n\n\nExamples\n\n\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nimport numpy as np\n\ndata = np.array([[i] for i in range(50)])\ntargets = np.array([[i] for i in range(50)])\n\ndata_gen = TimeseriesGenerator(data, targets,\n                               length=10, sampling_rate=2,\n                               batch_size=2)\nassert len(data_gen) == 20\n\nbatch_0 = data_gen[0]\nx, y = batch_0\nassert np.array_equal(x,\n                      np.array([[[0], [2], [4], [6], [8]],\n                                [[1], [3], [5], [7], [9]]]))\nassert np.array_equal(y,\n                      np.array([[10], [11]]))\n\n\n\n\n\n\npad_sequences\n\n\npad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)\n\n\n\n\nPads sequences to the same length.\n\n\nThis function transforms a list of\n\nnum_samples\n sequences (lists of integers)\ninto a 2D Numpy array of shape \n(num_samples, num_timesteps)\n.\n\nnum_timesteps\n is either the \nmaxlen\n argument if provided,\nor the length of the longest sequence otherwise.\n\n\nSequences that are shorter than \nnum_timesteps\n\nare padded with \nvalue\n at the end.\n\n\nSequences longer than \nnum_timesteps\n are truncated\nso that they fit the desired length.\nThe position where padding or truncation happens is determined by\nthe arguments \npadding\n and \ntruncating\n, respectively.\n\n\nPre-padding is the default.\n\n\nArguments\n\n\n\n\nsequences\n: List of lists, where each element is a sequence.\n\n\nmaxlen\n: Int, maximum length of all sequences.\n\n\ndtype\n: Type of the output sequences.\n\n\npadding\n: String, 'pre' or 'post':\npad either before or after each sequence.\n\n\ntruncating\n: String, 'pre' or 'post':\nremove values from sequences larger than\n\nmaxlen\n, either at the beginning or at the end of the sequences.\n\n\nvalue\n: Float, padding value.\n\n\n\n\nReturns\n\n\n\n\nx\n: Numpy array with shape \n(len(sequences), maxlen)\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid values for \ntruncating\n or \npadding\n,\nor in case of invalid shape for a \nsequences\n entry.\n\n\n\n\n\n\nskipgrams\n\n\nskipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None)\n\n\n\n\nGenerates skipgram word pairs.\n\n\nThis function transforms a sequence of word indexes (list of integers)\ninto tuples of words of the form:\n\n\n\n\n(word, word in the same window), with label 1 (positive samples).\n\n\n(word, random word from the vocabulary), with label 0 (negative samples).\n\n\n\n\nRead more about Skipgram in this gnomic paper by Mikolov et al.:\n\nEfficient Estimation of Word Representations in\nVector Space\n\n\nArguments\n\n\n\n\nsequence\n: A word sequence (sentence), encoded as a list\nof word indices (integers). If using a \nsampling_table\n,\nword indices are expected to match the rank\nof the words in a reference dataset (e.g. 10 would encode\nthe 10-th most frequently occurring token).\nNote that index 0 is expected to be a non-word and will be skipped.\n\n\nvocabulary_size\n: Int, maximum possible word index + 1\n\n\nwindow_size\n: Int, size of sampling windows (technically half-window).\nThe window of a word \nw_i\n will be\n\n[i - window_size, i + window_size+1]\n.\n\n\nnegative_samples\n: Float >= 0. 0 for no negative (i.e. random) samples.\n1 for same number as positive samples.\n\n\nshuffle\n: Whether to shuffle the word couples before returning them.\n\n\ncategorical\n: bool. if False, labels will be\nintegers (eg. \n[0, 1, 1 .. ]\n),\nif \nTrue\n, labels will be categorical, e.g.\n\n[[1,0],[0,1],[0,1] .. ]\n.\n\n\nsampling_table\n: 1D array of size \nvocabulary_size\n where the entry i\nencodes the probability to sample a word of rank i.\n\n\nseed\n: Random seed.\n\n\n\n\nReturns\n\n\ncouples, labels: where \ncouples\n are int pairs and\n\nlabels\n are either 0 or 1.\n\n\nNote\n\n\nBy convention, index 0 in the vocabulary is\na non-word and will be skipped.\n\n\n\n\nmake_sampling_table\n\n\nmake_sampling_table(size, sampling_factor=1e-05)\n\n\n\n\nGenerates a word rank-based probabilistic sampling table.\n\n\nUsed for generating the \nsampling_table\n argument for \nskipgrams\n.\n\nsampling_table[i]\n is the probability of sampling\nthe word i-th most common word in a dataset\n(more common words should be sampled less frequently, for balance).\n\n\nThe sampling probabilities are generated according\nto the sampling distribution used in word2vec:\n\n\np(word) = min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))\n\n\nWe assume that the word frequencies follow Zipf's law (s=1) to derive\na numerical approximation of frequency(rank):\n\n\nfrequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))\n\nwhere \ngamma\n is the Euler-Mascheroni constant.\n\n\nArguments\n\n\n\n\nsize\n: Int, number of possible words to sample.\n\n\nsampling_factor\n: The sampling factor in the word2vec formula.\n\n\n\n\nReturns\n\n\nA 1D Numpy array of length \nsize\n where the ith entry\nis the probability that a word of rank i should be sampled.",
            "title": "Sequence Preprocessing"
        },
        {
            "location": "/preprocessing/sequence/#timeseriesgenerator",
            "text": "keras.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128)  Utility class for generating batches of temporal data.  This class takes in a sequence of data-points gathered at\nequal intervals, along with time series parameters such as\nstride, length of history, etc., to produce batches for\ntraining/validation.  Arguments   data : Indexable generator (such as list or Numpy array)\ncontaining consecutive data points (timesteps).\nThe data should be at 2D, and axis 0 is expected\nto be the time dimension.  targets : Targets corresponding to timesteps in  data .\nIt should have same length as  data .  length : Length of the output sequences (in number of timesteps).  sampling_rate : Period between successive individual timesteps\nwithin sequences. For rate  r , timesteps data[i] ,  data[i-r] , ...  data[i - length] \nare used for create a sample sequence.  stride : Period between successive output sequences.\nFor stride  s , consecutive output samples would\nbe centered around  data[i] ,  data[i+s] ,  data[i+2*s] , etc.\nstart_index, end_index: Data points earlier than  start_index \nor later than  end_index  will not be used in the output sequences.\nThis is seful to reserve part of the data for test or validation.  shuffle : Whether to shuffle output samples,\nor instead draw them in chronological order.  reverse : Boolean: if  true , timesteps in each output sample will be\nin reverse chronological order.  batch_size : Number of timeseries samples in each batch\n(except maybe the last one).   Returns  A  Sequence  instance.  Examples  from keras.preprocessing.sequence import TimeseriesGenerator\nimport numpy as np\n\ndata = np.array([[i] for i in range(50)])\ntargets = np.array([[i] for i in range(50)])\n\ndata_gen = TimeseriesGenerator(data, targets,\n                               length=10, sampling_rate=2,\n                               batch_size=2)\nassert len(data_gen) == 20\n\nbatch_0 = data_gen[0]\nx, y = batch_0\nassert np.array_equal(x,\n                      np.array([[[0], [2], [4], [6], [8]],\n                                [[1], [3], [5], [7], [9]]]))\nassert np.array_equal(y,\n                      np.array([[10], [11]]))",
            "title": "TimeseriesGenerator"
        },
        {
            "location": "/preprocessing/sequence/#pad_sequences",
            "text": "pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)  Pads sequences to the same length.  This function transforms a list of num_samples  sequences (lists of integers)\ninto a 2D Numpy array of shape  (num_samples, num_timesteps) . num_timesteps  is either the  maxlen  argument if provided,\nor the length of the longest sequence otherwise.  Sequences that are shorter than  num_timesteps \nare padded with  value  at the end.  Sequences longer than  num_timesteps  are truncated\nso that they fit the desired length.\nThe position where padding or truncation happens is determined by\nthe arguments  padding  and  truncating , respectively.  Pre-padding is the default.  Arguments   sequences : List of lists, where each element is a sequence.  maxlen : Int, maximum length of all sequences.  dtype : Type of the output sequences.  padding : String, 'pre' or 'post':\npad either before or after each sequence.  truncating : String, 'pre' or 'post':\nremove values from sequences larger than maxlen , either at the beginning or at the end of the sequences.  value : Float, padding value.   Returns   x : Numpy array with shape  (len(sequences), maxlen)   Raises   ValueError : In case of invalid values for  truncating  or  padding ,\nor in case of invalid shape for a  sequences  entry.",
            "title": "pad_sequences"
        },
        {
            "location": "/preprocessing/sequence/#skipgrams",
            "text": "skipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None)  Generates skipgram word pairs.  This function transforms a sequence of word indexes (list of integers)\ninto tuples of words of the form:   (word, word in the same window), with label 1 (positive samples).  (word, random word from the vocabulary), with label 0 (negative samples).   Read more about Skipgram in this gnomic paper by Mikolov et al.: Efficient Estimation of Word Representations in\nVector Space  Arguments   sequence : A word sequence (sentence), encoded as a list\nof word indices (integers). If using a  sampling_table ,\nword indices are expected to match the rank\nof the words in a reference dataset (e.g. 10 would encode\nthe 10-th most frequently occurring token).\nNote that index 0 is expected to be a non-word and will be skipped.  vocabulary_size : Int, maximum possible word index + 1  window_size : Int, size of sampling windows (technically half-window).\nThe window of a word  w_i  will be [i - window_size, i + window_size+1] .  negative_samples : Float >= 0. 0 for no negative (i.e. random) samples.\n1 for same number as positive samples.  shuffle : Whether to shuffle the word couples before returning them.  categorical : bool. if False, labels will be\nintegers (eg.  [0, 1, 1 .. ] ),\nif  True , labels will be categorical, e.g. [[1,0],[0,1],[0,1] .. ] .  sampling_table : 1D array of size  vocabulary_size  where the entry i\nencodes the probability to sample a word of rank i.  seed : Random seed.   Returns  couples, labels: where  couples  are int pairs and labels  are either 0 or 1.  Note  By convention, index 0 in the vocabulary is\na non-word and will be skipped.",
            "title": "skipgrams"
        },
        {
            "location": "/preprocessing/sequence/#make_sampling_table",
            "text": "make_sampling_table(size, sampling_factor=1e-05)  Generates a word rank-based probabilistic sampling table.  Used for generating the  sampling_table  argument for  skipgrams . sampling_table[i]  is the probability of sampling\nthe word i-th most common word in a dataset\n(more common words should be sampled less frequently, for balance).  The sampling probabilities are generated according\nto the sampling distribution used in word2vec:  p(word) = min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))  We assume that the word frequencies follow Zipf's law (s=1) to derive\na numerical approximation of frequency(rank):  frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) \nwhere  gamma  is the Euler-Mascheroni constant.  Arguments   size : Int, number of possible words to sample.  sampling_factor : The sampling factor in the word2vec formula.   Returns  A 1D Numpy array of length  size  where the ith entry\nis the probability that a word of rank i should be sampled.",
            "title": "make_sampling_table"
        },
        {
            "location": "/preprocessing/text/",
            "text": "text_to_word_sequence\n\n\nkeras.preprocessing.text.text_to_word_sequence(text,\n                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                               lower=True,\n                                               split=\" \")\n\n\n\n\nSplit a sentence into a list of words.\n\n\n\n\n\n\nReturn\n: List of words (str).\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ntext\n: str.\n\n\nfilters\n: list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.\n\n\nlower\n: boolean. Whether to set the text to lowercase.\n\n\nsplit\n: str. Separator for word splitting.\n\n\n\n\n\n\n\n\none_hot\n\n\nkeras.preprocessing.text.one_hot(text,\n                                 n,\n                                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                 lower=True,\n                                 split=\" \")\n\n\n\n\nOne-hot encodes a text into a list of word indexes in a vocabulary of size n.\n\n\nThis is a wrapper to the \nhashing_trick\n function using \nhash\n as the hashing function.\n\n\n\n\n\n\nReturn\n: List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ntext\n: str.\n\n\nn\n: int. Size of vocabulary.\n\n\nfilters\n: list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.\n\n\nlower\n: boolean. Whether to set the text to lowercase.\n\n\nsplit\n: str. Separator for word splitting.\n\n\n\n\n\n\n\n\nhashing_trick\n\n\nkeras.preprocessing.text.hashing_trick(text, \n                                       n,\n                                       hash_function=None,\n                                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                       lower=True,\n                                       split=' ')\n\n\n\n\nConverts a text to a sequence of indices in a fixed-size hashing space\n\n\n\n\nReturn\n:\n        A list of integer word indices (unicity non-guaranteed).\n\n\nArguments\n:\n\n\ntext\n: str.\n\n\nn\n: Dimension of the hashing space.\n\n\nhash_function\n: defaults to python \nhash\n function, can be 'md5' or\n        any function that takes in input a string and returns a int.\n        Note that 'hash' is not a stable hashing function, so\n        it is not consistent across different runs, while 'md5'\n        is a stable hashing function.\n\n\nfilters\n: list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.\n\n\nlower\n: boolean. Whether to set the text to lowercase.\n\n\nsplit\n: str. Separator for word splitting.\n\n\n\n\n\n\n\n\nTokenizer\n\n\nkeras.preprocessing.text.Tokenizer(num_words=None,\n                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                   lower=True,\n                                   split=\" \",\n                                   char_level=False,\n                                   oov_token=None)\n\n\n\n\nClass for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n\n\n\n\n\n\nArguments\n: Same as \ntext_to_word_sequence\n above.\n\n\n\n\nnum_words\n: None or int. Maximum number of words to work with (if set, tokenization will be restricted to the top num_words most common words in the dataset).\n\n\nchar_level\n: if True, every character will be treated as a token.\n\n\noov_token\n: None or str. If given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls.\n\n\n\n\n\n\n\n\nMethods\n:\n\n\n\n\n\n\nfit_on_texts(texts)\n: \n\n\n\n\nArguments\n:\n\n\ntexts\n: list of texts to train on.\n\n\n\n\n\n\n\n\n\n\n\n\ntexts_to_sequences(texts)\n\n\n\n\nArguments\n: \n\n\ntexts\n: list of texts to turn to sequences.\n\n\n\n\n\n\nReturn\n: list of sequences (one per text input).\n\n\n\n\n\n\n\n\ntexts_to_sequences_generator(texts)\n: generator version of the above. \n\n\n\n\nReturn\n: yield one sequence per input text.\n\n\n\n\n\n\n\n\ntexts_to_matrix(texts)\n:\n\n\n\n\nReturn\n: numpy array of shape \n(len(texts), num_words)\n.\n\n\nArguments\n:\n\n\ntexts\n: list of texts to vectorize.\n\n\nmode\n: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").\n\n\n\n\n\n\n\n\n\n\n\n\nfit_on_sequences(sequences)\n: \n\n\n\n\nArguments\n:\n\n\nsequences\n: list of sequences to train on. \n\n\n\n\n\n\n\n\n\n\n\n\nsequences_to_matrix(sequences)\n:\n\n\n\n\nReturn\n: numpy array of shape \n(len(sequences), num_words)\n.\n\n\nArguments\n:\n\n\nsequences\n: list of sequences to vectorize.\n\n\nmode\n: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nword_counts\n: dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called. \n\n\nword_docs\n: dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n\n\nword_index\n: dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n\n\ndocument_count\n: int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.",
            "title": "Text Preprocessing"
        },
        {
            "location": "/preprocessing/text/#text_to_word_sequence",
            "text": "keras.preprocessing.text.text_to_word_sequence(text,\n                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                               lower=True,\n                                               split=\" \")  Split a sentence into a list of words.    Return : List of words (str).    Arguments :   text : str.  filters : list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.  lower : boolean. Whether to set the text to lowercase.  split : str. Separator for word splitting.",
            "title": "text_to_word_sequence"
        },
        {
            "location": "/preprocessing/text/#one_hot",
            "text": "keras.preprocessing.text.one_hot(text,\n                                 n,\n                                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                 lower=True,\n                                 split=\" \")  One-hot encodes a text into a list of word indexes in a vocabulary of size n.  This is a wrapper to the  hashing_trick  function using  hash  as the hashing function.    Return : List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).    Arguments :   text : str.  n : int. Size of vocabulary.  filters : list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.  lower : boolean. Whether to set the text to lowercase.  split : str. Separator for word splitting.",
            "title": "one_hot"
        },
        {
            "location": "/preprocessing/text/#hashing_trick",
            "text": "keras.preprocessing.text.hashing_trick(text, \n                                       n,\n                                       hash_function=None,\n                                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                       lower=True,\n                                       split=' ')  Converts a text to a sequence of indices in a fixed-size hashing space   Return :\n        A list of integer word indices (unicity non-guaranteed).  Arguments :  text : str.  n : Dimension of the hashing space.  hash_function : defaults to python  hash  function, can be 'md5' or\n        any function that takes in input a string and returns a int.\n        Note that 'hash' is not a stable hashing function, so\n        it is not consistent across different runs, while 'md5'\n        is a stable hashing function.  filters : list (or concatenation) of characters to filter out, such as\n     punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes\n     basic punctuation, tabs, and newlines.  lower : boolean. Whether to set the text to lowercase.  split : str. Separator for word splitting.",
            "title": "hashing_trick"
        },
        {
            "location": "/preprocessing/text/#tokenizer",
            "text": "keras.preprocessing.text.Tokenizer(num_words=None,\n                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                   lower=True,\n                                   split=\" \",\n                                   char_level=False,\n                                   oov_token=None)  Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).    Arguments : Same as  text_to_word_sequence  above.   num_words : None or int. Maximum number of words to work with (if set, tokenization will be restricted to the top num_words most common words in the dataset).  char_level : if True, every character will be treated as a token.  oov_token : None or str. If given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls.     Methods :    fit_on_texts(texts) :    Arguments :  texts : list of texts to train on.       texts_to_sequences(texts)   Arguments :   texts : list of texts to turn to sequences.    Return : list of sequences (one per text input).     texts_to_sequences_generator(texts) : generator version of the above.    Return : yield one sequence per input text.     texts_to_matrix(texts) :   Return : numpy array of shape  (len(texts), num_words) .  Arguments :  texts : list of texts to vectorize.  mode : one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").       fit_on_sequences(sequences) :    Arguments :  sequences : list of sequences to train on.        sequences_to_matrix(sequences) :   Return : numpy array of shape  (len(sequences), num_words) .  Arguments :  sequences : list of sequences to vectorize.  mode : one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").         Attributes :   word_counts : dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.   word_docs : dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.  word_index : dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.  document_count : int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.",
            "title": "Tokenizer"
        },
        {
            "location": "/preprocessing/image/",
            "text": "ImageDataGenerator\n\n\nkeras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    zca_epsilon=1e-6,\n    rotation_range=0.,\n    width_shift_range=0.,\n    height_shift_range=0.,\n    shear_range=0.,\n    zoom_range=0.,\n    channel_shift_range=0.,\n    fill_mode='nearest',\n    cval=0.,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    preprocessing_function=None,\n    data_format=K.image_data_format())\n\n\n\n\nGenerate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nfeaturewise_center\n: Boolean. Set input mean to 0 over the dataset, feature-wise.\n\n\nsamplewise_center\n: Boolean. Set each sample mean to 0.\n\n\nfeaturewise_std_normalization\n: Boolean. Divide inputs by std of the dataset, feature-wise.\n\n\nsamplewise_std_normalization\n: Boolean. Divide each input by its std.\n\n\nzca_epsilon\n: epsilon for ZCA whitening. Default is 1e-6.\n\n\nzca_whitening\n: Boolean. Apply ZCA whitening.\n\n\nrotation_range\n: Int. Degree range for random rotations.\n\n\nwidth_shift_range\n: Float (fraction of total width). Range for random horizontal shifts.\n\n\nheight_shift_range\n: Float (fraction of total height). Range for random vertical shifts.\n\n\nshear_range\n: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n\n\nzoom_range\n: Float or [lower, upper]. Range for random zoom. If a float, \n[lower, upper] = [1-zoom_range, 1+zoom_range]\n.\n\n\nchannel_shift_range\n: Float. Range for random channel shifts.\n\n\nfill_mode\n: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.  Points outside the boundaries of the input are filled according to the given mode:\n\n\n\"constant\": \nkkkkkkkk|abcd|kkkkkkkk\n (\ncval=k\n)\n\n\n\"nearest\":  \naaaaaaaa|abcd|dddddddd\n\n\n\"reflect\":  \nabcddcba|abcd|dcbaabcd\n\n\n\"wrap\":     \nabcdabcd|abcd|abcdabcd\n\n\n\n\n\n\ncval\n: Float or Int. Value used for points outside the boundaries when \nfill_mode = \"constant\"\n.\n\n\nhorizontal_flip\n: Boolean. Randomly flip inputs horizontally.\n\n\nvertical_flip\n: Boolean. Randomly flip inputs vertically.\n\n\nrescale\n: rescaling factor. Defaults to None. If None or 0, no rescaling is applied,\n        otherwise we multiply the data by the value provided (before applying\n        any other transformation).\n\n\npreprocessing_function\n: function that will be implied on each input.\n        The function will run before any other modification on it.\n        The function should take one argument:\n        one image (Numpy tensor with rank 3),\n        and should output a Numpy tensor with the same shape.\n\n\ndata_format\n: One of {\"channels_first\", \"channels_last\"}.\n    \"channels_last\" mode means that the images should have shape \n(samples, height, width, channels)\n,\n    \"channels_first\" mode means that the images should have shape \n(samples, channels, height, width)\n.\n    It defaults to the \nimage_data_format\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"channels_last\".\n\n\n\n\n\n\n\n\nMethods\n:\n\n\n\n\nfit(x)\n: Compute the internal data stats related to the data-dependent transformations, based on an array of sample data.\n    Only required if featurewise_center or featurewise_std_normalization or zca_whitening.\n\n\nArguments\n:\n\n\nx\n: sample data. Should have rank 4.\n    In case of grayscale data,\n    the channels axis should have value 1, and in case\n    of RGB data, it should have value 3.\n\n\naugment\n: Boolean (default: False). Whether to fit on randomly augmented samples.\n\n\nrounds\n: int (default: 1). If augment, how many augmentation passes over the data to use.\n\n\nseed\n: int (default: None). Random seed.\n\n\n\n\n\n\n\n\n\n\nflow(x, y)\n: Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n\n\nArguments\n:\n\n\nx\n: data. Should have rank 4.\n    In case of grayscale data,\n    the channels axis should have value 1, and in case\n    of RGB data, it should have value 3.\n\n\ny\n: labels.\n\n\nbatch_size\n: int (default: 32).\n\n\nshuffle\n: boolean (default: True).\n\n\nseed\n: int (default: None).\n\n\nsave_to_dir\n: None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).\n\n\nsave_prefix\n: str (default: \n''\n). Prefix to use for filenames of saved pictures (only relevant if \nsave_to_dir\n is set).\n\n\nsave_format\n: one of \"png\", \"jpeg\" (only relevant if \nsave_to_dir\n is set). Default: \"png\".\n\n\n\n\n\n\nyields\n: Tuples of \n(x, y)\n where \nx\n is a numpy array of image data and \ny\n is a numpy array of corresponding labels.\n    The generator loops indefinitely.\n\n\n\n\n\n\nflow_from_directory(directory)\n: Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n\n\nArguments\n:\n\n\ndirectory\n: path to the target directory. It should contain one subdirectory per class.\n    Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator.\n    See \nthis script\n for more details.\n\n\ntarget_size\n: tuple of integers \n(height, width)\n, default: \n(256, 256)\n. \n    The dimensions to which all images found will be resized.\n\n\ncolor_mode\n: one of \"grayscale\", \"rbg\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels.\n\n\nclasses\n: optional list of class subdirectories (e.g. \n['dogs', 'cats']\n). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under \ndirectory\n, where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute \nclass_indices\n.\n\n\nclass_mode\n: one of \"categorical\", \"binary\", \"sparse\", \"input\" or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders). If None, no labels are returned (the generator will only yield batches of image data, which is useful to use \nmodel.predict_generator()\n, \nmodel.evaluate_generator()\n, etc.). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of \ndirectory\n for it to work correctly.\n\n\nbatch_size\n: size of the batches of data (default: 32).\n\n\nshuffle\n: whether to shuffle the data (default: True)\n\n\nseed\n: optional random seed for shuffling and transformations.\n\n\nsave_to_dir\n: None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).\n\n\nsave_prefix\n: str. Prefix to use for filenames of saved pictures (only relevant if \nsave_to_dir\n is set).\n\n\nsave_format\n: one of \"png\", \"jpeg\" (only relevant if \nsave_to_dir\n is set). Default: \"png\".\n\n\nfollow_links\n: whether to follow symlinks inside class subdirectories (default: False).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n:\n\n\n\n\n\n\nExample of using \n.flow(x, y)\n:\n\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\ny_train = np_utils.to_categorical(y_train, num_classes)\ny_test = np_utils.to_categorical(y_test, num_classes)\n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\ndatagen.fit(x_train)\n\n# fits the model on batches with real-time data augmentation:\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n\n# here's a more \"manual\" example\nfor e in range(epochs):\n    print('Epoch', e)\n    batches = 0\n    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n        model.fit(x_batch, y_batch)\n        batches += 1\n        if batches >= len(x_train) / 32:\n            # we need to break the loop by hand because\n            # the generator loops indefinitely\n            break\n\n\n\n\nExample of using \n.flow_from_directory(directory)\n:\n\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(\n        train_generator,\n        steps_per_epoch=2000,\n        epochs=50,\n        validation_data=validation_generator,\n        validation_steps=800)\n\n\n\n\nExample of transforming images and masks together.\n\n\n# we create two instances with the same arguments\ndata_gen_args = dict(featurewise_center=True,\n                     featurewise_std_normalization=True,\n                     rotation_range=90.,\n                     width_shift_range=0.1,\n                     height_shift_range=0.1,\n                     zoom_range=0.2)\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen = ImageDataGenerator(**data_gen_args)\n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\nimage_datagen.fit(images, augment=True, seed=seed)\nmask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    'data/images',\n    class_mode=None,\n    seed=seed)\n\nmask_generator = mask_datagen.flow_from_directory(\n    'data/masks',\n    class_mode=None,\n    seed=seed)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=2000,\n    epochs=50)",
            "title": "Image Preprocessing"
        },
        {
            "location": "/preprocessing/image/#imagedatagenerator",
            "text": "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    zca_epsilon=1e-6,\n    rotation_range=0.,\n    width_shift_range=0.,\n    height_shift_range=0.,\n    shear_range=0.,\n    zoom_range=0.,\n    channel_shift_range=0.,\n    fill_mode='nearest',\n    cval=0.,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    preprocessing_function=None,\n    data_format=K.image_data_format())  Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.    Arguments :   featurewise_center : Boolean. Set input mean to 0 over the dataset, feature-wise.  samplewise_center : Boolean. Set each sample mean to 0.  featurewise_std_normalization : Boolean. Divide inputs by std of the dataset, feature-wise.  samplewise_std_normalization : Boolean. Divide each input by its std.  zca_epsilon : epsilon for ZCA whitening. Default is 1e-6.  zca_whitening : Boolean. Apply ZCA whitening.  rotation_range : Int. Degree range for random rotations.  width_shift_range : Float (fraction of total width). Range for random horizontal shifts.  height_shift_range : Float (fraction of total height). Range for random vertical shifts.  shear_range : Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)  zoom_range : Float or [lower, upper]. Range for random zoom. If a float,  [lower, upper] = [1-zoom_range, 1+zoom_range] .  channel_shift_range : Float. Range for random channel shifts.  fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.  Points outside the boundaries of the input are filled according to the given mode:  \"constant\":  kkkkkkkk|abcd|kkkkkkkk  ( cval=k )  \"nearest\":   aaaaaaaa|abcd|dddddddd  \"reflect\":   abcddcba|abcd|dcbaabcd  \"wrap\":      abcdabcd|abcd|abcdabcd    cval : Float or Int. Value used for points outside the boundaries when  fill_mode = \"constant\" .  horizontal_flip : Boolean. Randomly flip inputs horizontally.  vertical_flip : Boolean. Randomly flip inputs vertically.  rescale : rescaling factor. Defaults to None. If None or 0, no rescaling is applied,\n        otherwise we multiply the data by the value provided (before applying\n        any other transformation).  preprocessing_function : function that will be implied on each input.\n        The function will run before any other modification on it.\n        The function should take one argument:\n        one image (Numpy tensor with rank 3),\n        and should output a Numpy tensor with the same shape.  data_format : One of {\"channels_first\", \"channels_last\"}.\n    \"channels_last\" mode means that the images should have shape  (samples, height, width, channels) ,\n    \"channels_first\" mode means that the images should have shape  (samples, channels, height, width) .\n    It defaults to the  image_data_format  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"channels_last\".     Methods :   fit(x) : Compute the internal data stats related to the data-dependent transformations, based on an array of sample data.\n    Only required if featurewise_center or featurewise_std_normalization or zca_whitening.  Arguments :  x : sample data. Should have rank 4.\n    In case of grayscale data,\n    the channels axis should have value 1, and in case\n    of RGB data, it should have value 3.  augment : Boolean (default: False). Whether to fit on randomly augmented samples.  rounds : int (default: 1). If augment, how many augmentation passes over the data to use.  seed : int (default: None). Random seed.      flow(x, y) : Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.  Arguments :  x : data. Should have rank 4.\n    In case of grayscale data,\n    the channels axis should have value 1, and in case\n    of RGB data, it should have value 3.  y : labels.  batch_size : int (default: 32).  shuffle : boolean (default: True).  seed : int (default: None).  save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).  save_prefix : str (default:  '' ). Prefix to use for filenames of saved pictures (only relevant if  save_to_dir  is set).  save_format : one of \"png\", \"jpeg\" (only relevant if  save_to_dir  is set). Default: \"png\".    yields : Tuples of  (x, y)  where  x  is a numpy array of image data and  y  is a numpy array of corresponding labels.\n    The generator loops indefinitely.    flow_from_directory(directory) : Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.  Arguments :  directory : path to the target directory. It should contain one subdirectory per class.\n    Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator.\n    See  this script  for more details.  target_size : tuple of integers  (height, width) , default:  (256, 256) . \n    The dimensions to which all images found will be resized.  color_mode : one of \"grayscale\", \"rbg\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels.  classes : optional list of class subdirectories (e.g.  ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under  directory , where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute  class_indices .  class_mode : one of \"categorical\", \"binary\", \"sparse\", \"input\" or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders). If None, no labels are returned (the generator will only yield batches of image data, which is useful to use  model.predict_generator() ,  model.evaluate_generator() , etc.). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of  directory  for it to work correctly.  batch_size : size of the batches of data (default: 32).  shuffle : whether to shuffle the data (default: True)  seed : optional random seed for shuffling and transformations.  save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).  save_prefix : str. Prefix to use for filenames of saved pictures (only relevant if  save_to_dir  is set).  save_format : one of \"png\", \"jpeg\" (only relevant if  save_to_dir  is set). Default: \"png\".  follow_links : whether to follow symlinks inside class subdirectories (default: False).         Examples :    Example of using  .flow(x, y) :  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\ny_train = np_utils.to_categorical(y_train, num_classes)\ny_test = np_utils.to_categorical(y_test, num_classes)\n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\ndatagen.fit(x_train)\n\n# fits the model on batches with real-time data augmentation:\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n\n# here's a more \"manual\" example\nfor e in range(epochs):\n    print('Epoch', e)\n    batches = 0\n    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n        model.fit(x_batch, y_batch)\n        batches += 1\n        if batches >= len(x_train) / 32:\n            # we need to break the loop by hand because\n            # the generator loops indefinitely\n            break  Example of using  .flow_from_directory(directory) :  train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(\n        train_generator,\n        steps_per_epoch=2000,\n        epochs=50,\n        validation_data=validation_generator,\n        validation_steps=800)  Example of transforming images and masks together.  # we create two instances with the same arguments\ndata_gen_args = dict(featurewise_center=True,\n                     featurewise_std_normalization=True,\n                     rotation_range=90.,\n                     width_shift_range=0.1,\n                     height_shift_range=0.1,\n                     zoom_range=0.2)\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen = ImageDataGenerator(**data_gen_args)\n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\nimage_datagen.fit(images, augment=True, seed=seed)\nmask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    'data/images',\n    class_mode=None,\n    seed=seed)\n\nmask_generator = mask_datagen.flow_from_directory(\n    'data/masks',\n    class_mode=None,\n    seed=seed)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=2000,\n    epochs=50)",
            "title": "ImageDataGenerator"
        },
        {
            "location": "/losses/",
            "text": "Usage of loss functions\n\n\nA loss function (or objective function, or optimization score function) is one of the two parameters required to compile a model:\n\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n\n\n\nfrom keras import losses\n\nmodel.compile(loss=losses.mean_squared_error, optimizer='sgd')\n\n\n\n\nYou can either pass the name of an existing loss function, or pass a TensorFlow/Theano symbolic function that returns a scalar for each data-point and takes the following two arguments:\n\n\n\n\ny_true\n: True labels. TensorFlow/Theano tensor.\n\n\ny_pred\n: Predictions. TensorFlow/Theano tensor of the same shape as y_true.\n\n\n\n\nThe actual optimized objective is the mean of the output array across all datapoints.\n\n\nFor a few examples of such functions, check out the \nlosses source\n.\n\n\nAvailable loss functions\n\n\nmean_squared_error\n\n\nmean_squared_error(y_true, y_pred)\n\n\n\n\n\n\nmean_absolute_error\n\n\nmean_absolute_error(y_true, y_pred)\n\n\n\n\n\n\nmean_absolute_percentage_error\n\n\nmean_absolute_percentage_error(y_true, y_pred)\n\n\n\n\n\n\nmean_squared_logarithmic_error\n\n\nmean_squared_logarithmic_error(y_true, y_pred)\n\n\n\n\n\n\nsquared_hinge\n\n\nsquared_hinge(y_true, y_pred)\n\n\n\n\n\n\nhinge\n\n\nhinge(y_true, y_pred)\n\n\n\n\n\n\ncategorical_hinge\n\n\ncategorical_hinge(y_true, y_pred)\n\n\n\n\n\n\nlogcosh\n\n\nlogcosh(y_true, y_pred)\n\n\n\n\nLogarithm of the hyperbolic cosine of the prediction error.\n\n\nlog(cosh(x))\n is approximately equal to \n(x ** 2) / 2\n for small \nx\n and\nto \nabs(x) - log(2)\n for large \nx\n. This means that 'logcosh' works mostly\nlike the mean squared error, but will not be so strongly affected by the\noccasional wildly incorrect prediction.\n\n\nArguments\n\n\n\n\ny_true\n: tensor of true targets.\n\n\ny_pred\n: tensor of predicted targets.\n\n\n\n\nReturns\n\n\nTensor with one scalar loss entry per sample.\n\n\n\n\ncategorical_crossentropy\n\n\ncategorical_crossentropy(y_true, y_pred)\n\n\n\n\n\n\nsparse_categorical_crossentropy\n\n\nsparse_categorical_crossentropy(y_true, y_pred)\n\n\n\n\n\n\nbinary_crossentropy\n\n\nbinary_crossentropy(y_true, y_pred)\n\n\n\n\n\n\nkullback_leibler_divergence\n\n\nkullback_leibler_divergence(y_true, y_pred)\n\n\n\n\n\n\npoisson\n\n\npoisson(y_true, y_pred)\n\n\n\n\n\n\ncosine_proximity\n\n\ncosine_proximity(y_true, y_pred)\n\n\n\n\n\n\nNote\n: when using the \ncategorical_crossentropy\n loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). In order to convert \ninteger targets\n into \ncategorical targets\n, you can use the Keras utility \nto_categorical\n:\n\n\nfrom keras.utils.np_utils import to_categorical\n\ncategorical_labels = to_categorical(int_labels, num_classes=None)",
            "title": "Losses"
        },
        {
            "location": "/losses/#usage-of-loss-functions",
            "text": "A loss function (or objective function, or optimization score function) is one of the two parameters required to compile a model:  model.compile(loss='mean_squared_error', optimizer='sgd')  from keras import losses\n\nmodel.compile(loss=losses.mean_squared_error, optimizer='sgd')  You can either pass the name of an existing loss function, or pass a TensorFlow/Theano symbolic function that returns a scalar for each data-point and takes the following two arguments:   y_true : True labels. TensorFlow/Theano tensor.  y_pred : Predictions. TensorFlow/Theano tensor of the same shape as y_true.   The actual optimized objective is the mean of the output array across all datapoints.  For a few examples of such functions, check out the  losses source .",
            "title": "Usage of loss functions"
        },
        {
            "location": "/losses/#available-loss-functions",
            "text": "",
            "title": "Available loss functions"
        },
        {
            "location": "/losses/#mean_squared_error",
            "text": "mean_squared_error(y_true, y_pred)",
            "title": "mean_squared_error"
        },
        {
            "location": "/losses/#mean_absolute_error",
            "text": "mean_absolute_error(y_true, y_pred)",
            "title": "mean_absolute_error"
        },
        {
            "location": "/losses/#mean_absolute_percentage_error",
            "text": "mean_absolute_percentage_error(y_true, y_pred)",
            "title": "mean_absolute_percentage_error"
        },
        {
            "location": "/losses/#mean_squared_logarithmic_error",
            "text": "mean_squared_logarithmic_error(y_true, y_pred)",
            "title": "mean_squared_logarithmic_error"
        },
        {
            "location": "/losses/#squared_hinge",
            "text": "squared_hinge(y_true, y_pred)",
            "title": "squared_hinge"
        },
        {
            "location": "/losses/#hinge",
            "text": "hinge(y_true, y_pred)",
            "title": "hinge"
        },
        {
            "location": "/losses/#categorical_hinge",
            "text": "categorical_hinge(y_true, y_pred)",
            "title": "categorical_hinge"
        },
        {
            "location": "/losses/#logcosh",
            "text": "logcosh(y_true, y_pred)  Logarithm of the hyperbolic cosine of the prediction error.  log(cosh(x))  is approximately equal to  (x ** 2) / 2  for small  x  and\nto  abs(x) - log(2)  for large  x . This means that 'logcosh' works mostly\nlike the mean squared error, but will not be so strongly affected by the\noccasional wildly incorrect prediction.  Arguments   y_true : tensor of true targets.  y_pred : tensor of predicted targets.   Returns  Tensor with one scalar loss entry per sample.",
            "title": "logcosh"
        },
        {
            "location": "/losses/#categorical_crossentropy",
            "text": "categorical_crossentropy(y_true, y_pred)",
            "title": "categorical_crossentropy"
        },
        {
            "location": "/losses/#sparse_categorical_crossentropy",
            "text": "sparse_categorical_crossentropy(y_true, y_pred)",
            "title": "sparse_categorical_crossentropy"
        },
        {
            "location": "/losses/#binary_crossentropy",
            "text": "binary_crossentropy(y_true, y_pred)",
            "title": "binary_crossentropy"
        },
        {
            "location": "/losses/#kullback_leibler_divergence",
            "text": "kullback_leibler_divergence(y_true, y_pred)",
            "title": "kullback_leibler_divergence"
        },
        {
            "location": "/losses/#poisson",
            "text": "poisson(y_true, y_pred)",
            "title": "poisson"
        },
        {
            "location": "/losses/#cosine_proximity",
            "text": "cosine_proximity(y_true, y_pred)   Note : when using the  categorical_crossentropy  loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). In order to convert  integer targets  into  categorical targets , you can use the Keras utility  to_categorical :  from keras.utils.np_utils import to_categorical\n\ncategorical_labels = to_categorical(int_labels, num_classes=None)",
            "title": "cosine_proximity"
        },
        {
            "location": "/metrics/",
            "text": "Usage of metrics\n\n\nA metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the \nmetrics\n parameter when a model is compiled.\n\n\nmodel.compile(loss='mean_squared_error',\n              optimizer='sgd',\n              metrics=['mae', 'acc'])\n\n\n\n\nfrom keras import metrics\n\nmodel.compile(loss='mean_squared_error',\n              optimizer='sgd',\n              metrics=[metrics.mae, metrics.categorical_accuracy])\n\n\n\n\nA metric function is similar to a \nloss function\n, except that the results from evaluating a metric are not used when training the model.\n\n\nYou can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see \nCustom metrics\n).\n\n\nArguments\n\n\n\n\ny_true\n: True labels. Theano/TensorFlow tensor.\n\n\ny_pred\n: Predictions. Theano/TensorFlow tensor of the same shape as y_true.\n\n\n\n\nReturns\n\n\nSingle tensor value representing the mean of the output array across all\n  datapoints.\n\n\n\n\nAvailable metrics\n\n\nbinary_accuracy\n\n\nbinary_accuracy(y_true, y_pred)\n\n\n\n\n\n\ncategorical_accuracy\n\n\ncategorical_accuracy(y_true, y_pred)\n\n\n\n\n\n\nsparse_categorical_accuracy\n\n\nsparse_categorical_accuracy(y_true, y_pred)\n\n\n\n\n\n\ntop_k_categorical_accuracy\n\n\ntop_k_categorical_accuracy(y_true, y_pred, k=5)\n\n\n\n\n\n\nsparse_top_k_categorical_accuracy\n\n\nsparse_top_k_categorical_accuracy(y_true, y_pred, k=5)\n\n\n\n\n\n\nCustom metrics\n\n\nCustom metrics can be passed at the compilation step. The\nfunction would need to take \n(y_true, y_pred)\n as arguments and return\na single tensor value.\n\n\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred])",
            "title": "Metrics"
        },
        {
            "location": "/metrics/#usage-of-metrics",
            "text": "A metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the  metrics  parameter when a model is compiled.  model.compile(loss='mean_squared_error',\n              optimizer='sgd',\n              metrics=['mae', 'acc'])  from keras import metrics\n\nmodel.compile(loss='mean_squared_error',\n              optimizer='sgd',\n              metrics=[metrics.mae, metrics.categorical_accuracy])  A metric function is similar to a  loss function , except that the results from evaluating a metric are not used when training the model.  You can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see  Custom metrics ).",
            "title": "Usage of metrics"
        },
        {
            "location": "/metrics/#arguments",
            "text": "y_true : True labels. Theano/TensorFlow tensor.  y_pred : Predictions. Theano/TensorFlow tensor of the same shape as y_true.",
            "title": "Arguments"
        },
        {
            "location": "/metrics/#returns",
            "text": "Single tensor value representing the mean of the output array across all\n  datapoints.",
            "title": "Returns"
        },
        {
            "location": "/metrics/#available-metrics",
            "text": "",
            "title": "Available metrics"
        },
        {
            "location": "/metrics/#binary_accuracy",
            "text": "binary_accuracy(y_true, y_pred)",
            "title": "binary_accuracy"
        },
        {
            "location": "/metrics/#categorical_accuracy",
            "text": "categorical_accuracy(y_true, y_pred)",
            "title": "categorical_accuracy"
        },
        {
            "location": "/metrics/#sparse_categorical_accuracy",
            "text": "sparse_categorical_accuracy(y_true, y_pred)",
            "title": "sparse_categorical_accuracy"
        },
        {
            "location": "/metrics/#top_k_categorical_accuracy",
            "text": "top_k_categorical_accuracy(y_true, y_pred, k=5)",
            "title": "top_k_categorical_accuracy"
        },
        {
            "location": "/metrics/#sparse_top_k_categorical_accuracy",
            "text": "sparse_top_k_categorical_accuracy(y_true, y_pred, k=5)",
            "title": "sparse_top_k_categorical_accuracy"
        },
        {
            "location": "/metrics/#custom-metrics",
            "text": "Custom metrics can be passed at the compilation step. The\nfunction would need to take  (y_true, y_pred)  as arguments and return\na single tensor value.  import keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred])",
            "title": "Custom metrics"
        },
        {
            "location": "/optimizers/",
            "text": "Usage of optimizers\n\n\nAn optimizer is one of the two arguments required for compiling a Keras model:\n\n\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\nmodel.add(Activation('tanh'))\nmodel.add(Activation('softmax'))\n\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\n\n\n\n\nYou can either instantiate an optimizer before passing it to \nmodel.compile()\n , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used.\n\n\n# pass optimizer by name: default parameters will be used\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n\n\n\n\n\nParameters common to all Keras optimizers\n\n\nThe parameters \nclipnorm\n and \nclipvalue\n can be used with all optimizers to control gradient clipping:\n\n\nfrom keras import optimizers\n\n# All parameter gradients will be clipped to\n# a maximum norm of 1.\nsgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n\n\n\n\nfrom keras import optimizers\n\n# All parameter gradients will be clipped to\n# a maximum value of 0.5 and\n# a minimum value of -0.5.\nsgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\n\n\n\n\n\n\n[source]\n\n\nSGD\n\n\nkeras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n\n\n\n\nStochastic gradient descent optimizer.\n\n\nIncludes support for momentum,\nlearning rate decay, and Nesterov momentum.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nmomentum\n: float >= 0. Parameter that accelerates SGD\nin the relevant direction and dampens oscillations.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\nnesterov\n: boolean. Whether to apply Nesterov momentum.\n\n\n\n\n\n\n[source]\n\n\nRMSprop\n\n\nkeras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\n\n\n\nRMSProp optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).\n\n\nThis optimizer is usually a good choice for recurrent\nneural networks.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nrho\n: float >= 0.\n\n\nepsilon\n: float >= 0. Fuzz factor. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nrmsprop: Divide the gradient by a running average of its recent magnitude\n\n\n\n\n\n\n[source]\n\n\nAdagrad\n\n\nkeras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n\n\n\n\nAdagrad optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nepsilon\n: float >= 0. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdaptive Subgradient Methods for Online Learning and Stochastic Optimization\n\n\n\n\n\n\n[source]\n\n\nAdadelta\n\n\nkeras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n\n\n\n\nAdadelta optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\nIt is recommended to leave it at the default value.\n\n\nrho\n: float >= 0.\n\n\nepsilon\n: float >= 0. Fuzz factor. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdadelta - an adaptive learning rate method\n\n\n\n\n\n\n[source]\n\n\nAdam\n\n\nkeras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\n\n\n\nAdam optimizer.\n\n\nDefault parameters follow those provided in the original paper.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nbeta_1\n: float, 0 < beta < 1. Generally close to 1.\n\n\nbeta_2\n: float, 0 < beta < 1. Generally close to 1.\n\n\nepsilon\n: float >= 0. Fuzz factor. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\namsgrad\n: boolean. Whether to apply the AMSGrad variant of this\nalgorithm from the paper \"On the Convergence of Adam and\nBeyond\".\n\n\n\n\nReferences\n\n\n\n\nAdam - A Method for Stochastic Optimization\n\n\nOn the Convergence of Adam and Beyond\n\n\n\n\n\n\n[source]\n\n\nAdamax\n\n\nkeras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n\n\n\n\nAdamax optimizer from Adam paper's Section 7.\n\n\nIt is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nbeta_1/beta_2\n: floats, 0 < beta < 1. Generally close to 1.\n\n\nepsilon\n: float >= 0. Fuzz factor. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\ndecay\n: float >= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdam - A Method for Stochastic Optimization\n\n\n\n\n\n\n[source]\n\n\nNadam\n\n\nkeras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n\n\n\n\nNesterov Adam optimizer.\n\n\nMuch like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.\n\n\nDefault parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float >= 0. Learning rate.\n\n\nbeta_1/beta_2\n: floats, 0 < beta < 1. Generally close to 1.\n\n\nepsilon\n: float >= 0. Fuzz factor. If \nNone\n, defaults to \nK.epsilon()\n.\n\n\n\n\nReferences\n\n\n\n\nNadam report\n\n\nOn the importance of initialization and momentum in deep learning\n\n\n\n\n\n\n[source]\n\n\nTFOptimizer\n\n\nkeras.optimizers.TFOptimizer(optimizer)\n\n\n\n\nWrapper class for native TensorFlow optimizers.",
            "title": "Optimizers"
        },
        {
            "location": "/optimizers/#usage-of-optimizers",
            "text": "An optimizer is one of the two arguments required for compiling a Keras model:  from keras import optimizers\n\nmodel = Sequential()\nmodel.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\nmodel.add(Activation('tanh'))\nmodel.add(Activation('softmax'))\n\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)  You can either instantiate an optimizer before passing it to  model.compile()  , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used.  # pass optimizer by name: default parameters will be used\nmodel.compile(loss='mean_squared_error', optimizer='sgd')",
            "title": "Usage of optimizers"
        },
        {
            "location": "/optimizers/#parameters-common-to-all-keras-optimizers",
            "text": "The parameters  clipnorm  and  clipvalue  can be used with all optimizers to control gradient clipping:  from keras import optimizers\n\n# All parameter gradients will be clipped to\n# a maximum norm of 1.\nsgd = optimizers.SGD(lr=0.01, clipnorm=1.)  from keras import optimizers\n\n# All parameter gradients will be clipped to\n# a maximum value of 0.5 and\n# a minimum value of -0.5.\nsgd = optimizers.SGD(lr=0.01, clipvalue=0.5)   [source]",
            "title": "Parameters common to all Keras optimizers"
        },
        {
            "location": "/optimizers/#sgd",
            "text": "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)  Stochastic gradient descent optimizer.  Includes support for momentum,\nlearning rate decay, and Nesterov momentum.  Arguments   lr : float >= 0. Learning rate.  momentum : float >= 0. Parameter that accelerates SGD\nin the relevant direction and dampens oscillations.  decay : float >= 0. Learning rate decay over each update.  nesterov : boolean. Whether to apply Nesterov momentum.    [source]",
            "title": "SGD"
        },
        {
            "location": "/optimizers/#rmsprop",
            "text": "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)  RMSProp optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).  This optimizer is usually a good choice for recurrent\nneural networks.  Arguments   lr : float >= 0. Learning rate.  rho : float >= 0.  epsilon : float >= 0. Fuzz factor. If  None , defaults to  K.epsilon() .  decay : float >= 0. Learning rate decay over each update.   References   rmsprop: Divide the gradient by a running average of its recent magnitude    [source]",
            "title": "RMSprop"
        },
        {
            "location": "/optimizers/#adagrad",
            "text": "keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)  Adagrad optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float >= 0. Learning rate.  epsilon : float >= 0. If  None , defaults to  K.epsilon() .  decay : float >= 0. Learning rate decay over each update.   References   Adaptive Subgradient Methods for Online Learning and Stochastic Optimization    [source]",
            "title": "Adagrad"
        },
        {
            "location": "/optimizers/#adadelta",
            "text": "keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)  Adadelta optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float >= 0. Learning rate.\nIt is recommended to leave it at the default value.  rho : float >= 0.  epsilon : float >= 0. Fuzz factor. If  None , defaults to  K.epsilon() .  decay : float >= 0. Learning rate decay over each update.   References   Adadelta - an adaptive learning rate method    [source]",
            "title": "Adadelta"
        },
        {
            "location": "/optimizers/#adam",
            "text": "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)  Adam optimizer.  Default parameters follow those provided in the original paper.  Arguments   lr : float >= 0. Learning rate.  beta_1 : float, 0 < beta < 1. Generally close to 1.  beta_2 : float, 0 < beta < 1. Generally close to 1.  epsilon : float >= 0. Fuzz factor. If  None , defaults to  K.epsilon() .  decay : float >= 0. Learning rate decay over each update.  amsgrad : boolean. Whether to apply the AMSGrad variant of this\nalgorithm from the paper \"On the Convergence of Adam and\nBeyond\".   References   Adam - A Method for Stochastic Optimization  On the Convergence of Adam and Beyond    [source]",
            "title": "Adam"
        },
        {
            "location": "/optimizers/#adamax",
            "text": "keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)  Adamax optimizer from Adam paper's Section 7.  It is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.  Arguments   lr : float >= 0. Learning rate.  beta_1/beta_2 : floats, 0 < beta < 1. Generally close to 1.  epsilon : float >= 0. Fuzz factor. If  None , defaults to  K.epsilon() .  decay : float >= 0. Learning rate decay over each update.   References   Adam - A Method for Stochastic Optimization    [source]",
            "title": "Adamax"
        },
        {
            "location": "/optimizers/#nadam",
            "text": "keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)  Nesterov Adam optimizer.  Much like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.  Default parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float >= 0. Learning rate.  beta_1/beta_2 : floats, 0 < beta < 1. Generally close to 1.  epsilon : float >= 0. Fuzz factor. If  None , defaults to  K.epsilon() .   References   Nadam report  On the importance of initialization and momentum in deep learning    [source]",
            "title": "Nadam"
        },
        {
            "location": "/optimizers/#tfoptimizer",
            "text": "keras.optimizers.TFOptimizer(optimizer)  Wrapper class for native TensorFlow optimizers.",
            "title": "TFOptimizer"
        },
        {
            "location": "/activations/",
            "text": "Usage of activations\n\n\nActivations can either be used through an \nActivation\n layer, or through the \nactivation\n argument supported by all forward layers:\n\n\nfrom keras.layers import Activation, Dense\n\nmodel.add(Dense(64))\nmodel.add(Activation('tanh'))\n\n\n\n\nThis is equivalent to:\n\n\nmodel.add(Dense(64, activation='tanh'))\n\n\n\n\nYou can also pass an element-wise TensorFlow/Theano/CNTK function as an activation:\n\n\nfrom keras import backend as K\n\nmodel.add(Dense(64, activation=K.tanh))\nmodel.add(Activation(K.tanh))\n\n\n\n\nAvailable activations\n\n\nsoftmax\n\n\nsoftmax(x, axis=-1)\n\n\n\n\nSoftmax activation function.\n\n\nArguments\n\n\nx : Tensor.\n- \naxis\n: Integer, axis along which the softmax normalization is applied.\n\n\nReturns\n\n\nTensor, output of softmax transformation.\n\n\nRaises\n\n\n\n\nValueError\n: In case \ndim(x) == 1\n.\n\n\n\n\n\n\nelu\n\n\nelu(x, alpha=1.0)\n\n\n\n\n\n\nselu\n\n\nselu(x)\n\n\n\n\nScaled Exponential Linear Unit. (Klambauer et al., 2017).\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable to compute the activation function for.\n\n\n\n\nReturns\n\n\nTensor with the same shape and dtype as \nx\n.\n\n\nNote\n\n\n\n\nTo be used together with the initialization \"lecun_normal\".\n\n\nTo be used together with the dropout variant \"AlphaDropout\".\n\n\n\n\nReferences\n\n\n\n\nSelf-Normalizing Neural Networks\n\n\n\n\n\n\nsoftplus\n\n\nsoftplus(x)\n\n\n\n\n\n\nsoftsign\n\n\nsoftsign(x)\n\n\n\n\n\n\nrelu\n\n\nrelu(x, alpha=0.0, max_value=None)\n\n\n\n\n\n\ntanh\n\n\ntanh(x)\n\n\n\n\n\n\nsigmoid\n\n\nsigmoid(x)\n\n\n\n\n\n\nhard_sigmoid\n\n\nhard_sigmoid(x)\n\n\n\n\n\n\nlinear\n\n\nlinear(x)\n\n\n\n\nOn \"Advanced Activations\"\n\n\nActivations that are more complex than a simple TensorFlow/Theano/CNTK function (eg. learnable activations, which maintain a state) are available as \nAdvanced Activation layers\n, and can be found in the module \nkeras.layers.advanced_activations\n. These include \nPReLU\n and \nLeakyReLU\n.",
            "title": "Activations"
        },
        {
            "location": "/activations/#usage-of-activations",
            "text": "Activations can either be used through an  Activation  layer, or through the  activation  argument supported by all forward layers:  from keras.layers import Activation, Dense\n\nmodel.add(Dense(64))\nmodel.add(Activation('tanh'))  This is equivalent to:  model.add(Dense(64, activation='tanh'))  You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation:  from keras import backend as K\n\nmodel.add(Dense(64, activation=K.tanh))\nmodel.add(Activation(K.tanh))",
            "title": "Usage of activations"
        },
        {
            "location": "/activations/#available-activations",
            "text": "",
            "title": "Available activations"
        },
        {
            "location": "/activations/#softmax",
            "text": "softmax(x, axis=-1)  Softmax activation function.  Arguments  x : Tensor.\n-  axis : Integer, axis along which the softmax normalization is applied.  Returns  Tensor, output of softmax transformation.  Raises   ValueError : In case  dim(x) == 1 .",
            "title": "softmax"
        },
        {
            "location": "/activations/#elu",
            "text": "elu(x, alpha=1.0)",
            "title": "elu"
        },
        {
            "location": "/activations/#selu",
            "text": "selu(x)  Scaled Exponential Linear Unit. (Klambauer et al., 2017).  Arguments   x : A tensor or variable to compute the activation function for.   Returns  Tensor with the same shape and dtype as  x .  Note   To be used together with the initialization \"lecun_normal\".  To be used together with the dropout variant \"AlphaDropout\".   References   Self-Normalizing Neural Networks",
            "title": "selu"
        },
        {
            "location": "/activations/#softplus",
            "text": "softplus(x)",
            "title": "softplus"
        },
        {
            "location": "/activations/#softsign",
            "text": "softsign(x)",
            "title": "softsign"
        },
        {
            "location": "/activations/#relu",
            "text": "relu(x, alpha=0.0, max_value=None)",
            "title": "relu"
        },
        {
            "location": "/activations/#tanh",
            "text": "tanh(x)",
            "title": "tanh"
        },
        {
            "location": "/activations/#sigmoid",
            "text": "sigmoid(x)",
            "title": "sigmoid"
        },
        {
            "location": "/activations/#hard_sigmoid",
            "text": "hard_sigmoid(x)",
            "title": "hard_sigmoid"
        },
        {
            "location": "/activations/#linear",
            "text": "linear(x)",
            "title": "linear"
        },
        {
            "location": "/activations/#on-advanced-activations",
            "text": "Activations that are more complex than a simple TensorFlow/Theano/CNTK function (eg. learnable activations, which maintain a state) are available as  Advanced Activation layers , and can be found in the module  keras.layers.advanced_activations . These include  PReLU  and  LeakyReLU .",
            "title": "On \"Advanced Activations\""
        },
        {
            "location": "/callbacks/",
            "text": "Usage of callbacks\n\n\nA callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument \ncallbacks\n) to the \n.fit()\n method of the \nSequential\n or \nModel\n classes. The relevant methods of the callbacks will then be called at each stage of the training. \n\n\n\n\n[source]\n\n\nCallback\n\n\nkeras.callbacks.Callback()\n\n\n\n\nAbstract base class used to build new callbacks.\n\n\nProperties\n\n\n\n\nparams\n: dict. Training parameters\n(eg. verbosity, batch size, number of epochs...).\n\n\nmodel\n: instance of \nkeras.models.Model\n.\nReference of the model being trained.\n\n\n\n\nThe \nlogs\n dictionary that callback methods\ntake as argument will contain keys for quantities relevant to\nthe current batch or epoch.\n\n\nCurrently, the \n.fit()\n method of the \nSequential\n model class\nwill include the following quantities in the \nlogs\n that\nit passes to its callbacks:\n\n\n\n\non_epoch_end\n: logs include \nacc\n and \nloss\n, and\noptionally include \nval_loss\n\n(if validation is enabled in \nfit\n), and \nval_acc\n\n(if validation and accuracy monitoring are enabled).\n\n\non_batch_begin\n: logs include \nsize\n,\nthe number of samples in the current batch.\n\n\non_batch_end\n: logs include \nloss\n, and optionally \nacc\n\n(if accuracy monitoring is enabled).\n\n\n\n\n\n\n[source]\n\n\nBaseLogger\n\n\nkeras.callbacks.BaseLogger(stateful_metrics=None)\n\n\n\n\nCallback that accumulates epoch averages of metrics.\n\n\nThis callback is automatically applied to every Keras model.\n\n\nArguments\n\n\n\n\nstateful_metrics\n: Iterable of string names of metrics that\nshould \nnot\n be averaged over an epoch.\nMetrics in this list will be logged as-is in \non_epoch_end\n.\nAll others will be averaged in \non_epoch_end\n.\n\n\n\n\n\n\n[source]\n\n\nTerminateOnNaN\n\n\nkeras.callbacks.TerminateOnNaN()\n\n\n\n\nCallback that terminates training when a NaN loss is encountered.\n\n\n\n\n[source]\n\n\nProgbarLogger\n\n\nkeras.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None)\n\n\n\n\nCallback that prints metrics to stdout.\n\n\nArguments\n\n\n\n\ncount_mode\n: One of \"steps\" or \"samples\".\nWhether the progress bar should\ncount samples seen or steps (batches) seen.\n\n\nstateful_metrics\n: Iterable of string names of metrics that\nshould \nnot\n be averaged over an epoch.\nMetrics in this list will be logged as-is.\nAll others will be averaged over time (e.g. loss, etc).\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of invalid \ncount_mode\n.\n\n\n\n\n\n\n[source]\n\n\nHistory\n\n\nkeras.callbacks.History()\n\n\n\n\nCallback that records events into a \nHistory\n object.\n\n\nThis callback is automatically applied to\nevery Keras model. The \nHistory\n object\ngets returned by the \nfit\n method of models.\n\n\n\n\n[source]\n\n\nModelCheckpoint\n\n\nkeras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\n\n\n\nSave the model after every epoch.\n\n\nfilepath\n can contain named formatting options,\nwhich will be filled the value of \nepoch\n and\nkeys in \nlogs\n (passed in \non_epoch_end\n).\n\n\nFor example: if \nfilepath\n is \nweights.{epoch:02d}-{val_loss:.2f}.hdf5\n,\nthen the model checkpoints will be saved with the epoch number and\nthe validation loss in the filename.\n\n\nArguments\n\n\n\n\nfilepath\n: string, path to save the model file.\n\n\nmonitor\n: quantity to monitor.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\nsave_best_only\n: if \nsave_best_only=True\n,\nthe latest best model according to\nthe quantity monitored will not be overwritten.\n\n\nmode\n: one of {auto, min, max}.\nIf \nsave_best_only=True\n, the decision\nto overwrite the current save file is made\nbased on either the maximization or the\nminimization of the monitored quantity. For \nval_acc\n,\nthis should be \nmax\n, for \nval_loss\n this should\nbe \nmin\n, etc. In \nauto\n mode, the direction is\nautomatically inferred from the name of the monitored quantity.\n\n\nsave_weights_only\n: if True, then only the model's weights will be\nsaved (\nmodel.save_weights(filepath)\n), else the full model\nis saved (\nmodel.save(filepath)\n).\n\n\nperiod\n: Interval (number of epochs) between checkpoints.\n\n\n\n\n\n\n[source]\n\n\nEarlyStopping\n\n\nkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n\n\n\nStop training when a monitored quantity has stopped improving.\n\n\nArguments\n\n\n\n\nmonitor\n: quantity to be monitored.\n\n\nmin_delta\n: minimum change in the monitored quantity\nto qualify as an improvement, i.e. an absolute\nchange of less than min_delta, will count as no\nimprovement.\n\n\npatience\n: number of epochs with no improvement\nafter which training will be stopped.\n\n\nverbose\n: verbosity mode.\n\n\nmode\n: one of {auto, min, max}. In \nmin\n mode,\ntraining will stop when the quantity\nmonitored has stopped decreasing; in \nmax\n\nmode it will stop when the quantity\nmonitored has stopped increasing; in \nauto\n\nmode, the direction is automatically inferred\nfrom the name of the monitored quantity.\n\n\n\n\n\n\n[source]\n\n\nRemoteMonitor\n\n\nkeras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None)\n\n\n\n\nCallback used to stream events to a server.\n\n\nRequires the \nrequests\n library.\nEvents are sent to \nroot + '/publish/epoch/end/'\n by default. Calls are\nHTTP POST, with a \ndata\n argument which is a\nJSON-encoded dictionary of event data.\n\n\nArguments\n\n\n\n\nroot\n: String; root url of the target server.\n\n\npath\n: String; path relative to \nroot\n to which the events will be sent.\n\n\nfield\n: String; JSON field under which the data will be stored.\n\n\nheaders\n: Dictionary; optional custom HTTP headers.\n\n\n\n\n\n\n[source]\n\n\nLearningRateScheduler\n\n\nkeras.callbacks.LearningRateScheduler(schedule, verbose=0)\n\n\n\n\nLearning rate scheduler.\n\n\nArguments\n\n\n\n\nschedule\n: a function that takes an epoch index as input\n(integer, indexed from 0) and current learning rate\nand returns a new learning rate as output (float).\n\n\nverbose\n: int. 0: quiet, 1: update messages.\n\n\n\n\n\n\n[source]\n\n\nTensorBoard\n\n\nkeras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n\n\n\n\nTensorBoard basic visualizations.\n\n\nTensorBoard\n\nis a visualization tool provided with TensorFlow.\n\n\nThis callback writes a log for TensorBoard, which allows\nyou to visualize dynamic graphs of your training and test\nmetrics, as well as activation histograms for the different\nlayers in your model.\n\n\nIf you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:\n\n\ntensorboard --logdir=/full_path_to_your_logs\n\n\n\n\nWhen using a backend other than TensorFlow, TensorBoard will still work\n(if you have TensorFlow installed), but the only feature available will\nbe the display of the losses and metrics plots.\n\n\nArguments\n\n\n\n\nlog_dir\n: the path of the directory where to save the log\nfiles to be parsed by TensorBoard.\n\n\nhistogram_freq\n: frequency (in epochs) at which to compute activation\nand weight histograms for the layers of the model. If set to 0,\nhistograms won't be computed. Validation data (or split) must be\nspecified for histogram visualizations.\n\n\nwrite_graph\n: whether to visualize the graph in TensorBoard.\nThe log file can become quite large when\nwrite_graph is set to True.\n\n\nwrite_grads\n: whether to visualize gradient histograms in TensorBoard.\n\nhistogram_freq\n must be greater than 0.\n\n\nbatch_size\n: size of batch of inputs to feed to the network\nfor histograms computation.\n\n\nwrite_images\n: whether to write model weights to visualize as\nimage in TensorBoard.\n\n\nembeddings_freq\n: frequency (in epochs) at which selected embedding\nlayers will be saved.\n\n\nembeddings_layer_names\n: a list of names of layers to keep eye on. If\nNone or empty list all the embedding layer will be watched.\n\n\nembeddings_metadata\n: a dictionary which maps layer name to a file name\nin which metadata for this embedding layer is saved. See the\n\ndetails\n\nabout metadata files format. In case if the same metadata file is\nused for all embedding layers, string can be passed.\n\n\n\n\n\n\n[source]\n\n\nReduceLROnPlateau\n\n\nkeras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n\n\n\n\nReduce learning rate when a metric has stopped improving.\n\n\nModels often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This callback monitors a\nquantity and if no improvement is seen for a 'patience' number\nof epochs, the learning rate is reduced.\n\n\nExample\n\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nmodel.fit(X_train, Y_train, callbacks=[reduce_lr])\n\n\n\n\nArguments\n\n\n\n\nmonitor\n: quantity to be monitored.\n\n\nfactor\n: factor by which the learning rate will\nbe reduced. new_lr = lr * factor\n\n\npatience\n: number of epochs with no improvement\nafter which learning rate will be reduced.\n\n\nverbose\n: int. 0: quiet, 1: update messages.\n\n\nmode\n: one of {auto, min, max}. In \nmin\n mode,\nlr will be reduced when the quantity\nmonitored has stopped decreasing; in \nmax\n\nmode it will be reduced when the quantity\nmonitored has stopped increasing; in \nauto\n\nmode, the direction is automatically inferred\nfrom the name of the monitored quantity.\n\n\nepsilon\n: threshold for measuring the new optimum,\nto only focus on significant changes.\n\n\ncooldown\n: number of epochs to wait before resuming\nnormal operation after lr has been reduced.\n\n\nmin_lr\n: lower bound on the learning rate.\n\n\n\n\n\n\n[source]\n\n\nCSVLogger\n\n\nkeras.callbacks.CSVLogger(filename, separator=',', append=False)\n\n\n\n\nCallback that streams epoch results to a csv file.\n\n\nSupports all values that can be represented as a string,\nincluding 1D iterables such as np.ndarray.\n\n\nExample\n\n\ncsv_logger = CSVLogger('training.log')\nmodel.fit(X_train, Y_train, callbacks=[csv_logger])\n\n\n\n\nArguments\n\n\n\n\nfilename\n: filename of the csv file, e.g. 'run/log.csv'.\n\n\nseparator\n: string used to separate elements in the csv file.\n\n\nappend\n: True: append if file exists (useful for continuing\ntraining). False: overwrite existing file,\n\n\n\n\n\n\n[source]\n\n\nLambdaCallback\n\n\nkeras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None)\n\n\n\n\nCallback for creating simple, custom callbacks on-the-fly.\n\n\nThis callback is constructed with anonymous functions that will be called\nat the appropriate time. Note that the callbacks expects positional\narguments, as:\n\n\n\n\non_epoch_begin\n and \non_epoch_end\n expect two positional arguments:\n\nepoch\n, \nlogs\n\n\non_batch_begin\n and \non_batch_end\n expect two positional arguments:\n\nbatch\n, \nlogs\n\n\non_train_begin\n and \non_train_end\n expect one positional argument:\n\nlogs\n\n\n\n\nArguments\n\n\n\n\non_epoch_begin\n: called at the beginning of every epoch.\n\n\non_epoch_end\n: called at the end of every epoch.\n\n\non_batch_begin\n: called at the beginning of every batch.\n\n\non_batch_end\n: called at the end of every batch.\n\n\non_train_begin\n: called at the beginning of model training.\n\n\non_train_end\n: called at the end of model training.\n\n\n\n\nExample\n\n\n# Print the batch number at the beginning of every batch.\nbatch_print_callback = LambdaCallback(\n    on_batch_begin=lambda batch,logs: print(batch))\n\n# Stream the epoch loss to a file in JSON format. The file content\n# is not well-formed JSON but rather has a JSON object per line.\nimport json\njson_log = open('loss_log.json', mode='wt', buffering=1)\njson_logging_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: json_log.write(\n        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n    on_train_end=lambda logs: json_log.close()\n)\n\n# Terminate some processes after having finished model training.\nprocesses = ...\ncleanup_callback = LambdaCallback(\n    on_train_end=lambda logs: [\n        p.terminate() for p in processes if p.is_alive()])\n\nmodel.fit(...,\n          callbacks=[batch_print_callback,\n                     json_logging_callback,\n                     cleanup_callback])\n\n\n\n\n\n\nCreate a callback\n\n\nYou can create a custom callback by extending the base class \nkeras.callbacks.Callback\n. A callback has access to its associated model through the class property \nself.model\n.\n\n\nHere's a simple example saving a list of losses over each batch during training:\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\n\n\n\n\n\nExample: recording loss history\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nhistory = LossHistory()\nmodel.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])\n\nprint(history.losses)\n# outputs\n'''\n[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n'''\n\n\n\n\n\n\nExample: model checkpoints\n\n\nfrom keras.callbacks import ModelCheckpoint\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n'''\nsaves the model weights after each epoch if the validation loss decreased\n'''\ncheckpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\nmodel.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])",
            "title": "Callbacks"
        },
        {
            "location": "/callbacks/#usage-of-callbacks",
            "text": "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument  callbacks ) to the  .fit()  method of the  Sequential  or  Model  classes. The relevant methods of the callbacks will then be called at each stage of the training.    [source]",
            "title": "Usage of callbacks"
        },
        {
            "location": "/callbacks/#callback",
            "text": "keras.callbacks.Callback()  Abstract base class used to build new callbacks.  Properties   params : dict. Training parameters\n(eg. verbosity, batch size, number of epochs...).  model : instance of  keras.models.Model .\nReference of the model being trained.   The  logs  dictionary that callback methods\ntake as argument will contain keys for quantities relevant to\nthe current batch or epoch.  Currently, the  .fit()  method of the  Sequential  model class\nwill include the following quantities in the  logs  that\nit passes to its callbacks:   on_epoch_end : logs include  acc  and  loss , and\noptionally include  val_loss \n(if validation is enabled in  fit ), and  val_acc \n(if validation and accuracy monitoring are enabled).  on_batch_begin : logs include  size ,\nthe number of samples in the current batch.  on_batch_end : logs include  loss , and optionally  acc \n(if accuracy monitoring is enabled).    [source]",
            "title": "Callback"
        },
        {
            "location": "/callbacks/#baselogger",
            "text": "keras.callbacks.BaseLogger(stateful_metrics=None)  Callback that accumulates epoch averages of metrics.  This callback is automatically applied to every Keras model.  Arguments   stateful_metrics : Iterable of string names of metrics that\nshould  not  be averaged over an epoch.\nMetrics in this list will be logged as-is in  on_epoch_end .\nAll others will be averaged in  on_epoch_end .    [source]",
            "title": "BaseLogger"
        },
        {
            "location": "/callbacks/#terminateonnan",
            "text": "keras.callbacks.TerminateOnNaN()  Callback that terminates training when a NaN loss is encountered.   [source]",
            "title": "TerminateOnNaN"
        },
        {
            "location": "/callbacks/#progbarlogger",
            "text": "keras.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None)  Callback that prints metrics to stdout.  Arguments   count_mode : One of \"steps\" or \"samples\".\nWhether the progress bar should\ncount samples seen or steps (batches) seen.  stateful_metrics : Iterable of string names of metrics that\nshould  not  be averaged over an epoch.\nMetrics in this list will be logged as-is.\nAll others will be averaged over time (e.g. loss, etc).   Raises   ValueError : In case of invalid  count_mode .    [source]",
            "title": "ProgbarLogger"
        },
        {
            "location": "/callbacks/#history",
            "text": "keras.callbacks.History()  Callback that records events into a  History  object.  This callback is automatically applied to\nevery Keras model. The  History  object\ngets returned by the  fit  method of models.   [source]",
            "title": "History"
        },
        {
            "location": "/callbacks/#modelcheckpoint",
            "text": "keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)  Save the model after every epoch.  filepath  can contain named formatting options,\nwhich will be filled the value of  epoch  and\nkeys in  logs  (passed in  on_epoch_end ).  For example: if  filepath  is  weights.{epoch:02d}-{val_loss:.2f}.hdf5 ,\nthen the model checkpoints will be saved with the epoch number and\nthe validation loss in the filename.  Arguments   filepath : string, path to save the model file.  monitor : quantity to monitor.  verbose : verbosity mode, 0 or 1.  save_best_only : if  save_best_only=True ,\nthe latest best model according to\nthe quantity monitored will not be overwritten.  mode : one of {auto, min, max}.\nIf  save_best_only=True , the decision\nto overwrite the current save file is made\nbased on either the maximization or the\nminimization of the monitored quantity. For  val_acc ,\nthis should be  max , for  val_loss  this should\nbe  min , etc. In  auto  mode, the direction is\nautomatically inferred from the name of the monitored quantity.  save_weights_only : if True, then only the model's weights will be\nsaved ( model.save_weights(filepath) ), else the full model\nis saved ( model.save(filepath) ).  period : Interval (number of epochs) between checkpoints.    [source]",
            "title": "ModelCheckpoint"
        },
        {
            "location": "/callbacks/#earlystopping",
            "text": "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')  Stop training when a monitored quantity has stopped improving.  Arguments   monitor : quantity to be monitored.  min_delta : minimum change in the monitored quantity\nto qualify as an improvement, i.e. an absolute\nchange of less than min_delta, will count as no\nimprovement.  patience : number of epochs with no improvement\nafter which training will be stopped.  verbose : verbosity mode.  mode : one of {auto, min, max}. In  min  mode,\ntraining will stop when the quantity\nmonitored has stopped decreasing; in  max \nmode it will stop when the quantity\nmonitored has stopped increasing; in  auto \nmode, the direction is automatically inferred\nfrom the name of the monitored quantity.    [source]",
            "title": "EarlyStopping"
        },
        {
            "location": "/callbacks/#remotemonitor",
            "text": "keras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None)  Callback used to stream events to a server.  Requires the  requests  library.\nEvents are sent to  root + '/publish/epoch/end/'  by default. Calls are\nHTTP POST, with a  data  argument which is a\nJSON-encoded dictionary of event data.  Arguments   root : String; root url of the target server.  path : String; path relative to  root  to which the events will be sent.  field : String; JSON field under which the data will be stored.  headers : Dictionary; optional custom HTTP headers.    [source]",
            "title": "RemoteMonitor"
        },
        {
            "location": "/callbacks/#learningratescheduler",
            "text": "keras.callbacks.LearningRateScheduler(schedule, verbose=0)  Learning rate scheduler.  Arguments   schedule : a function that takes an epoch index as input\n(integer, indexed from 0) and current learning rate\nand returns a new learning rate as output (float).  verbose : int. 0: quiet, 1: update messages.    [source]",
            "title": "LearningRateScheduler"
        },
        {
            "location": "/callbacks/#tensorboard",
            "text": "keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)  TensorBoard basic visualizations.  TensorBoard \nis a visualization tool provided with TensorFlow.  This callback writes a log for TensorBoard, which allows\nyou to visualize dynamic graphs of your training and test\nmetrics, as well as activation histograms for the different\nlayers in your model.  If you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:  tensorboard --logdir=/full_path_to_your_logs  When using a backend other than TensorFlow, TensorBoard will still work\n(if you have TensorFlow installed), but the only feature available will\nbe the display of the losses and metrics plots.  Arguments   log_dir : the path of the directory where to save the log\nfiles to be parsed by TensorBoard.  histogram_freq : frequency (in epochs) at which to compute activation\nand weight histograms for the layers of the model. If set to 0,\nhistograms won't be computed. Validation data (or split) must be\nspecified for histogram visualizations.  write_graph : whether to visualize the graph in TensorBoard.\nThe log file can become quite large when\nwrite_graph is set to True.  write_grads : whether to visualize gradient histograms in TensorBoard. histogram_freq  must be greater than 0.  batch_size : size of batch of inputs to feed to the network\nfor histograms computation.  write_images : whether to write model weights to visualize as\nimage in TensorBoard.  embeddings_freq : frequency (in epochs) at which selected embedding\nlayers will be saved.  embeddings_layer_names : a list of names of layers to keep eye on. If\nNone or empty list all the embedding layer will be watched.  embeddings_metadata : a dictionary which maps layer name to a file name\nin which metadata for this embedding layer is saved. See the details \nabout metadata files format. In case if the same metadata file is\nused for all embedding layers, string can be passed.    [source]",
            "title": "TensorBoard"
        },
        {
            "location": "/callbacks/#reducelronplateau",
            "text": "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)  Reduce learning rate when a metric has stopped improving.  Models often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This callback monitors a\nquantity and if no improvement is seen for a 'patience' number\nof epochs, the learning rate is reduced.  Example  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nmodel.fit(X_train, Y_train, callbacks=[reduce_lr])  Arguments   monitor : quantity to be monitored.  factor : factor by which the learning rate will\nbe reduced. new_lr = lr * factor  patience : number of epochs with no improvement\nafter which learning rate will be reduced.  verbose : int. 0: quiet, 1: update messages.  mode : one of {auto, min, max}. In  min  mode,\nlr will be reduced when the quantity\nmonitored has stopped decreasing; in  max \nmode it will be reduced when the quantity\nmonitored has stopped increasing; in  auto \nmode, the direction is automatically inferred\nfrom the name of the monitored quantity.  epsilon : threshold for measuring the new optimum,\nto only focus on significant changes.  cooldown : number of epochs to wait before resuming\nnormal operation after lr has been reduced.  min_lr : lower bound on the learning rate.    [source]",
            "title": "ReduceLROnPlateau"
        },
        {
            "location": "/callbacks/#csvlogger",
            "text": "keras.callbacks.CSVLogger(filename, separator=',', append=False)  Callback that streams epoch results to a csv file.  Supports all values that can be represented as a string,\nincluding 1D iterables such as np.ndarray.  Example  csv_logger = CSVLogger('training.log')\nmodel.fit(X_train, Y_train, callbacks=[csv_logger])  Arguments   filename : filename of the csv file, e.g. 'run/log.csv'.  separator : string used to separate elements in the csv file.  append : True: append if file exists (useful for continuing\ntraining). False: overwrite existing file,    [source]",
            "title": "CSVLogger"
        },
        {
            "location": "/callbacks/#lambdacallback",
            "text": "keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None)  Callback for creating simple, custom callbacks on-the-fly.  This callback is constructed with anonymous functions that will be called\nat the appropriate time. Note that the callbacks expects positional\narguments, as:   on_epoch_begin  and  on_epoch_end  expect two positional arguments: epoch ,  logs  on_batch_begin  and  on_batch_end  expect two positional arguments: batch ,  logs  on_train_begin  and  on_train_end  expect one positional argument: logs   Arguments   on_epoch_begin : called at the beginning of every epoch.  on_epoch_end : called at the end of every epoch.  on_batch_begin : called at the beginning of every batch.  on_batch_end : called at the end of every batch.  on_train_begin : called at the beginning of model training.  on_train_end : called at the end of model training.   Example  # Print the batch number at the beginning of every batch.\nbatch_print_callback = LambdaCallback(\n    on_batch_begin=lambda batch,logs: print(batch))\n\n# Stream the epoch loss to a file in JSON format. The file content\n# is not well-formed JSON but rather has a JSON object per line.\nimport json\njson_log = open('loss_log.json', mode='wt', buffering=1)\njson_logging_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: json_log.write(\n        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n    on_train_end=lambda logs: json_log.close()\n)\n\n# Terminate some processes after having finished model training.\nprocesses = ...\ncleanup_callback = LambdaCallback(\n    on_train_end=lambda logs: [\n        p.terminate() for p in processes if p.is_alive()])\n\nmodel.fit(...,\n          callbacks=[batch_print_callback,\n                     json_logging_callback,\n                     cleanup_callback])",
            "title": "LambdaCallback"
        },
        {
            "location": "/callbacks/#create-a-callback",
            "text": "You can create a custom callback by extending the base class  keras.callbacks.Callback . A callback has access to its associated model through the class property  self.model .  Here's a simple example saving a list of losses over each batch during training:  class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))",
            "title": "Create a callback"
        },
        {
            "location": "/callbacks/#example-recording-loss-history",
            "text": "class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nhistory = LossHistory()\nmodel.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])\n\nprint(history.losses)\n# outputs\n'''\n[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n'''",
            "title": "Example: recording loss history"
        },
        {
            "location": "/callbacks/#example-model-checkpoints",
            "text": "from keras.callbacks import ModelCheckpoint\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n'''\nsaves the model weights after each epoch if the validation loss decreased\n'''\ncheckpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\nmodel.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])",
            "title": "Example: model checkpoints"
        },
        {
            "location": "/datasets/",
            "text": "Datasets\n\n\nCIFAR10 small image classification\n\n\nDataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n\n\nUsage:\n\n\nfrom keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n\n\n\n\n\nReturns:\n\n\n2 tuples:\n\n\nx_train, x_test\n: uint8 array of RGB image data with shape (num_samples, 3, 32, 32).\n\n\ny_train, y_test\n: uint8 array of category labels (integers in range 0-9) with shape (num_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIFAR100 small image classification\n\n\nDataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.\n\n\nUsage:\n\n\nfrom keras.datasets import cifar100\n\n(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n\n2 tuples:\n\n\nx_train, x_test\n: uint8 array of RGB image data with shape (num_samples, 3, 32, 32).\n\n\ny_train, y_test\n: uint8 array of category labels with shape (num_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\nlabel_mode\n: \"fine\" or \"coarse\".\n\n\n\n\n\n\n\n\n\n\nIMDB Movie reviews sentiment classification\n\n\nDataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a \nsequence\n of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n\n\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n\n\nUsage:\n\n\nfrom keras.datasets import imdb\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n                                                      num_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n\n2 tuples:\n\n\nx_train, x_test\n: list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.\n\n\ny_train, y_test\n: list of integer labels (1 or 0). \n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do not have the data locally (at \n'~/.keras/datasets/' + path\n), it will be downloaded to this location.\n\n\nnum_words\n: integer or None. Top most frequent words to consider. Any less frequent word will appear as \noov_char\n value in the sequence data.\n\n\nskip_top\n: integer. Top most frequent words to ignore (they will appear as \noov_char\n value in the sequence data).\n\n\nmaxlen\n: int. Maximum sequence length. Any longer sequence will be truncated.\n\n\nseed\n: int. Seed for reproducible data shuffling.\n\n\nstart_char\n: int. The start of a sequence will be marked with this character.\n    Set to 1 because 0 is usually the padding character.\n\n\noov_char\n: int. words that were cut out because of the \nnum_words\n\n    or \nskip_top\n limit will be replaced with this character.\n\n\nindex_from\n: int. Index actual words with this index and higher.\n\n\n\n\n\n\n\n\n\n\nReuters newswire topics classification\n\n\nDataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).\n\n\nUsage:\n\n\nfrom keras.datasets import reuters\n\n(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n                                                         num_words=None,\n                                                         skip_top=0,\n                                                         maxlen=None,\n                                                         test_split=0.2,\n                                                         seed=113,\n                                                         start_char=1,\n                                                         oov_char=2,\n                                                         index_from=3)\n\n\n\n\nThe specifications are the same as that of the IMDB dataset, with the addition of:\n\n\n\n\ntest_split\n: float. Fraction of the dataset to be used as test data.\n\n\n\n\nThis dataset also makes available the word index used for encoding the sequences:\n\n\nword_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n\n\n\n\n\n\n\n\nReturns:\n A dictionary where key are words (str) and values are indexes (integer). eg. \nword_index[\"giraffe\"]\n might return \n1234\n. \n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do not have the index file locally (at \n'~/.keras/datasets/' + path\n), it will be downloaded to this location.\n\n\n\n\n\n\n\n\n\n\nMNIST database of handwritten digits\n\n\nDataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n\n\nUsage:\n\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n\n2 tuples:\n\n\nx_train, x_test\n: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n\n\ny_train, y_test\n: uint8 array of digit labels (integers in range 0-9) with shape (num_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do not have the index file locally (at \n'~/.keras/datasets/' + path\n), it will be downloaded to this location.\n\n\n\n\n\n\n\n\n\n\nFashion-MNIST database of fashion articles\n\n\nDataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n\n\n\n\n\n\n\n\nLabel\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n0\n\n\nT-shirt/top\n\n\n\n\n\n\n1\n\n\nTrouser\n\n\n\n\n\n\n2\n\n\nPullover\n\n\n\n\n\n\n3\n\n\nDress\n\n\n\n\n\n\n4\n\n\nCoat\n\n\n\n\n\n\n5\n\n\nSandal\n\n\n\n\n\n\n6\n\n\nShirt\n\n\n\n\n\n\n7\n\n\nSneaker\n\n\n\n\n\n\n8\n\n\nBag\n\n\n\n\n\n\n9\n\n\nAnkle boot\n\n\n\n\n\n\n\n\nUsage:\n\n\nfrom keras.datasets import fashion_mnist\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n\n\n\n\n\nReturns:\n\n\n2 tuples:\n\n\nx_train, x_test\n: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n\n\ny_train, y_test\n: uint8 array of labels (integers in range 0-9) with shape (num_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoston housing price regression dataset\n\n\nDataset taken from the StatLib library which is maintained at Carnegie Mellon University. \n\n\nSamples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s.\nTargets are the median values of the houses at a location (in k$).\n\n\nUsage:\n\n\nfrom keras.datasets import boston_housing\n\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: path where to cache the dataset locally\n    (relative to ~/.keras/datasets).\n\n\nseed\n: Random seed for shuffling the data\n    before computing the test split.\n\n\ntest_split\n: fraction of the data to reserve as test set.\n\n\n\n\n\n\n\n\nReturns:\n\n    Tuple of Numpy arrays: \n(x_train, y_train), (x_test, y_test)\n.",
            "title": "Datasets"
        },
        {
            "location": "/datasets/#datasets",
            "text": "",
            "title": "Datasets"
        },
        {
            "location": "/datasets/#cifar10-small-image-classification",
            "text": "Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.",
            "title": "CIFAR10 small image classification"
        },
        {
            "location": "/datasets/#usage",
            "text": "from keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()   Returns:  2 tuples:  x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32).  y_train, y_test : uint8 array of category labels (integers in range 0-9) with shape (num_samples,).",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#cifar100-small-image-classification",
            "text": "Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.",
            "title": "CIFAR100 small image classification"
        },
        {
            "location": "/datasets/#usage_1",
            "text": "from keras.datasets import cifar100\n\n(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')    Returns:   2 tuples:  x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32).  y_train, y_test : uint8 array of category labels with shape (num_samples,).       Arguments:   label_mode : \"fine\" or \"coarse\".",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#imdb-movie-reviews-sentiment-classification",
            "text": "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a  sequence  of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".  As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.",
            "title": "IMDB Movie reviews sentiment classification"
        },
        {
            "location": "/datasets/#usage_2",
            "text": "from keras.datasets import imdb\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n                                                      num_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)    Returns:   2 tuples:  x_train, x_test : list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.  y_train, y_test : list of integer labels (1 or 0).        Arguments:   path : if you do not have the data locally (at  '~/.keras/datasets/' + path ), it will be downloaded to this location.  num_words : integer or None. Top most frequent words to consider. Any less frequent word will appear as  oov_char  value in the sequence data.  skip_top : integer. Top most frequent words to ignore (they will appear as  oov_char  value in the sequence data).  maxlen : int. Maximum sequence length. Any longer sequence will be truncated.  seed : int. Seed for reproducible data shuffling.  start_char : int. The start of a sequence will be marked with this character.\n    Set to 1 because 0 is usually the padding character.  oov_char : int. words that were cut out because of the  num_words \n    or  skip_top  limit will be replaced with this character.  index_from : int. Index actual words with this index and higher.",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#reuters-newswire-topics-classification",
            "text": "Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).",
            "title": "Reuters newswire topics classification"
        },
        {
            "location": "/datasets/#usage_3",
            "text": "from keras.datasets import reuters\n\n(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n                                                         num_words=None,\n                                                         skip_top=0,\n                                                         maxlen=None,\n                                                         test_split=0.2,\n                                                         seed=113,\n                                                         start_char=1,\n                                                         oov_char=2,\n                                                         index_from=3)  The specifications are the same as that of the IMDB dataset, with the addition of:   test_split : float. Fraction of the dataset to be used as test data.   This dataset also makes available the word index used for encoding the sequences:  word_index = reuters.get_word_index(path=\"reuters_word_index.json\")    Returns:  A dictionary where key are words (str) and values are indexes (integer). eg.  word_index[\"giraffe\"]  might return  1234 .     Arguments:   path : if you do not have the index file locally (at  '~/.keras/datasets/' + path ), it will be downloaded to this location.",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#mnist-database-of-handwritten-digits",
            "text": "Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.",
            "title": "MNIST database of handwritten digits"
        },
        {
            "location": "/datasets/#usage_4",
            "text": "from keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()    Returns:   2 tuples:  x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28).  y_train, y_test : uint8 array of digit labels (integers in range 0-9) with shape (num_samples,).       Arguments:   path : if you do not have the index file locally (at  '~/.keras/datasets/' + path ), it will be downloaded to this location.",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#fashion-mnist-database-of-fashion-articles",
            "text": "Dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:     Label  Description      0  T-shirt/top    1  Trouser    2  Pullover    3  Dress    4  Coat    5  Sandal    6  Shirt    7  Sneaker    8  Bag    9  Ankle boot",
            "title": "Fashion-MNIST database of fashion articles"
        },
        {
            "location": "/datasets/#usage_5",
            "text": "from keras.datasets import fashion_mnist\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()   Returns:  2 tuples:  x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28).  y_train, y_test : uint8 array of labels (integers in range 0-9) with shape (num_samples,).",
            "title": "Usage:"
        },
        {
            "location": "/datasets/#boston-housing-price-regression-dataset",
            "text": "Dataset taken from the StatLib library which is maintained at Carnegie Mellon University.   Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s.\nTargets are the median values of the houses at a location (in k$).",
            "title": "Boston housing price regression dataset"
        },
        {
            "location": "/datasets/#usage_6",
            "text": "from keras.datasets import boston_housing\n\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()    Arguments:   path : path where to cache the dataset locally\n    (relative to ~/.keras/datasets).  seed : Random seed for shuffling the data\n    before computing the test split.  test_split : fraction of the data to reserve as test set.     Returns: \n    Tuple of Numpy arrays:  (x_train, y_train), (x_test, y_test) .",
            "title": "Usage:"
        },
        {
            "location": "/applications/",
            "text": "Applications\n\n\nKeras Applications are deep learning models that are made available alongside pre-trained weights.\nThese models can be used for prediction, feature extraction, and fine-tuning.\n\n\nWeights are downloaded automatically when instantiating a model. They are stored at \n~/.keras/models/\n.\n\n\nAvailable models\n\n\nModels for image classification with weights trained on ImageNet:\n\n\n\n\nXception\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInceptionV3\n\n\nInceptionResNetV2\n\n\nMobileNet\n\n\nDenseNet\n\n\nNASNet\n\n\n\n\nAll of these architectures (except Xception and MobileNet) are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image data format set in your Keras configuration file at \n~/.keras/keras.json\n. For instance, if you have set \nimage_data_format=channels_last\n, then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\".\n\n\nThe Xception model is only available for TensorFlow, due to its reliance on \nSeparableConvolution\n layers.\nThe MobileNet model is only available for TensorFlow, due to its reliance on \nDepthwiseConvolution\n layers.\n\n\n\n\nUsage examples for image classification models\n\n\nClassify ImageNet classes with ResNet50\n\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\n\n\n\n\nExtract features with VGG16\n\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)\n\n\n\n\nExtract features from an arbitrary intermediate layer with VGG19\n\n\nfrom keras.applications.vgg19 import VGG19\nfrom keras.preprocessing import image\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.models import Model\nimport numpy as np\n\nbase_model = VGG19(weights='imagenet')\nmodel = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nblock4_pool_features = model.predict(x)\n\n\n\n\nFine-tune InceptionV3 on a new set of classes\n\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(200, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel.fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n   print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:249]:\n   layer.trainable = False\nfor layer in model.layers[249:]:\n   layer.trainable = True\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...)\n\n\n\n\nBuild InceptionV3 over a custom input tensor\n\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Input\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_data_format() == 'channels_last'\n\nmodel = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)\n\n\n\n\n\n\nDocumentation for individual models\n\n\n\n\n\n\n\n\nModel\n\n\nSize\n\n\nTop-1 Accuracy\n\n\nTop-5 Accuracy\n\n\nParameters\n\n\nDepth\n\n\n\n\n\n\n\n\n\n\nXception\n\n\n88 MB\n\n\n0.790\n\n\n0.945\n\n\n22,910,480\n\n\n126\n\n\n\n\n\n\nVGG16\n\n\n528 MB\n\n\n0.715\n\n\n0.901\n\n\n138,357,544\n\n\n23\n\n\n\n\n\n\nVGG19\n\n\n549 MB\n\n\n0.727\n\n\n0.910\n\n\n143,667,240\n\n\n26\n\n\n\n\n\n\nResNet50\n\n\n99 MB\n\n\n0.759\n\n\n0.929\n\n\n25,636,712\n\n\n168\n\n\n\n\n\n\nInceptionV3\n\n\n92 MB\n\n\n0.788\n\n\n0.944\n\n\n23,851,784\n\n\n159\n\n\n\n\n\n\nInceptionResNetV2\n\n\n215 MB\n\n\n0.804\n\n\n0.953\n\n\n55,873,736\n\n\n572\n\n\n\n\n\n\nMobileNet\n\n\n17 MB\n\n\n0.665\n\n\n0.871\n\n\n4,253,864\n\n\n88\n\n\n\n\n\n\nDenseNet121\n\n\n33 MB\n\n\n0.745\n\n\n0.918\n\n\n8,062,504\n\n\n121\n\n\n\n\n\n\nDenseNet169\n\n\n57 MB\n\n\n0.759\n\n\n0.928\n\n\n14,307,880\n\n\n169\n\n\n\n\n\n\nDenseNet201\n\n\n80 MB\n\n\n0.770\n\n\n0.933\n\n\n20,242,984\n\n\n201\n\n\n\n\n\n\n\n\nThe top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.\n\n\n\n\nXception\n\n\nkeras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nXception V1 model, with weights pre-trained on ImageNet.\n\n\nOn ImageNet, this model gets to a top-1 validation accuracy of 0.790\nand a top-5 validation accuracy of 0.945.\n\n\nNote that this model is only available for the TensorFlow backend,\ndue to its reliance on \nSeparableConvolution\n layers. Additionally it only supports\nthe data format \n'channels_last'\n (height, width, channels).\n\n\nThe default input size for this model is 299x299.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(299, 299, 3)\n.\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 71.\n    E.g. \n(150, 150, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nXception: Deep Learning with Depthwise Separable Convolutions\n\n\n\n\nLicense\n\n\nThese weights are trained by ourselves and are released under the MIT license.\n\n\n\n\nVGG16\n\n\nkeras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nVGG16 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \n'channels_first'\n data format (channels, height, width) or \n'channels_last'\n data format (height, width, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the 3 fully-connected layers at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 224, 224)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 48.\n    E.g. \n(200, 200, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition\n: please cite this paper if you use the VGG models in your work.\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by VGG at Oxford\n under the \nCreative Commons Attribution License\n.\n\n\n\n\nVGG19\n\n\nkeras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nVGG19 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \n'channels_first'\n data format (channels, height, width) or \n'channels_last'\n data format (height, width, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the 3 fully-connected layers at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 224, 224)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 48.\n    E.g. \n(200, 200, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by VGG at Oxford\n under the \nCreative Commons Attribution License\n.\n\n\n\n\nResNet50\n\n\nkeras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nResNet50 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \n'channels_first'\n data format (channels, height, width) or \n'channels_last'\n data format (height, width, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 224, 224)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 197.\n    E.g. \n(200, 200, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nDeep Residual Learning for Image Recognition\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by Kaiming He\n under the \nMIT license\n.\n\n\n\n\nInceptionV3\n\n\nkeras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nInception V3 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \n'channels_first'\n data format (channels, height, width) or \n'channels_last'\n data format (height, width, channels).\n\n\nThe default input size for this model is 299x299.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(299, 299, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 299, 299)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 139.\n    E.g. \n(150, 150, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nRethinking the Inception Architecture for Computer Vision\n\n\n\n\nLicense\n\n\nThese weights are released under \nthe Apache License\n.\n\n\n\n\nInceptionResNetV2\n\n\nkeras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nInception-ResNet V2 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for Theano, TensorFlow and CNTK backends, and can be built both\nwith \n'channels_first'\n data format (channels, height, width) or \n'channels_last'\n data format (height, width, channels).\n\n\nThe default input size for this model is 299x299.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \n'imagenet'\n (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(299, 299, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 299, 299)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 139.\n    E.g. \n(150, 150, 3)\n would be one valid value.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images \n    into, only to be specified if \ninclude_top\n is \nTrue\n, and \n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\n\n\n\n\nLicense\n\n\nThese weights are released under \nthe Apache License\n.\n\n\n\n\nMobileNet\n\n\nkeras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n\n\n\n\nMobileNet model, with weights pre-trained on ImageNet.\n\n\nNote that only TensorFlow is supported for now,\ntherefore it only works with the data format\n\nimage_data_format='channels_last'\n in your Keras config at \n~/.keras/keras.json\n.\nTo load a MobileNet model via \nload_model\n, import the custom objects \nrelu6\n and \nDepthwiseConv2D\n and pass them to the \ncustom_objects\n parameter.\n\n\nE.g.\n\n\nmodel = load_model('mobilenet.h5', custom_objects={\n                   'relu6': mobilenet.relu6,\n                   'DepthwiseConv2D': mobilenet.DepthwiseConv2D})\n\n\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 224, 224)\n (with \n'channels_first'\n data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 32.\n    E.g. \n(200, 200, 3)\n would be one valid value.\n\n\nalpha: controls the width of the network.\n\n\nIf \nalpha\n < 1.0, proportionally decreases the number\n    of filters in each layer.\n\n\nIf \nalpha\n > 1.0, proportionally increases the number\n    of filters in each layer.\n\n\nIf \nalpha\n = 1, default number of filters from the paper\n    are used at each layer.\n\n\n\n\n\n\ndepth_multiplier: depth multiplier for depthwise convolution\n    (also called the resolution multiplier)\n\n\ndropout: dropout rate\n\n\ninclude_top: whether to include the fully-connected\n    layer at the top of the network.\n\n\nweights: \nNone\n (random initialization) or\n    \n'imagenet'\n (ImageNet weights)\n\n\ninput_tensor: optional Keras tensor (i.e. output of\n    \nlayers.Input()\n)\n    to use as image input for the model.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model\nwill be the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a\n    2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images\n    into, only to be specified if \ninclude_top\n is \nTrue\n, and\n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n\n\n\n\nLicense\n\n\nThese weights are released under \nthe Apache License\n.\n\n\n\n\nDenseNet\n\n\nkeras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\nkeras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\nkeras.applications.densenet.DenseNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n\n\n\n\nOptionally loads weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set\n\nimage_data_format='channels_last'\n in your Keras config\nat ~/.keras/keras.json.\n\n\nThe model and the weights are compatible with\nTensorFlow, Theano, and CNTK. The data format\nconvention used by the model is the one\nspecified in your Keras config file.\n\n\nArguments\n\n\n\n\nblocks: numbers of building blocks for the four dense layers.\n\n\ninclude_top: whether to include the fully-connected\n    layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization),\n    'imagenet' (pre-training on ImageNet),\n    or the path to the weights file to be loaded.\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n)\n    to use as image input for the model.\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is False (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \nchannels_last\n data format)\n    or \n(3, 224, 224)\n (with \nchannels_first\n data format).\n    It should have exactly 3 inputs channels.\n\n\npooling: optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.\n\n\navg\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.\n\n\nmax\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images\n    into, only to be specified if \ninclude_top\n is True, and\n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nDensely Connected Convolutional Networks\n (CVPR 2017 Best Paper Award)\n\n\n\n\nLicense\n\n\nThese weights are released under \nthe BSD 3-clause License\n.\n\n\n\n\nNASNet\n\n\nkeras.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\nkeras.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n\n\n\n\nNeural Architecture Search Network (NASNet) model, with weights pre-trained on ImageNet.\n\n\nNote that only TensorFlow is supported for now,\ntherefore it only works with the data format\n\nimage_data_format='channels_last'\n in your Keras config at \n~/.keras/keras.json\n.\n\n\nThe default input size for the NASNetLarge model is 331x331 and for the\nNASNetMobile model is 224x224.\n\n\nArguments\n\n\n\n\ninput_shape: optional shape tuple, only to be specified\n    if \ninclude_top\n is \nFalse\n (otherwise the input shape\n    has to be \n(224, 224, 3)\n (with \n'channels_last'\n data format)\n    or \n(3, 224, 224)\n (with \n'channels_first'\n data format)\n    for NASNetMobile or \n(331, 331, 3)\n (with \n'channels_last'\n\n    data format) or \n(3, 331, 331)\n (with \n'channels_first'\n\n    data format) for NASNetLarge.\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 32.\n    E.g. \n(200, 200, 3)\n would be one valid value.\n\n\ninclude_top: whether to include the fully-connected\n    layer at the top of the network.\n\n\nweights: \nNone\n (random initialization) or\n    \n'imagenet'\n (ImageNet weights)\n\n\ninput_tensor: optional Keras tensor (i.e. output of\n    \nlayers.Input()\n)\n    to use as image input for the model.\n\n\npooling: Optional pooling mode for feature extraction\n    when \ninclude_top\n is \nFalse\n.\n\n\nNone\n means that the output of the model\nwill be the 4D tensor output of the\n    last convolutional layer.\n\n\n'avg'\n means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a\n    2D tensor.\n\n\n'max'\n means that global max pooling will\n    be applied.\n\n\n\n\n\n\nclasses: optional number of classes to classify images\n    into, only to be specified if \ninclude_top\n is \nTrue\n, and\n    if no \nweights\n argument is specified.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance.\n\n\nReferences\n\n\n\n\nLearning Transferable Architectures for Scalable Image Recognition\n\n\n\n\nLicense\n\n\nThese weights are released under \nthe Apache License\n.",
            "title": "Applications"
        },
        {
            "location": "/applications/#applications",
            "text": "Keras Applications are deep learning models that are made available alongside pre-trained weights.\nThese models can be used for prediction, feature extraction, and fine-tuning.  Weights are downloaded automatically when instantiating a model. They are stored at  ~/.keras/models/ .",
            "title": "Applications"
        },
        {
            "location": "/applications/#available-models",
            "text": "",
            "title": "Available models"
        },
        {
            "location": "/applications/#models-for-image-classification-with-weights-trained-on-imagenet",
            "text": "Xception  VGG16  VGG19  ResNet50  InceptionV3  InceptionResNetV2  MobileNet  DenseNet  NASNet   All of these architectures (except Xception and MobileNet) are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image data format set in your Keras configuration file at  ~/.keras/keras.json . For instance, if you have set  image_data_format=channels_last , then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\".  The Xception model is only available for TensorFlow, due to its reliance on  SeparableConvolution  layers.\nThe MobileNet model is only available for TensorFlow, due to its reliance on  DepthwiseConvolution  layers.",
            "title": "Models for image classification with weights trained on ImageNet:"
        },
        {
            "location": "/applications/#usage-examples-for-image-classification-models",
            "text": "",
            "title": "Usage examples for image classification models"
        },
        {
            "location": "/applications/#classify-imagenet-classes-with-resnet50",
            "text": "from keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]",
            "title": "Classify ImageNet classes with ResNet50"
        },
        {
            "location": "/applications/#extract-features-with-vgg16",
            "text": "from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)",
            "title": "Extract features with VGG16"
        },
        {
            "location": "/applications/#extract-features-from-an-arbitrary-intermediate-layer-with-vgg19",
            "text": "from keras.applications.vgg19 import VGG19\nfrom keras.preprocessing import image\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.models import Model\nimport numpy as np\n\nbase_model = VGG19(weights='imagenet')\nmodel = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nblock4_pool_features = model.predict(x)",
            "title": "Extract features from an arbitrary intermediate layer with VGG19"
        },
        {
            "location": "/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes",
            "text": "from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(200, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel.fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n   print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:249]:\n   layer.trainable = False\nfor layer in model.layers[249:]:\n   layer.trainable = True\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...)",
            "title": "Fine-tune InceptionV3 on a new set of classes"
        },
        {
            "location": "/applications/#build-inceptionv3-over-a-custom-input-tensor",
            "text": "from keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Input\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_data_format() == 'channels_last'\n\nmodel = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)",
            "title": "Build InceptionV3 over a custom input tensor"
        },
        {
            "location": "/applications/#documentation-for-individual-models",
            "text": "Model  Size  Top-1 Accuracy  Top-5 Accuracy  Parameters  Depth      Xception  88 MB  0.790  0.945  22,910,480  126    VGG16  528 MB  0.715  0.901  138,357,544  23    VGG19  549 MB  0.727  0.910  143,667,240  26    ResNet50  99 MB  0.759  0.929  25,636,712  168    InceptionV3  92 MB  0.788  0.944  23,851,784  159    InceptionResNetV2  215 MB  0.804  0.953  55,873,736  572    MobileNet  17 MB  0.665  0.871  4,253,864  88    DenseNet121  33 MB  0.745  0.918  8,062,504  121    DenseNet169  57 MB  0.759  0.928  14,307,880  169    DenseNet201  80 MB  0.770  0.933  20,242,984  201     The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.",
            "title": "Documentation for individual models"
        },
        {
            "location": "/applications/#xception",
            "text": "keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  Xception V1 model, with weights pre-trained on ImageNet.  On ImageNet, this model gets to a top-1 validation accuracy of 0.790\nand a top-5 validation accuracy of 0.945.  Note that this model is only available for the TensorFlow backend,\ndue to its reliance on  SeparableConvolution  layers. Additionally it only supports\nthe data format  'channels_last'  (height, width, channels).  The default input size for this model is 299x299.",
            "title": "Xception"
        },
        {
            "location": "/applications/#arguments",
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (299, 299, 3) .\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 71.\n    E.g.  (150, 150, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references",
            "text": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "title": "References"
        },
        {
            "location": "/applications/#license",
            "text": "These weights are trained by ourselves and are released under the MIT license.",
            "title": "License"
        },
        {
            "location": "/applications/#vgg16",
            "text": "keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  VGG16 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith  'channels_first'  data format (channels, height, width) or  'channels_last'  data format (height, width, channels).  The default input size for this model is 224x224.",
            "title": "VGG16"
        },
        {
            "location": "/applications/#arguments_1",
            "text": "include_top: whether to include the 3 fully-connected layers at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (224, 224, 3)  (with  'channels_last'  data format)\n    or  (3, 224, 224)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 48.\n    E.g.  (200, 200, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_1",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_1",
            "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition : please cite this paper if you use the VGG models in your work.",
            "title": "References"
        },
        {
            "location": "/applications/#license_1",
            "text": "These weights are ported from the ones  released by VGG at Oxford  under the  Creative Commons Attribution License .",
            "title": "License"
        },
        {
            "location": "/applications/#vgg19",
            "text": "keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  VGG19 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith  'channels_first'  data format (channels, height, width) or  'channels_last'  data format (height, width, channels).  The default input size for this model is 224x224.",
            "title": "VGG19"
        },
        {
            "location": "/applications/#arguments_2",
            "text": "include_top: whether to include the 3 fully-connected layers at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (224, 224, 3)  (with  'channels_last'  data format)\n    or  (3, 224, 224)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 48.\n    E.g.  (200, 200, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_2",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_2",
            "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "title": "References"
        },
        {
            "location": "/applications/#license_2",
            "text": "These weights are ported from the ones  released by VGG at Oxford  under the  Creative Commons Attribution License .",
            "title": "License"
        },
        {
            "location": "/applications/#resnet50",
            "text": "keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  ResNet50 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith  'channels_first'  data format (channels, height, width) or  'channels_last'  data format (height, width, channels).  The default input size for this model is 224x224.",
            "title": "ResNet50"
        },
        {
            "location": "/applications/#arguments_3",
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (224, 224, 3)  (with  'channels_last'  data format)\n    or  (3, 224, 224)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 197.\n    E.g.  (200, 200, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_3",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_3",
            "text": "Deep Residual Learning for Image Recognition",
            "title": "References"
        },
        {
            "location": "/applications/#license_3",
            "text": "These weights are ported from the ones  released by Kaiming He  under the  MIT license .",
            "title": "License"
        },
        {
            "location": "/applications/#inceptionv3",
            "text": "keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  Inception V3 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith  'channels_first'  data format (channels, height, width) or  'channels_last'  data format (height, width, channels).  The default input size for this model is 299x299.",
            "title": "InceptionV3"
        },
        {
            "location": "/applications/#arguments_4",
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (299, 299, 3)  (with  'channels_last'  data format)\n    or  (3, 299, 299)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 139.\n    E.g.  (150, 150, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_4",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_4",
            "text": "Rethinking the Inception Architecture for Computer Vision",
            "title": "References"
        },
        {
            "location": "/applications/#license_4",
            "text": "These weights are released under  the Apache License .",
            "title": "License"
        },
        {
            "location": "/applications/#inceptionresnetv2",
            "text": "keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  Inception-ResNet V2 model, with weights pre-trained on ImageNet.  This model is available for Theano, TensorFlow and CNTK backends, and can be built both\nwith  'channels_first'  data format (channels, height, width) or  'channels_last'  data format (height, width, channels).  The default input size for this model is 299x299.",
            "title": "InceptionResNetV2"
        },
        {
            "location": "/applications/#arguments_5",
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or  'imagenet'  (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (299, 299, 3)  (with  'channels_last'  data format)\n    or  (3, 299, 299)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 139.\n    E.g.  (150, 150, 3)  would be one valid value.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images \n    into, only to be specified if  include_top  is  True , and \n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_5",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_5",
            "text": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "title": "References"
        },
        {
            "location": "/applications/#license_5",
            "text": "These weights are released under  the Apache License .",
            "title": "License"
        },
        {
            "location": "/applications/#mobilenet",
            "text": "keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)  MobileNet model, with weights pre-trained on ImageNet.  Note that only TensorFlow is supported for now,\ntherefore it only works with the data format image_data_format='channels_last'  in your Keras config at  ~/.keras/keras.json .\nTo load a MobileNet model via  load_model , import the custom objects  relu6  and  DepthwiseConv2D  and pass them to the  custom_objects  parameter.  E.g.  model = load_model('mobilenet.h5', custom_objects={\n                   'relu6': mobilenet.relu6,\n                   'DepthwiseConv2D': mobilenet.DepthwiseConv2D})  The default input size for this model is 224x224.",
            "title": "MobileNet"
        },
        {
            "location": "/applications/#arguments_6",
            "text": "input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (224, 224, 3)  (with  'channels_last'  data format)\n    or  (3, 224, 224)  (with  'channels_first'  data format).\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 32.\n    E.g.  (200, 200, 3)  would be one valid value.  alpha: controls the width of the network.  If  alpha  < 1.0, proportionally decreases the number\n    of filters in each layer.  If  alpha  > 1.0, proportionally increases the number\n    of filters in each layer.  If  alpha  = 1, default number of filters from the paper\n    are used at each layer.    depth_multiplier: depth multiplier for depthwise convolution\n    (also called the resolution multiplier)  dropout: dropout rate  include_top: whether to include the fully-connected\n    layer at the top of the network.  weights:  None  (random initialization) or\n     'imagenet'  (ImageNet weights)  input_tensor: optional Keras tensor (i.e. output of\n     layers.Input() )\n    to use as image input for the model.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model\nwill be the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a\n    2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images\n    into, only to be specified if  include_top  is  True , and\n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_6",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_6",
            "text": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "title": "References"
        },
        {
            "location": "/applications/#license_6",
            "text": "These weights are released under  the Apache License .",
            "title": "License"
        },
        {
            "location": "/applications/#densenet",
            "text": "keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\nkeras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\nkeras.applications.densenet.DenseNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)  Optionally loads weights pre-trained\non ImageNet. Note that when using TensorFlow,\nfor best performance you should set image_data_format='channels_last'  in your Keras config\nat ~/.keras/keras.json.  The model and the weights are compatible with\nTensorFlow, Theano, and CNTK. The data format\nconvention used by the model is the one\nspecified in your Keras config file.",
            "title": "DenseNet"
        },
        {
            "location": "/applications/#arguments_7",
            "text": "blocks: numbers of building blocks for the four dense layers.  include_top: whether to include the fully-connected\n    layer at the top of the network.  weights: one of  None  (random initialization),\n    'imagenet' (pre-training on ImageNet),\n    or the path to the weights file to be loaded.  input_tensor: optional Keras tensor (i.e. output of  layers.Input() )\n    to use as image input for the model.  input_shape: optional shape tuple, only to be specified\n    if  include_top  is False (otherwise the input shape\n    has to be  (224, 224, 3)  (with  channels_last  data format)\n    or  (3, 224, 224)  (with  channels_first  data format).\n    It should have exactly 3 inputs channels.  pooling: optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model will be\n    the 4D tensor output of the\n    last convolutional layer.  avg  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a 2D tensor.  max  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images\n    into, only to be specified if  include_top  is True, and\n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_7",
            "text": "A Keras model instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_7",
            "text": "Densely Connected Convolutional Networks  (CVPR 2017 Best Paper Award)",
            "title": "References"
        },
        {
            "location": "/applications/#license_7",
            "text": "These weights are released under  the BSD 3-clause License .",
            "title": "License"
        },
        {
            "location": "/applications/#nasnet",
            "text": "keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\nkeras.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)  Neural Architecture Search Network (NASNet) model, with weights pre-trained on ImageNet.  Note that only TensorFlow is supported for now,\ntherefore it only works with the data format image_data_format='channels_last'  in your Keras config at  ~/.keras/keras.json .  The default input size for the NASNetLarge model is 331x331 and for the\nNASNetMobile model is 224x224.",
            "title": "NASNet"
        },
        {
            "location": "/applications/#arguments_8",
            "text": "input_shape: optional shape tuple, only to be specified\n    if  include_top  is  False  (otherwise the input shape\n    has to be  (224, 224, 3)  (with  'channels_last'  data format)\n    or  (3, 224, 224)  (with  'channels_first'  data format)\n    for NASNetMobile or  (331, 331, 3)  (with  'channels_last' \n    data format) or  (3, 331, 331)  (with  'channels_first' \n    data format) for NASNetLarge.\n    It should have exactly 3 inputs channels,\n    and width and height should be no smaller than 32.\n    E.g.  (200, 200, 3)  would be one valid value.  include_top: whether to include the fully-connected\n    layer at the top of the network.  weights:  None  (random initialization) or\n     'imagenet'  (ImageNet weights)  input_tensor: optional Keras tensor (i.e. output of\n     layers.Input() )\n    to use as image input for the model.  pooling: Optional pooling mode for feature extraction\n    when  include_top  is  False .  None  means that the output of the model\nwill be the 4D tensor output of the\n    last convolutional layer.  'avg'  means that global average pooling\n    will be applied to the output of the\n    last convolutional layer, and thus\n    the output of the model will be a\n    2D tensor.  'max'  means that global max pooling will\n    be applied.    classes: optional number of classes to classify images\n    into, only to be specified if  include_top  is  True , and\n    if no  weights  argument is specified.",
            "title": "Arguments"
        },
        {
            "location": "/applications/#returns_8",
            "text": "A Keras  Model  instance.",
            "title": "Returns"
        },
        {
            "location": "/applications/#references_8",
            "text": "Learning Transferable Architectures for Scalable Image Recognition",
            "title": "References"
        },
        {
            "location": "/applications/#license_8",
            "text": "These weights are released under  the Apache License .",
            "title": "License"
        },
        {
            "location": "/backend/",
            "text": "Keras backends\n\n\nWhat is a \"backend\"?\n\n\nKeras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n\n\nAt this time, Keras has three backend implementations available: the \nTensorFlow\n backend, the \nTheano\n backend, and the \nCNTK\n backend.\n\n\n\n\nTensorFlow\n is an open-source symbolic tensor manipulation framework developed by Google.\n\n\nTheano\n is an open-source symbolic tensor manipulation framework developed by LISA Lab at Universit\u00e9 de Montr\u00e9al.\n\n\nCNTK\n is an open-source toolkit for deep learning developed by Microsoft.\n\n\n\n\nIn the future, we are likely to add more backend options.\n\n\n\n\nSwitching from one backend to another\n\n\nIf you have run Keras at least once, you will find the Keras configuration file at:\n\n\n$HOME/.keras/keras.json\n\n\nIf it isn't there, you can create it.\n\n\nNOTE for Windows Users:\n Please replace \n$HOME\n with \n%USERPROFILE%\n.\n\n\nThe default configuration file looks like this:\n\n\n{\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}\n\n\n\n\nSimply change the field \nbackend\n to \n\"theano\"\n, \n\"tensorflow\"\n, or \n\"cntk\"\n, and Keras will use the new configuration next time you run any Keras code.\n\n\nYou can also define the environment variable \nKERAS_BACKEND\n and this will\noverride what is defined in your config file :\n\n\nKERAS_BACKEND=tensorflow python -c \"from keras import backend\"\nUsing TensorFlow backend.\n\n\n\n\n\n\nkeras.json details\n\n\nThe \nkeras.json\n configuration file contains the following settings:\n\n\n{\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}\n\n\n\n\nYou can change these settings by editing \n$HOME/.keras/keras.json\n. \n\n\n\n\nimage_data_format\n: String, either \n\"channels_last\"\n or \n\"channels_first\"\n. It specifies which data format convention Keras will follow. (\nkeras.backend.image_data_format()\n returns it.)\n\n\nFor 2D data (e.g. image), \n\"channels_last\"\n assumes \n(rows, cols, channels)\n while \n\"channels_first\"\n assumes \n(channels, rows, cols)\n. \n\n\nFor 3D data, \n\"channels_last\"\n assumes \n(conv_dim1, conv_dim2, conv_dim3, channels)\n while \n\"channels_first\"\n assumes \n(channels, conv_dim1, conv_dim2, conv_dim3)\n.\n\n\nepsilon\n: Float, a numeric fuzzing constant used to avoid dividing by zero in some operations.\n\n\nfloatx\n: String, \n\"float16\"\n, \n\"float32\"\n, or \n\"float64\"\n. Default float precision.\n\n\nbackend\n: String, \n\"tensorflow\"\n, \n\"theano\"\n, or \n\"cntk\"\n.\n\n\n\n\n\n\nUsing the abstract Keras backend to write new code\n\n\nIf you want the Keras modules you write to be compatible with both Theano (\nth\n) and TensorFlow (\ntf\n), you have to write them via the abstract Keras backend API. Here's an intro.\n\n\nYou can import the backend module via:\n\n\nfrom keras import backend as K\n\n\n\n\nThe code below instantiates an input placeholder. It's equivalent to \ntf.placeholder()\n or \nth.tensor.matrix()\n, \nth.tensor.tensor3()\n, etc.\n\n\ninputs = K.placeholder(shape=(2, 4, 5))\n# also works:\ninputs = K.placeholder(shape=(None, 4, 5))\n# also works:\ninputs = K.placeholder(ndim=3)\n\n\n\n\nThe code below instantiates a variable. It's equivalent to \ntf.Variable()\n or \nth.shared()\n.\n\n\nimport numpy as np\nval = np.random.random((3, 4, 5))\nvar = K.variable(value=val)\n\n# all-zeros variable:\nvar = K.zeros(shape=(3, 4, 5))\n# all-ones:\nvar = K.ones(shape=(3, 4, 5))\n\n\n\n\nMost tensor operations you will need can be done as you would in TensorFlow or Theano:\n\n\n# Initializing Tensors with Random Numbers\nb = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # Uniform distribution\nc = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Gaussian distribution\nd = K.random_normal_variable(shape=(3, 4), mean=0, scale=1)\n\n# Tensor Arithmetic\na = b + c * K.abs(d)\nc = K.dot(a, K.transpose(b))\na = K.sum(b, axis=1)\na = K.softmax(b)\na = K.concatenate([b, c], axis=-1)\n# etc...\n\n\n\n\n\n\nBackend functions\n\n\nbackend\n\n\nbackend.backend()\n\n\n\n\nPublicly accessible method\nfor determining the current backend.\n\n\nReturns\n\n\nString, the name of the backend Keras is currently using.\n\n\nExample\n\n\n>>> keras.backend.backend()\n'tensorflow'\n\n\n\n\n\n\nepsilon\n\n\nkeras.backend.epsilon()\n\n\n\n\nReturns the value of the fuzz factor used in numeric expressions.\n\n\nReturns\n\n\nA float.\n\n\nExample\n\n\n>>> keras.backend.epsilon()\n1e-07\n\n\n\n\n\n\nset_epsilon\n\n\nkeras.backend.set_epsilon(e)\n\n\n\n\nSets the value of the fuzz factor used in numeric expressions.\n\n\nArguments\n\n\n\n\ne\n: float. New value of epsilon.\n\n\n\n\nExample\n\n\n>>> from keras import backend as K\n>>> K.epsilon()\n1e-07\n>>> K.set_epsilon(1e-05)\n>>> K.epsilon()\n1e-05\n\n\n\n\n\n\nfloatx\n\n\nkeras.backend.floatx()\n\n\n\n\nReturns the default float type, as a string.\n(e.g. 'float16', 'float32', 'float64').\n\n\nReturns\n\n\nString, the current default float type.\n\n\nExample\n\n\n>>> keras.backend.floatx()\n'float32'\n\n\n\n\n\n\nset_floatx\n\n\nkeras.backend.set_floatx(floatx)\n\n\n\n\nSets the default float type.\n\n\nArguments\n\n\n\n\nfloatx\n: String, 'float16', 'float32', or 'float64'.\n\n\n\n\nExample\n\n\n>>> from keras import backend as K\n>>> K.floatx()\n'float32'\n>>> K.set_floatx('float16')\n>>> K.floatx()\n'float16'\n\n\n\n\n\n\ncast_to_floatx\n\n\nkeras.backend.cast_to_floatx(x)\n\n\n\n\nCast a Numpy array to the default Keras float type.\n\n\nArguments\n\n\n\n\nx\n: Numpy array.\n\n\n\n\nReturns\n\n\nThe same Numpy array, cast to its new type.\n\n\nExample\n\n\n>>> from keras import backend as K\n>>> K.floatx()\n'float32'\n>>> arr = numpy.array([1.0, 2.0], dtype='float64')\n>>> arr.dtype\ndtype('float64')\n>>> new_arr = K.cast_to_floatx(arr)\n>>> new_arr\narray([ 1.,  2.], dtype=float32)\n>>> new_arr.dtype\ndtype('float32')\n\n\n\n\n\n\nimage_data_format\n\n\nkeras.backend.image_data_format()\n\n\n\n\nReturns the default image data format convention ('channels_first' or 'channels_last').\n\n\nReturns\n\n\nA string, either \n'channels_first'\n or \n'channels_last'\n\n\nExample\n\n\n>>> keras.backend.image_data_format()\n'channels_first'\n\n\n\n\n\n\nset_image_data_format\n\n\nkeras.backend.set_image_data_format(data_format)\n\n\n\n\nSets the value of the data format convention.\n\n\nArguments\n\n\n\n\ndata_format\n: string. \n'channels_first'\n or \n'channels_last'\n.\n\n\n\n\nExample\n\n\n>>> from keras import backend as K\n>>> K.image_data_format()\n'channels_first'\n>>> K.set_image_data_format('channels_last')\n>>> K.image_data_format()\n'channels_last'\n\n\n\n\n\n\nlearning_phase\n\n\nkeras.backend.learning_phase()\n\n\n\n\n\n\nset_learning_phase\n\n\nkeras.backend.set_learning_phase(value)\n\n\n\n\n\n\nget_uid\n\n\nkeras.backend.get_uid(prefix='')\n\n\n\n\nProvides a unique UID given a string prefix.\n\n\nArguments\n\n\n\n\nprefix\n: string.\n\n\n\n\nReturns\n\n\nAn integer.\n\n\nExample\n\n\n>>> keras.backend.get_uid('dense')\n1\n>>> keras.backend.get_uid('dense')\n2\n\n\n\n\n\n\nreset_uids\n\n\nkeras.backend.reset_uids()\n\n\n\n\n\n\nis_sparse\n\n\nkeras.backend.is_sparse(tensor)\n\n\n\n\n\n\nto_dense\n\n\nkeras.backend.to_dense(tensor)\n\n\n\n\n\n\nname_scope\n\n\nkeras.backend.name_scope()\n\n\n\n\n\n\nvariable\n\n\nkeras.backend.variable(value, dtype=None, name=None, constraint=None)\n\n\n\n\nInstantiates a variable and returns it.\n\n\nArguments\n\n\n\n\nvalue\n: Numpy array, initial value of the tensor.\n\n\ndtype\n: Tensor type.\n\n\nname\n: Optional name string for the tensor.\n\n\nconstraint\n: Optional projection function to be\napplied to the variable after an optimizer update.\n\n\n\n\nReturns\n\n\nA variable instance (with Keras metadata included).\n\n\n\n\nconstant\n\n\nkeras.backend.constant(value, dtype=None, shape=None, name=None)\n\n\n\n\n\n\nis_keras_tensor\n\n\nkeras.backend.is_keras_tensor(x)\n\n\n\n\nReturns whether \nx\n is a Keras tensor.\n\n\nA \"Keras tensor\" is a tensor that was returned by a Keras layer,\n(\nLayer\n class) or by \nInput\n.\n\n\nArguments\n\n\n\n\nx\n: A candidate tensor.\n\n\n\n\nReturns\n\n\nA boolean: Whether the argument is a Keras tensor.\n\n\nRaises\n\n\n\n\nValueError\n: In case \nx\n is not a symbolic tensor.\n\n\n\n\nExamples\n\n\n>>> from keras import backend as K\n>>> from keras.layers import Input, Dense\n>>> np_var = numpy.array([1, 2])\n>>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\nValueError\n>>> k_var = tf.placeholder('float32', shape=(1,1))\n>>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\nFalse\n>>> keras_var = K.variable(np_var)\n>>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\nFalse\n>>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n>>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\nFalse\n>>> keras_input = Input([10])\n>>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\nTrue\n>>> keras_layer_output = Dense(10)(keras_input)\n>>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\nTrue\n\n\n\n\n\n\nplaceholder\n\n\nkeras.backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)\n\n\n\n\nInstantiate an input data placeholder variable.\n\n\n\n\nis_placeholder\n\n\nkeras.backend.is_placeholder(x)\n\n\n\n\nReturns whether \nx\n is a placeholder.\n\n\nArguments\n\n\n\n\nx\n: A candidate placeholder.\n\n\n\n\nReturns\n\n\nBoolean.\n\n\n\n\nshape\n\n\nkeras.backend.shape(x)\n\n\n\n\nReturns the shape of a tensor.\n\n\n\n\nWarning\n: type returned will be different for\nTheano backend (Theano tensor type) and TF backend (TF TensorShape).\n\n\n\n\n\n\nint_shape\n\n\nkeras.backend.int_shape(x)\n\n\n\n\nReturns the shape of a Keras tensor or a Keras variable as a tuple of\nintegers or None entries.\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nA tuple of integers (or None entries).\n\n\n\n\nndim\n\n\nkeras.backend.ndim(x)\n\n\n\n\n\n\ndtype\n\n\nkeras.backend.dtype(x)\n\n\n\n\n\n\neval\n\n\nkeras.backend.eval(x)\n\n\n\n\nReturns the value of a tensor.\n\n\n\n\nzeros\n\n\nkeras.backend.zeros(shape, dtype=None, name=None)\n\n\n\n\nInstantiates an all-zeros variable.\n\n\n\n\nones\n\n\nkeras.backend.ones(shape, dtype=None, name=None)\n\n\n\n\nInstantiates an all-ones variable.\n\n\n\n\neye\n\n\nkeras.backend.eye(size, dtype=None, name=None)\n\n\n\n\nInstantiates an identity matrix.\n\n\n\n\nones_like\n\n\nkeras.backend.ones_like(x, dtype=None, name=None)\n\n\n\n\n\n\nzeros_like\n\n\nkeras.backend.zeros_like(x, dtype=None, name=None)\n\n\n\n\n\n\nidentity\n\n\nkeras.backend.identity(x, name=None)\n\n\n\n\nReturns a tensor with the same content as the input tensor.\n\n\nArguments\n\n\n\n\nx\n: The input tensor.\n\n\nname\n: String, name for the variable to create.\n\n\n\n\nReturns\n\n\nA tensor of the same shape, type and content.\n\n\n\n\nrandom_uniform_variable\n\n\nkeras.backend.random_uniform_variable(shape, low, high, dtype=None, name=None)\n\n\n\n\n\n\nrandom_normal_variable\n\n\nkeras.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None)\n\n\n\n\n\n\ncount_params\n\n\nkeras.backend.count_params(x)\n\n\n\n\nReturns the number of scalars in a tensor.\n\n\n\n\nReturn\n: numpy integer.\n\n\n\n\n\n\ncast\n\n\nkeras.backend.cast(x, dtype)\n\n\n\n\n\n\nupdate\n\n\nkeras.backend.update(x, new_x)\n\n\n\n\n\n\nupdate_add\n\n\nkeras.backend.update_add(x, increment)\n\n\n\n\n\n\nupdate_sub\n\n\nkeras.backend.update_sub(x, decrement)\n\n\n\n\n\n\nmoving_average_update\n\n\nkeras.backend.moving_average_update(variable, value, momentum)\n\n\n\n\n\n\ndot\n\n\nkeras.backend.dot(x, y)\n\n\n\n\n\n\nbatch_dot\n\n\nkeras.backend.batch_dot(x, y, axes=None)\n\n\n\n\nBatchwise dot product.\n\n\nbatch_dot results in a tensor with less dimensions than the input.\nIf the number of dimensions is reduced to 1, we use \nexpand_dims\n to\nmake sure that ndim is at least 2.\n\n\nArguments\n\n\nx, y: tensors with ndim >= 2\n- \naxes\n: list (or single) int with target dimensions\n\n\nReturns\n\n\nA tensor with shape equal to the concatenation of x's shape\n(less the dimension that was summed over) and y's shape\n(less the batch dimension and the dimension that was summed over).\nIf the final rank is 1, we reshape it to (batch_size, 1).\n\n\nExamples\n\n\nAssume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\nbatch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\nof x.dot(y.T), although we never have to calculate the off-diagonal\nelements.\n\n\nShape inference:\nLet x's shape be (100, 20) and y's shape be (100, 30, 20).\nIf dot_axes is (1, 2), to find the output shape of resultant tensor,\nloop through each dimension in x's shape and y's shape:\nx.shape[0] : 100 : append to output shape\nx.shape[1] : 20 : do not append to output shape,\ndimension 1 of x has been summed over. (dot_axes[0] = 1)\ny.shape[0] : 100 : do not append to output shape,\nalways ignore first dimension of y\ny.shape[1] : 30 : append to output shape\ny.shape[2] : 20 : do not append to output shape,\ndimension 2 of y has been summed over. (dot_axes[1] = 2)\n\n\noutput_shape = (100, 30)\n\n\n\n\ntranspose\n\n\nkeras.backend.transpose(x)\n\n\n\n\n\n\ngather\n\n\nkeras.backend.gather(reference, indices)\n\n\n\n\nRetrieves the elements of indices \nindices\n in the tensor \nreference\n.\n\n\nArguments\n\n\n\n\nreference\n: A tensor.\n\n\nindices\n: An integer tensor of indices.\n\n\n\n\nReturns\n\n\nA tensor of same type as \nreference\n.\n\n\n\n\nmax\n\n\nkeras.backend.max(x, axis=None, keepdims=False)\n\n\n\n\n\n\nmin\n\n\nkeras.backend.min(x, axis=None, keepdims=False)\n\n\n\n\n\n\nsum\n\n\nkeras.backend.sum(x, axis=None, keepdims=False)\n\n\n\n\nSum of the values in a tensor, alongside the specified axis.\n\n\n\n\nprod\n\n\nkeras.backend.prod(x, axis=None, keepdims=False)\n\n\n\n\nMultiply the values in a tensor, alongside the specified axis.\n\n\n\n\ncumsum\n\n\nkeras.backend.cumsum(x, axis=0)\n\n\n\n\nCumulative sum of the values in a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to compute the sum.\n\n\n\n\nReturns\n\n\nA tensor of the cumulative sum of values of \nx\n along \naxis\n.\n\n\n\n\ncumprod\n\n\nkeras.backend.cumprod(x, axis=0)\n\n\n\n\nCumulative product of the values in a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to compute the product.\n\n\n\n\nReturns\n\n\nA tensor of the cumulative product of values of \nx\n along \naxis\n.\n\n\n\n\nmean\n\n\nkeras.backend.mean(x, axis=None, keepdims=False)\n\n\n\n\nMean of a tensor, alongside the specified axis.\n\n\n\n\nstd\n\n\nkeras.backend.std(x, axis=None, keepdims=False)\n\n\n\n\n\n\nvar\n\n\nkeras.backend.var(x, axis=None, keepdims=False)\n\n\n\n\n\n\nany\n\n\nkeras.backend.any(x, axis=None, keepdims=False)\n\n\n\n\nBitwise reduction (logical OR).\n\n\n\n\nall\n\n\nkeras.backend.all(x, axis=None, keepdims=False)\n\n\n\n\nBitwise reduction (logical AND).\n\n\n\n\nargmax\n\n\nkeras.backend.argmax(x, axis=-1)\n\n\n\n\n\n\nargmin\n\n\nkeras.backend.argmin(x, axis=-1)\n\n\n\n\n\n\nsquare\n\n\nkeras.backend.square(x)\n\n\n\n\n\n\nabs\n\n\nkeras.backend.abs(x)\n\n\n\n\n\n\nsqrt\n\n\nkeras.backend.sqrt(x)\n\n\n\n\n\n\nexp\n\n\nkeras.backend.exp(x)\n\n\n\n\n\n\nlog\n\n\nkeras.backend.log(x)\n\n\n\n\n\n\nlogsumexp\n\n\nkeras.backend.logsumexp(x, axis=None, keepdims=False)\n\n\n\n\nComputes log(sum(exp(elements across dimensions of a tensor))).\n\n\nThis function is more numerically stable than log(sum(exp(x))).\nIt avoids overflows caused by taking the exp of large inputs and\nunderflows caused by taking the log of small inputs.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to reduce over.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n, the reduced dimension is\nretained with length 1.\n\n\n\n\nReturns\n\n\nThe reduced tensor.\n\n\n\n\nround\n\n\nkeras.backend.round(x)\n\n\n\n\n\n\nsign\n\n\nkeras.backend.sign(x)\n\n\n\n\n\n\npow\n\n\nkeras.backend.pow(x, a)\n\n\n\n\n\n\nclip\n\n\nkeras.backend.clip(x, min_value, max_value)\n\n\n\n\n\n\nequal\n\n\nkeras.backend.equal(x, y)\n\n\n\n\n\n\nnot_equal\n\n\nkeras.backend.not_equal(x, y)\n\n\n\n\n\n\ngreater\n\n\nkeras.backend.greater(x, y)\n\n\n\n\n\n\ngreater_equal\n\n\nkeras.backend.greater_equal(x, y)\n\n\n\n\n\n\nless\n\n\nkeras.backend.less(x, y)\n\n\n\n\n\n\nless_equal\n\n\nkeras.backend.less_equal(x, y)\n\n\n\n\n\n\nmaximum\n\n\nkeras.backend.maximum(x, y)\n\n\n\n\n\n\nminimum\n\n\nkeras.backend.minimum(x, y)\n\n\n\n\n\n\nsin\n\n\nkeras.backend.sin(x)\n\n\n\n\n\n\ncos\n\n\nkeras.backend.cos(x)\n\n\n\n\n\n\nnormalize_batch_in_training\n\n\nkeras.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)\n\n\n\n\nComputes mean and std for batch then apply batch_normalization on batch.\n\n\n\n\nbatch_normalization\n\n\nkeras.backend.batch_normalization(x, mean, var, beta, gamma, epsilon=0.001)\n\n\n\n\nApply batch normalization on x given mean, var, beta and gamma.\n\n\n\n\nconcatenate\n\n\nkeras.backend.concatenate(tensors, axis=-1)\n\n\n\n\n\n\nreshape\n\n\nkeras.backend.reshape(x, shape)\n\n\n\n\n\n\npermute_dimensions\n\n\nkeras.backend.permute_dimensions(x, pattern)\n\n\n\n\nTranspose dimensions.\n\n\npattern should be a tuple or list of\ndimension indices, e.g. [0, 2, 1].\n\n\n\n\nrepeat_elements\n\n\nkeras.backend.repeat_elements(x, rep, axis)\n\n\n\n\nRepeat the elements of a tensor along an axis, like np.repeat.\n\n\nIf x has shape (s1, s2, s3) and axis=1, the output\nwill have shape (s1, s2 * rep, s3).\n\n\n\n\nresize_images\n\n\nkeras.backend.resize_images(x, height_factor, width_factor, data_format)\n\n\n\n\nResize the images contained in a 4D tensor of shape\n- [batch, channels, height, width] (for 'channels_first' data_format)\n- [batch, height, width, channels] (for 'channels_last' data_format)\nby a factor of (height_factor, width_factor). Both factors should be\npositive integers.\n\n\n\n\nresize_volumes\n\n\nkeras.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format)\n\n\n\n\nResize the volume contained in a 5D tensor of shape\n- [batch, channels, depth, height, width] (for 'channels_first' data_format)\n- [batch, depth, height, width, channels] (for 'channels_last' data_format)\nby a factor of (depth_factor, height_factor, width_factor).\nBoth factors should be positive integers.\n\n\n\n\nrepeat\n\n\nkeras.backend.repeat(x, n)\n\n\n\n\nRepeat a 2D tensor.\n\n\nIf x has shape (samples, dim) and n=2,\nthe output will have shape (samples, 2, dim).\n\n\n\n\narange\n\n\nkeras.backend.arange(start, stop=None, step=1, dtype='int32')\n\n\n\n\nCreates a 1-D tensor containing a sequence of integers.\n\n\nThe function arguments use the same convention as\nTheano's arange: if only one argument is provided,\nit is in fact the \"stop\" argument.\n\n\nThe default type of the returned tensor is 'int32' to\nmatch TensorFlow's default.\n\n\n\n\ntile\n\n\nkeras.backend.tile(x, n)\n\n\n\n\n\n\nflatten\n\n\nkeras.backend.flatten(x)\n\n\n\n\n\n\nbatch_flatten\n\n\nkeras.backend.batch_flatten(x)\n\n\n\n\nTurn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n\n\n\n\nexpand_dims\n\n\nkeras.backend.expand_dims(x, axis=-1)\n\n\n\n\nAdd a 1-sized dimension at index \"dim\".\n\n\n\n\nsqueeze\n\n\nkeras.backend.squeeze(x, axis)\n\n\n\n\nRemove a 1-dimension from the tensor at index \"axis\".\n\n\n\n\ntemporal_padding\n\n\nkeras.backend.temporal_padding(x, padding=(1, 1))\n\n\n\n\nPad the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.\n\n\nApologies for the inane API, but Theano makes this\nreally hard.\n\n\n\n\nspatial_2d_padding\n\n\nkeras.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)\n\n\n\n\nPad the 2nd and 3rd dimensions of a 4D tensor\nwith \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n\n\n\n\nspatial_3d_padding\n\n\nkeras.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)\n\n\n\n\nPad the 2nd, 3rd and 4th dimensions of a 5D tensor\nwith \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.\n\n\n\n\nstack\n\n\nkeras.backend.stack(x, axis=0)\n\n\n\n\n\n\none_hot\n\n\nkeras.backend.one_hot(indices, num_classes)\n\n\n\n\nInput: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n- \nOutput\n: (n + 1)D one hot representation of the input\nwith shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)\n\n\n\n\nreverse\n\n\nkeras.backend.reverse(x, axes)\n\n\n\n\nReverse a tensor along the specified axes\n\n\n\n\npattern_broadcast\n\n\nkeras.backend.pattern_broadcast(x, broadcastable)\n\n\n\n\n\n\nget_value\n\n\nkeras.backend.get_value(x)\n\n\n\n\n\n\nbatch_get_value\n\n\nkeras.backend.batch_get_value(xs)\n\n\n\n\nReturns the value of more than one tensor variable,\nas a list of Numpy arrays.\n\n\n\n\nset_value\n\n\nkeras.backend.set_value(x, value)\n\n\n\n\n\n\nbatch_set_value\n\n\nkeras.backend.batch_set_value(tuples)\n\n\n\n\n\n\nprint_tensor\n\n\nkeras.backend.print_tensor(x, message='')\n\n\n\n\nPrint the message and the tensor when evaluated and return the same\ntensor.\n\n\n\n\nfunction\n\n\nkeras.backend.function(inputs, outputs, updates=[])\n\n\n\n\n\n\ngradients\n\n\nkeras.backend.gradients(loss, variables)\n\n\n\n\n\n\nstop_gradient\n\n\nkeras.backend.stop_gradient(variables)\n\n\n\n\nReturns \nvariables\n but with zero gradient w.r.t. every other variable.\n\n\nArguments\n\n\n\n\nvariables\n: tensor or list of tensors to consider constant with respect\nto any other variable.\n\n\n\n\nReturns\n\n\nA single tensor or a list of tensors (depending on the passed argument)\nthat has constant gradient with respect to any other variable.\n\n\n\n\nrnn\n\n\nkeras.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)\n\n\n\n\nIterates over the time dimension of a tensor.\n\n\nArguments\n\n\n\n\ninputs\n: tensor of temporal data of shape (samples, time, ...)\n(at least 3D).\n\n\nstep_function\n:\n\n\nParameters\n:\n\n\ninputs\n: tensor with shape (samples, ...) (no time dimension),\nrepresenting input for the batch of samples at a certain\ntime step.\n\n\nstates\n: list of tensors.\n\n\nReturns\n:\n\n\noutputs\n: tensor with shape (samples, ...) (no time dimension),\n\n\nnew_states\n: list of tensors, same length and shapes\nas 'states'.\n\n\ninitial_states\n: tensor with shape (samples, ...) (no time dimension),\ncontaining the initial values for the states used in\nthe step function.\n\n\ngo_backwards\n: boolean. If True, do the iteration over the time\ndimension in reverse order and return the reversed sequence.\n\n\nmask\n: binary tensor with shape (samples, time),\nwith a zero for every element that is masked.\n\n\nconstants\n: a list of constant values passed at each step.\n\n\nunroll\n: whether to unroll the RNN or to use a symbolic loop (\nwhile_loop\n or \nscan\n depending on backend).\n\n\ninput_length\n: must be specified if using \nunroll\n.\n\n\n\n\nReturns\n\n\nA tuple (last_output, outputs, new_states).\n- \nlast_output\n: the latest output of the rnn, of shape (samples, ...)\n- \noutputs\n: tensor with shape (samples, time, ...) where each\nentry outputs[s, t] is the output of the step function\nat time t for sample s.\n- \nnew_states\n: list of tensors, latest states returned by\nthe step function, of shape (samples, ...).\n\n\n\n\nswitch\n\n\nkeras.backend.switch(condition, then_expression, else_expression)\n\n\n\n\nSwitches between two operations depending on a scalar value.\n\n\nNote that both \nthen_expression\n and \nelse_expression\n\nshould be symbolic tensors of the \nsame shape\n.\n\n\nArguments\n\n\n\n\ncondition\n: scalar tensor (\nint\n or \nbool\n).\n\n\nthen_expression\n: either a tensor, or a callable that returns a tensor.\n\n\nelse_expression\n: either a tensor, or a callable that returns a tensor.\n\n\n\n\nReturns\n\n\nThe selected tensor.\n\n\n\n\nin_train_phase\n\n\nkeras.backend.in_train_phase(x, alt, training=None)\n\n\n\n\nSelects \nx\n in train phase, and \nalt\n otherwise.\n\n\nNote that \nalt\n should have the \nsame shape\n as \nx\n.\n\n\nReturns\n\n\nEither \nx\n or \nalt\n based on the \ntraining\n flag.\nthe \ntraining\n flag defaults to \nK.learning_phase()\n.\n\n\n\n\nin_test_phase\n\n\nkeras.backend.in_test_phase(x, alt, training=None)\n\n\n\n\nSelects \nx\n in test phase, and \nalt\n otherwise.\nNote that \nalt\n should have the \nsame shape\n as \nx\n.\n\n\nReturns\n\n\nEither \nx\n or \nalt\n based on \nK.learning_phase\n.\n\n\n\n\nelu\n\n\nkeras.backend.elu(x, alpha=1.0)\n\n\n\n\nExponential linear unit\n\n\nArguments\n\n\n\n\nx\n: Tensor to compute the activation function for.\n\n\nalpha\n: scalar\n\n\n\n\n\n\nrelu\n\n\nkeras.backend.relu(x, alpha=0.0, max_value=None)\n\n\n\n\n\n\nsoftmax\n\n\nkeras.backend.softmax(x)\n\n\n\n\n\n\nsoftplus\n\n\nkeras.backend.softplus(x)\n\n\n\n\n\n\nsoftsign\n\n\nkeras.backend.softsign(x)\n\n\n\n\n\n\ncategorical_crossentropy\n\n\nkeras.backend.categorical_crossentropy(target, output, from_logits=False)\n\n\n\n\n\n\nsparse_categorical_crossentropy\n\n\nkeras.backend.sparse_categorical_crossentropy(target, output, from_logits=False)\n\n\n\n\n\n\nbinary_crossentropy\n\n\nkeras.backend.binary_crossentropy(target, output, from_logits=False)\n\n\n\n\n\n\nsigmoid\n\n\nkeras.backend.sigmoid(x)\n\n\n\n\n\n\nhard_sigmoid\n\n\nkeras.backend.hard_sigmoid(x)\n\n\n\n\n\n\ntanh\n\n\nkeras.backend.tanh(x)\n\n\n\n\n\n\ndropout\n\n\nkeras.backend.dropout(x, level, noise_shape=None, seed=None)\n\n\n\n\nSets entries in \nx\n to zero at random,\nwhile scaling the entire tensor.\n\n\nArguments\n\n\n\n\nx\n: tensor\n\n\nlevel\n: fraction of the entries in the tensor\nthat will be set to 0.\n\n\nnoise_shape\n: shape for randomly generated keep/drop flags,\nmust be broadcastable to the shape of \nx\n\n\nseed\n: random seed to ensure determinism.\n\n\n\n\n\n\nl2_normalize\n\n\nkeras.backend.l2_normalize(x, axis=None)\n\n\n\n\n\n\nin_top_k\n\n\nkeras.backend.in_top_k(predictions, targets, k)\n\n\n\n\nReturns whether the \ntargets\n are in the top \nk\n \npredictions\n.\n\n\nArguments\n\n\n\n\npredictions\n: A tensor of shape \n(batch_size, classes)\n and type \nfloat32\n.\n\n\ntargets\n: A 1D tensor of length \nbatch_size\n and type \nint32\n or \nint64\n.\n\n\nk\n: An \nint\n, number of top elements to consider.\n\n\n\n\nReturns\n\n\nA 1D tensor of length \nbatch_size\n and type \nbool\n.\n\noutput[i]\n is \nTrue\n if \npredictions[i, targets[i]]\n is within top-\nk\n\nvalues of \npredictions[i]\n.\n\n\n\n\nconv1d\n\n\nkeras.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)\n\n\n\n\n1D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: stride integer.\n\n\npadding\n: string, \n\"same\"\n, \n\"causal\"\n or \n\"valid\"\n.\n\n\ndata_format\n: string, one of \"channels_last\", \"channels_first\"\n\n\ndilation_rate\n: integer.\n\n\n\n\n\n\nconv2d\n\n\nkeras.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))\n\n\n\n\n2D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: strides tuple.\n\n\npadding\n: string, \"same\" or \"valid\".\n\n\ndata_format\n: \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.\n\n\n\n\n\n\nconv2d_transpose\n\n\nkeras.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None)\n\n\n\n\n2D deconvolution (transposed convolution).\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\noutput_shape\n: desired dimensions of output.\n\n\nstrides\n: strides tuple.\n\n\npadding\n: string, \"same\" or \"valid\".\n\n\ndata_format\n: \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: if using an even kernel size with padding 'same'.\n\n\n\n\n\n\nseparable_conv1d\n\n\nkeras.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)\n\n\n\n\n\n\nseparable_conv2d\n\n\nkeras.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))\n\n\n\n\n\n\ndepthwise_conv2d\n\n\nkeras.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))\n\n\n\n\n2D convolution with separable filters.\n\n\nArguments\n\n\n\n\nx\n: input tensor\n\n\ndepthwise_kernel\n: convolution kernel for the depthwise convolution.\n\n\nstrides\n: strides tuple (length 2).\n\n\npadding\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\ndata_format\n: string, \n\"channels_last\"\n or \n\"channels_first\"\n.\n\n\ndilation_rate\n: tuple of integers,\ndilation rates for the separable convolution.\n\n\n\n\nReturns\n\n\nOutput tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndata_format\n is neither \n\"channels_last\"\n or \n\"channels_first\"\n.\n\n\n\n\n\n\nconv3d\n\n\nkeras.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))\n\n\n\n\n3D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: strides tuple.\n\n\npadding\n: string, \"same\" or \"valid\".\n\n\ndata_format\n: \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.\n\n\n\n\n\n\nconv3d_transpose\n\n\nkeras.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)\n\n\n\n\n3D deconvolution (transposed convolution).\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\noutput_shape\n: desired dimensions of output.\n\n\nstrides\n: strides tuple.\n\n\npadding\n: string, \"same\" or \"valid\".\n\n\ndata_format\n: \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: if using an even kernel size with padding 'same'.\n\n\n\n\n\n\npool2d\n\n\nkeras.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')\n\n\n\n\n\n\npool3d\n\n\nkeras.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')\n\n\n\n\n\n\nbias_add\n\n\nkeras.backend.bias_add(x, bias, data_format=None)\n\n\n\n\n\n\nrandom_normal\n\n\nkeras.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)\n\n\n\n\n\n\nrandom_uniform\n\n\nkeras.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)\n\n\n\n\n\n\nrandom_binomial\n\n\nkeras.backend.random_binomial(shape, p=0.0, dtype=None, seed=None)\n\n\n\n\n\n\ntruncated_normal\n\n\nkeras.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)\n\n\n\n\n\n\nctc_interleave_blanks\n\n\nkeras.backend.ctc_interleave_blanks(Y)\n\n\n\n\n\n\nctc_create_skip_idxs\n\n\nkeras.backend.ctc_create_skip_idxs(Y)\n\n\n\n\n\n\nctc_update_log_p\n\n\nkeras.backend.ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev)\n\n\n\n\n\n\nctc_path_probs\n\n\nkeras.backend.ctc_path_probs(predict, Y, alpha=0.0001)\n\n\n\n\n\n\nctc_cost\n\n\nkeras.backend.ctc_cost(predict, Y)\n\n\n\n\n\n\nctc_batch_cost\n\n\nkeras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n\n\n\n\nRuns CTC loss algorithm on each batch element.\n\n\nArguments\n\n\n\n\ny_true\n: tensor (samples, max_string_length) containing the truth labels\n\n\ny_pred\n: tensor (samples, time_steps, num_categories) containing the prediction,\nor output of the softmax\n\n\ninput_length\n: tensor (samples,1) containing the sequence length for\neach batch item in y_pred\n\n\nlabel_length\n: tensor (samples,1) containing the sequence length for\neach batch item in y_true\n\n\n\n\nReturns\n\n\nTensor with shape (samples,1) containing the\nCTC loss of each element\n\n\n\n\nmap_fn\n\n\nkeras.backend.map_fn(fn, elems, name=None, dtype=None)\n\n\n\n\nMap the function fn over the elements elems and return the outputs.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems\n\n\nelems\n: tensor, at least 2 dimensional\n\n\nname\n: A string name for the map node in the graph\n\n\n\n\nReturns\n\n\nTensor with first dimension equal to the elems and second depending on\nfn\n\n\n\n\nfoldl\n\n\nkeras.backend.foldl(fn, elems, initializer=None, name=None)\n\n\n\n\nReduce elems using fn to combine them from left to right.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems and an\naccumulator, for instance lambda acc, x: acc + x\n\n\nelems\n: tensor\n\n\ninitializer\n: The first value used (elems[0] in case of None)\n\n\nname\n: A string name for the foldl node in the graph\n\n\n\n\nReturns\n\n\nSame type and shape as initializer\n\n\n\n\nfoldr\n\n\nkeras.backend.foldr(fn, elems, initializer=None, name=None)\n\n\n\n\nReduce elems using fn to combine them from right to left.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems and an\naccumulator, for instance lambda acc, x: acc + x\n\n\nelems\n: tensor\n\n\ninitializer\n: The first value used (elems[-1] in case of None)\n\n\nname\n: A string name for the foldr node in the graph\n\n\n\n\nReturns\n\n\nSame type and shape as initializer\n\n\n\n\nlocal_conv1d\n\n\nkeras.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)\n\n\n\n\n\n\nlocal_conv2d\n\n\nkeras.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
            "title": "Backend"
        },
        {
            "location": "/backend/#keras-backends",
            "text": "",
            "title": "Keras backends"
        },
        {
            "location": "/backend/#what-is-a-backend",
            "text": "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.  At this time, Keras has three backend implementations available: the  TensorFlow  backend, the  Theano  backend, and the  CNTK  backend.   TensorFlow  is an open-source symbolic tensor manipulation framework developed by Google.  Theano  is an open-source symbolic tensor manipulation framework developed by LISA Lab at Universit\u00e9 de Montr\u00e9al.  CNTK  is an open-source toolkit for deep learning developed by Microsoft.   In the future, we are likely to add more backend options.",
            "title": "What is a \"backend\"?"
        },
        {
            "location": "/backend/#switching-from-one-backend-to-another",
            "text": "If you have run Keras at least once, you will find the Keras configuration file at:  $HOME/.keras/keras.json  If it isn't there, you can create it.  NOTE for Windows Users:  Please replace  $HOME  with  %USERPROFILE% .  The default configuration file looks like this:  {\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}  Simply change the field  backend  to  \"theano\" ,  \"tensorflow\" , or  \"cntk\" , and Keras will use the new configuration next time you run any Keras code.  You can also define the environment variable  KERAS_BACKEND  and this will\noverride what is defined in your config file :  KERAS_BACKEND=tensorflow python -c \"from keras import backend\"\nUsing TensorFlow backend.",
            "title": "Switching from one backend to another"
        },
        {
            "location": "/backend/#kerasjson-details",
            "text": "The  keras.json  configuration file contains the following settings:  {\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}  You can change these settings by editing  $HOME/.keras/keras.json .    image_data_format : String, either  \"channels_last\"  or  \"channels_first\" . It specifies which data format convention Keras will follow. ( keras.backend.image_data_format()  returns it.)  For 2D data (e.g. image),  \"channels_last\"  assumes  (rows, cols, channels)  while  \"channels_first\"  assumes  (channels, rows, cols) .   For 3D data,  \"channels_last\"  assumes  (conv_dim1, conv_dim2, conv_dim3, channels)  while  \"channels_first\"  assumes  (channels, conv_dim1, conv_dim2, conv_dim3) .  epsilon : Float, a numeric fuzzing constant used to avoid dividing by zero in some operations.  floatx : String,  \"float16\" ,  \"float32\" , or  \"float64\" . Default float precision.  backend : String,  \"tensorflow\" ,  \"theano\" , or  \"cntk\" .",
            "title": "keras.json details"
        },
        {
            "location": "/backend/#using-the-abstract-keras-backend-to-write-new-code",
            "text": "If you want the Keras modules you write to be compatible with both Theano ( th ) and TensorFlow ( tf ), you have to write them via the abstract Keras backend API. Here's an intro.  You can import the backend module via:  from keras import backend as K  The code below instantiates an input placeholder. It's equivalent to  tf.placeholder()  or  th.tensor.matrix() ,  th.tensor.tensor3() , etc.  inputs = K.placeholder(shape=(2, 4, 5))\n# also works:\ninputs = K.placeholder(shape=(None, 4, 5))\n# also works:\ninputs = K.placeholder(ndim=3)  The code below instantiates a variable. It's equivalent to  tf.Variable()  or  th.shared() .  import numpy as np\nval = np.random.random((3, 4, 5))\nvar = K.variable(value=val)\n\n# all-zeros variable:\nvar = K.zeros(shape=(3, 4, 5))\n# all-ones:\nvar = K.ones(shape=(3, 4, 5))  Most tensor operations you will need can be done as you would in TensorFlow or Theano:  # Initializing Tensors with Random Numbers\nb = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # Uniform distribution\nc = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Gaussian distribution\nd = K.random_normal_variable(shape=(3, 4), mean=0, scale=1)\n\n# Tensor Arithmetic\na = b + c * K.abs(d)\nc = K.dot(a, K.transpose(b))\na = K.sum(b, axis=1)\na = K.softmax(b)\na = K.concatenate([b, c], axis=-1)\n# etc...",
            "title": "Using the abstract Keras backend to write new code"
        },
        {
            "location": "/backend/#backend-functions",
            "text": "",
            "title": "Backend functions"
        },
        {
            "location": "/backend/#backend",
            "text": "backend.backend()  Publicly accessible method\nfor determining the current backend.  Returns  String, the name of the backend Keras is currently using.  Example  >>> keras.backend.backend()\n'tensorflow'",
            "title": "backend"
        },
        {
            "location": "/backend/#epsilon",
            "text": "keras.backend.epsilon()  Returns the value of the fuzz factor used in numeric expressions.  Returns  A float.  Example  >>> keras.backend.epsilon()\n1e-07",
            "title": "epsilon"
        },
        {
            "location": "/backend/#set_epsilon",
            "text": "keras.backend.set_epsilon(e)  Sets the value of the fuzz factor used in numeric expressions.  Arguments   e : float. New value of epsilon.   Example  >>> from keras import backend as K\n>>> K.epsilon()\n1e-07\n>>> K.set_epsilon(1e-05)\n>>> K.epsilon()\n1e-05",
            "title": "set_epsilon"
        },
        {
            "location": "/backend/#floatx",
            "text": "keras.backend.floatx()  Returns the default float type, as a string.\n(e.g. 'float16', 'float32', 'float64').  Returns  String, the current default float type.  Example  >>> keras.backend.floatx()\n'float32'",
            "title": "floatx"
        },
        {
            "location": "/backend/#set_floatx",
            "text": "keras.backend.set_floatx(floatx)  Sets the default float type.  Arguments   floatx : String, 'float16', 'float32', or 'float64'.   Example  >>> from keras import backend as K\n>>> K.floatx()\n'float32'\n>>> K.set_floatx('float16')\n>>> K.floatx()\n'float16'",
            "title": "set_floatx"
        },
        {
            "location": "/backend/#cast_to_floatx",
            "text": "keras.backend.cast_to_floatx(x)  Cast a Numpy array to the default Keras float type.  Arguments   x : Numpy array.   Returns  The same Numpy array, cast to its new type.  Example  >>> from keras import backend as K\n>>> K.floatx()\n'float32'\n>>> arr = numpy.array([1.0, 2.0], dtype='float64')\n>>> arr.dtype\ndtype('float64')\n>>> new_arr = K.cast_to_floatx(arr)\n>>> new_arr\narray([ 1.,  2.], dtype=float32)\n>>> new_arr.dtype\ndtype('float32')",
            "title": "cast_to_floatx"
        },
        {
            "location": "/backend/#image_data_format",
            "text": "keras.backend.image_data_format()  Returns the default image data format convention ('channels_first' or 'channels_last').  Returns  A string, either  'channels_first'  or  'channels_last'  Example  >>> keras.backend.image_data_format()\n'channels_first'",
            "title": "image_data_format"
        },
        {
            "location": "/backend/#set_image_data_format",
            "text": "keras.backend.set_image_data_format(data_format)  Sets the value of the data format convention.  Arguments   data_format : string.  'channels_first'  or  'channels_last' .   Example  >>> from keras import backend as K\n>>> K.image_data_format()\n'channels_first'\n>>> K.set_image_data_format('channels_last')\n>>> K.image_data_format()\n'channels_last'",
            "title": "set_image_data_format"
        },
        {
            "location": "/backend/#learning_phase",
            "text": "keras.backend.learning_phase()",
            "title": "learning_phase"
        },
        {
            "location": "/backend/#set_learning_phase",
            "text": "keras.backend.set_learning_phase(value)",
            "title": "set_learning_phase"
        },
        {
            "location": "/backend/#get_uid",
            "text": "keras.backend.get_uid(prefix='')  Provides a unique UID given a string prefix.  Arguments   prefix : string.   Returns  An integer.  Example  >>> keras.backend.get_uid('dense')\n1\n>>> keras.backend.get_uid('dense')\n2",
            "title": "get_uid"
        },
        {
            "location": "/backend/#reset_uids",
            "text": "keras.backend.reset_uids()",
            "title": "reset_uids"
        },
        {
            "location": "/backend/#is_sparse",
            "text": "keras.backend.is_sparse(tensor)",
            "title": "is_sparse"
        },
        {
            "location": "/backend/#to_dense",
            "text": "keras.backend.to_dense(tensor)",
            "title": "to_dense"
        },
        {
            "location": "/backend/#name_scope",
            "text": "keras.backend.name_scope()",
            "title": "name_scope"
        },
        {
            "location": "/backend/#variable",
            "text": "keras.backend.variable(value, dtype=None, name=None, constraint=None)  Instantiates a variable and returns it.  Arguments   value : Numpy array, initial value of the tensor.  dtype : Tensor type.  name : Optional name string for the tensor.  constraint : Optional projection function to be\napplied to the variable after an optimizer update.   Returns  A variable instance (with Keras metadata included).",
            "title": "variable"
        },
        {
            "location": "/backend/#constant",
            "text": "keras.backend.constant(value, dtype=None, shape=None, name=None)",
            "title": "constant"
        },
        {
            "location": "/backend/#is_keras_tensor",
            "text": "keras.backend.is_keras_tensor(x)  Returns whether  x  is a Keras tensor.  A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n( Layer  class) or by  Input .  Arguments   x : A candidate tensor.   Returns  A boolean: Whether the argument is a Keras tensor.  Raises   ValueError : In case  x  is not a symbolic tensor.   Examples  >>> from keras import backend as K\n>>> from keras.layers import Input, Dense\n>>> np_var = numpy.array([1, 2])\n>>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\nValueError\n>>> k_var = tf.placeholder('float32', shape=(1,1))\n>>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\nFalse\n>>> keras_var = K.variable(np_var)\n>>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\nFalse\n>>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n>>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\nFalse\n>>> keras_input = Input([10])\n>>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\nTrue\n>>> keras_layer_output = Dense(10)(keras_input)\n>>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\nTrue",
            "title": "is_keras_tensor"
        },
        {
            "location": "/backend/#placeholder",
            "text": "keras.backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)  Instantiate an input data placeholder variable.",
            "title": "placeholder"
        },
        {
            "location": "/backend/#is_placeholder",
            "text": "keras.backend.is_placeholder(x)  Returns whether  x  is a placeholder.  Arguments   x : A candidate placeholder.   Returns  Boolean.",
            "title": "is_placeholder"
        },
        {
            "location": "/backend/#shape",
            "text": "keras.backend.shape(x)  Returns the shape of a tensor.   Warning : type returned will be different for\nTheano backend (Theano tensor type) and TF backend (TF TensorShape).",
            "title": "shape"
        },
        {
            "location": "/backend/#int_shape",
            "text": "keras.backend.int_shape(x)  Returns the shape of a Keras tensor or a Keras variable as a tuple of\nintegers or None entries.  Arguments   x : Tensor or variable.   Returns  A tuple of integers (or None entries).",
            "title": "int_shape"
        },
        {
            "location": "/backend/#ndim",
            "text": "keras.backend.ndim(x)",
            "title": "ndim"
        },
        {
            "location": "/backend/#dtype",
            "text": "keras.backend.dtype(x)",
            "title": "dtype"
        },
        {
            "location": "/backend/#eval",
            "text": "keras.backend.eval(x)  Returns the value of a tensor.",
            "title": "eval"
        },
        {
            "location": "/backend/#zeros",
            "text": "keras.backend.zeros(shape, dtype=None, name=None)  Instantiates an all-zeros variable.",
            "title": "zeros"
        },
        {
            "location": "/backend/#ones",
            "text": "keras.backend.ones(shape, dtype=None, name=None)  Instantiates an all-ones variable.",
            "title": "ones"
        },
        {
            "location": "/backend/#eye",
            "text": "keras.backend.eye(size, dtype=None, name=None)  Instantiates an identity matrix.",
            "title": "eye"
        },
        {
            "location": "/backend/#ones_like",
            "text": "keras.backend.ones_like(x, dtype=None, name=None)",
            "title": "ones_like"
        },
        {
            "location": "/backend/#zeros_like",
            "text": "keras.backend.zeros_like(x, dtype=None, name=None)",
            "title": "zeros_like"
        },
        {
            "location": "/backend/#identity",
            "text": "keras.backend.identity(x, name=None)  Returns a tensor with the same content as the input tensor.  Arguments   x : The input tensor.  name : String, name for the variable to create.   Returns  A tensor of the same shape, type and content.",
            "title": "identity"
        },
        {
            "location": "/backend/#random_uniform_variable",
            "text": "keras.backend.random_uniform_variable(shape, low, high, dtype=None, name=None)",
            "title": "random_uniform_variable"
        },
        {
            "location": "/backend/#random_normal_variable",
            "text": "keras.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None)",
            "title": "random_normal_variable"
        },
        {
            "location": "/backend/#count_params",
            "text": "keras.backend.count_params(x)  Returns the number of scalars in a tensor.   Return : numpy integer.",
            "title": "count_params"
        },
        {
            "location": "/backend/#cast",
            "text": "keras.backend.cast(x, dtype)",
            "title": "cast"
        },
        {
            "location": "/backend/#update",
            "text": "keras.backend.update(x, new_x)",
            "title": "update"
        },
        {
            "location": "/backend/#update_add",
            "text": "keras.backend.update_add(x, increment)",
            "title": "update_add"
        },
        {
            "location": "/backend/#update_sub",
            "text": "keras.backend.update_sub(x, decrement)",
            "title": "update_sub"
        },
        {
            "location": "/backend/#moving_average_update",
            "text": "keras.backend.moving_average_update(variable, value, momentum)",
            "title": "moving_average_update"
        },
        {
            "location": "/backend/#dot",
            "text": "keras.backend.dot(x, y)",
            "title": "dot"
        },
        {
            "location": "/backend/#batch_dot",
            "text": "keras.backend.batch_dot(x, y, axes=None)  Batchwise dot product.  batch_dot results in a tensor with less dimensions than the input.\nIf the number of dimensions is reduced to 1, we use  expand_dims  to\nmake sure that ndim is at least 2.  Arguments  x, y: tensors with ndim >= 2\n-  axes : list (or single) int with target dimensions  Returns  A tensor with shape equal to the concatenation of x's shape\n(less the dimension that was summed over) and y's shape\n(less the batch dimension and the dimension that was summed over).\nIf the final rank is 1, we reshape it to (batch_size, 1).  Examples  Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\nbatch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\nof x.dot(y.T), although we never have to calculate the off-diagonal\nelements.  Shape inference:\nLet x's shape be (100, 20) and y's shape be (100, 30, 20).\nIf dot_axes is (1, 2), to find the output shape of resultant tensor,\nloop through each dimension in x's shape and y's shape:\nx.shape[0] : 100 : append to output shape\nx.shape[1] : 20 : do not append to output shape,\ndimension 1 of x has been summed over. (dot_axes[0] = 1)\ny.shape[0] : 100 : do not append to output shape,\nalways ignore first dimension of y\ny.shape[1] : 30 : append to output shape\ny.shape[2] : 20 : do not append to output shape,\ndimension 2 of y has been summed over. (dot_axes[1] = 2)  output_shape = (100, 30)",
            "title": "batch_dot"
        },
        {
            "location": "/backend/#transpose",
            "text": "keras.backend.transpose(x)",
            "title": "transpose"
        },
        {
            "location": "/backend/#gather",
            "text": "keras.backend.gather(reference, indices)  Retrieves the elements of indices  indices  in the tensor  reference .  Arguments   reference : A tensor.  indices : An integer tensor of indices.   Returns  A tensor of same type as  reference .",
            "title": "gather"
        },
        {
            "location": "/backend/#max",
            "text": "keras.backend.max(x, axis=None, keepdims=False)",
            "title": "max"
        },
        {
            "location": "/backend/#min",
            "text": "keras.backend.min(x, axis=None, keepdims=False)",
            "title": "min"
        },
        {
            "location": "/backend/#sum",
            "text": "keras.backend.sum(x, axis=None, keepdims=False)  Sum of the values in a tensor, alongside the specified axis.",
            "title": "sum"
        },
        {
            "location": "/backend/#prod",
            "text": "keras.backend.prod(x, axis=None, keepdims=False)  Multiply the values in a tensor, alongside the specified axis.",
            "title": "prod"
        },
        {
            "location": "/backend/#cumsum",
            "text": "keras.backend.cumsum(x, axis=0)  Cumulative sum of the values in a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to compute the sum.   Returns  A tensor of the cumulative sum of values of  x  along  axis .",
            "title": "cumsum"
        },
        {
            "location": "/backend/#cumprod",
            "text": "keras.backend.cumprod(x, axis=0)  Cumulative product of the values in a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to compute the product.   Returns  A tensor of the cumulative product of values of  x  along  axis .",
            "title": "cumprod"
        },
        {
            "location": "/backend/#mean",
            "text": "keras.backend.mean(x, axis=None, keepdims=False)  Mean of a tensor, alongside the specified axis.",
            "title": "mean"
        },
        {
            "location": "/backend/#std",
            "text": "keras.backend.std(x, axis=None, keepdims=False)",
            "title": "std"
        },
        {
            "location": "/backend/#var",
            "text": "keras.backend.var(x, axis=None, keepdims=False)",
            "title": "var"
        },
        {
            "location": "/backend/#any",
            "text": "keras.backend.any(x, axis=None, keepdims=False)  Bitwise reduction (logical OR).",
            "title": "any"
        },
        {
            "location": "/backend/#all",
            "text": "keras.backend.all(x, axis=None, keepdims=False)  Bitwise reduction (logical AND).",
            "title": "all"
        },
        {
            "location": "/backend/#argmax",
            "text": "keras.backend.argmax(x, axis=-1)",
            "title": "argmax"
        },
        {
            "location": "/backend/#argmin",
            "text": "keras.backend.argmin(x, axis=-1)",
            "title": "argmin"
        },
        {
            "location": "/backend/#square",
            "text": "keras.backend.square(x)",
            "title": "square"
        },
        {
            "location": "/backend/#abs",
            "text": "keras.backend.abs(x)",
            "title": "abs"
        },
        {
            "location": "/backend/#sqrt",
            "text": "keras.backend.sqrt(x)",
            "title": "sqrt"
        },
        {
            "location": "/backend/#exp",
            "text": "keras.backend.exp(x)",
            "title": "exp"
        },
        {
            "location": "/backend/#log",
            "text": "keras.backend.log(x)",
            "title": "log"
        },
        {
            "location": "/backend/#logsumexp",
            "text": "keras.backend.logsumexp(x, axis=None, keepdims=False)  Computes log(sum(exp(elements across dimensions of a tensor))).  This function is more numerically stable than log(sum(exp(x))).\nIt avoids overflows caused by taking the exp of large inputs and\nunderflows caused by taking the log of small inputs.  Arguments   x : A tensor or variable.  axis : An integer, the axis to reduce over.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True , the reduced dimension is\nretained with length 1.   Returns  The reduced tensor.",
            "title": "logsumexp"
        },
        {
            "location": "/backend/#round",
            "text": "keras.backend.round(x)",
            "title": "round"
        },
        {
            "location": "/backend/#sign",
            "text": "keras.backend.sign(x)",
            "title": "sign"
        },
        {
            "location": "/backend/#pow",
            "text": "keras.backend.pow(x, a)",
            "title": "pow"
        },
        {
            "location": "/backend/#clip",
            "text": "keras.backend.clip(x, min_value, max_value)",
            "title": "clip"
        },
        {
            "location": "/backend/#equal",
            "text": "keras.backend.equal(x, y)",
            "title": "equal"
        },
        {
            "location": "/backend/#not_equal",
            "text": "keras.backend.not_equal(x, y)",
            "title": "not_equal"
        },
        {
            "location": "/backend/#greater",
            "text": "keras.backend.greater(x, y)",
            "title": "greater"
        },
        {
            "location": "/backend/#greater_equal",
            "text": "keras.backend.greater_equal(x, y)",
            "title": "greater_equal"
        },
        {
            "location": "/backend/#less",
            "text": "keras.backend.less(x, y)",
            "title": "less"
        },
        {
            "location": "/backend/#less_equal",
            "text": "keras.backend.less_equal(x, y)",
            "title": "less_equal"
        },
        {
            "location": "/backend/#maximum",
            "text": "keras.backend.maximum(x, y)",
            "title": "maximum"
        },
        {
            "location": "/backend/#minimum",
            "text": "keras.backend.minimum(x, y)",
            "title": "minimum"
        },
        {
            "location": "/backend/#sin",
            "text": "keras.backend.sin(x)",
            "title": "sin"
        },
        {
            "location": "/backend/#cos",
            "text": "keras.backend.cos(x)",
            "title": "cos"
        },
        {
            "location": "/backend/#normalize_batch_in_training",
            "text": "keras.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)  Computes mean and std for batch then apply batch_normalization on batch.",
            "title": "normalize_batch_in_training"
        },
        {
            "location": "/backend/#batch_normalization",
            "text": "keras.backend.batch_normalization(x, mean, var, beta, gamma, epsilon=0.001)  Apply batch normalization on x given mean, var, beta and gamma.",
            "title": "batch_normalization"
        },
        {
            "location": "/backend/#concatenate",
            "text": "keras.backend.concatenate(tensors, axis=-1)",
            "title": "concatenate"
        },
        {
            "location": "/backend/#reshape",
            "text": "keras.backend.reshape(x, shape)",
            "title": "reshape"
        },
        {
            "location": "/backend/#permute_dimensions",
            "text": "keras.backend.permute_dimensions(x, pattern)  Transpose dimensions.  pattern should be a tuple or list of\ndimension indices, e.g. [0, 2, 1].",
            "title": "permute_dimensions"
        },
        {
            "location": "/backend/#repeat_elements",
            "text": "keras.backend.repeat_elements(x, rep, axis)  Repeat the elements of a tensor along an axis, like np.repeat.  If x has shape (s1, s2, s3) and axis=1, the output\nwill have shape (s1, s2 * rep, s3).",
            "title": "repeat_elements"
        },
        {
            "location": "/backend/#resize_images",
            "text": "keras.backend.resize_images(x, height_factor, width_factor, data_format)  Resize the images contained in a 4D tensor of shape\n- [batch, channels, height, width] (for 'channels_first' data_format)\n- [batch, height, width, channels] (for 'channels_last' data_format)\nby a factor of (height_factor, width_factor). Both factors should be\npositive integers.",
            "title": "resize_images"
        },
        {
            "location": "/backend/#resize_volumes",
            "text": "keras.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format)  Resize the volume contained in a 5D tensor of shape\n- [batch, channels, depth, height, width] (for 'channels_first' data_format)\n- [batch, depth, height, width, channels] (for 'channels_last' data_format)\nby a factor of (depth_factor, height_factor, width_factor).\nBoth factors should be positive integers.",
            "title": "resize_volumes"
        },
        {
            "location": "/backend/#repeat",
            "text": "keras.backend.repeat(x, n)  Repeat a 2D tensor.  If x has shape (samples, dim) and n=2,\nthe output will have shape (samples, 2, dim).",
            "title": "repeat"
        },
        {
            "location": "/backend/#arange",
            "text": "keras.backend.arange(start, stop=None, step=1, dtype='int32')  Creates a 1-D tensor containing a sequence of integers.  The function arguments use the same convention as\nTheano's arange: if only one argument is provided,\nit is in fact the \"stop\" argument.  The default type of the returned tensor is 'int32' to\nmatch TensorFlow's default.",
            "title": "arange"
        },
        {
            "location": "/backend/#tile",
            "text": "keras.backend.tile(x, n)",
            "title": "tile"
        },
        {
            "location": "/backend/#flatten",
            "text": "keras.backend.flatten(x)",
            "title": "flatten"
        },
        {
            "location": "/backend/#batch_flatten",
            "text": "keras.backend.batch_flatten(x)  Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.",
            "title": "batch_flatten"
        },
        {
            "location": "/backend/#expand_dims",
            "text": "keras.backend.expand_dims(x, axis=-1)  Add a 1-sized dimension at index \"dim\".",
            "title": "expand_dims"
        },
        {
            "location": "/backend/#squeeze",
            "text": "keras.backend.squeeze(x, axis)  Remove a 1-dimension from the tensor at index \"axis\".",
            "title": "squeeze"
        },
        {
            "location": "/backend/#temporal_padding",
            "text": "keras.backend.temporal_padding(x, padding=(1, 1))  Pad the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.  Apologies for the inane API, but Theano makes this\nreally hard.",
            "title": "temporal_padding"
        },
        {
            "location": "/backend/#spatial_2d_padding",
            "text": "keras.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)  Pad the 2nd and 3rd dimensions of a 4D tensor\nwith \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.",
            "title": "spatial_2d_padding"
        },
        {
            "location": "/backend/#spatial_3d_padding",
            "text": "keras.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)  Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\nwith \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.",
            "title": "spatial_3d_padding"
        },
        {
            "location": "/backend/#stack",
            "text": "keras.backend.stack(x, axis=0)",
            "title": "stack"
        },
        {
            "location": "/backend/#one_hot",
            "text": "keras.backend.one_hot(indices, num_classes)  Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n-  Output : (n + 1)D one hot representation of the input\nwith shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)",
            "title": "one_hot"
        },
        {
            "location": "/backend/#reverse",
            "text": "keras.backend.reverse(x, axes)  Reverse a tensor along the specified axes",
            "title": "reverse"
        },
        {
            "location": "/backend/#pattern_broadcast",
            "text": "keras.backend.pattern_broadcast(x, broadcastable)",
            "title": "pattern_broadcast"
        },
        {
            "location": "/backend/#get_value",
            "text": "keras.backend.get_value(x)",
            "title": "get_value"
        },
        {
            "location": "/backend/#batch_get_value",
            "text": "keras.backend.batch_get_value(xs)  Returns the value of more than one tensor variable,\nas a list of Numpy arrays.",
            "title": "batch_get_value"
        },
        {
            "location": "/backend/#set_value",
            "text": "keras.backend.set_value(x, value)",
            "title": "set_value"
        },
        {
            "location": "/backend/#batch_set_value",
            "text": "keras.backend.batch_set_value(tuples)",
            "title": "batch_set_value"
        },
        {
            "location": "/backend/#print_tensor",
            "text": "keras.backend.print_tensor(x, message='')  Print the message and the tensor when evaluated and return the same\ntensor.",
            "title": "print_tensor"
        },
        {
            "location": "/backend/#function",
            "text": "keras.backend.function(inputs, outputs, updates=[])",
            "title": "function"
        },
        {
            "location": "/backend/#gradients",
            "text": "keras.backend.gradients(loss, variables)",
            "title": "gradients"
        },
        {
            "location": "/backend/#stop_gradient",
            "text": "keras.backend.stop_gradient(variables)  Returns  variables  but with zero gradient w.r.t. every other variable.  Arguments   variables : tensor or list of tensors to consider constant with respect\nto any other variable.   Returns  A single tensor or a list of tensors (depending on the passed argument)\nthat has constant gradient with respect to any other variable.",
            "title": "stop_gradient"
        },
        {
            "location": "/backend/#rnn",
            "text": "keras.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)  Iterates over the time dimension of a tensor.  Arguments   inputs : tensor of temporal data of shape (samples, time, ...)\n(at least 3D).  step_function :  Parameters :  inputs : tensor with shape (samples, ...) (no time dimension),\nrepresenting input for the batch of samples at a certain\ntime step.  states : list of tensors.  Returns :  outputs : tensor with shape (samples, ...) (no time dimension),  new_states : list of tensors, same length and shapes\nas 'states'.  initial_states : tensor with shape (samples, ...) (no time dimension),\ncontaining the initial values for the states used in\nthe step function.  go_backwards : boolean. If True, do the iteration over the time\ndimension in reverse order and return the reversed sequence.  mask : binary tensor with shape (samples, time),\nwith a zero for every element that is masked.  constants : a list of constant values passed at each step.  unroll : whether to unroll the RNN or to use a symbolic loop ( while_loop  or  scan  depending on backend).  input_length : must be specified if using  unroll .   Returns  A tuple (last_output, outputs, new_states).\n-  last_output : the latest output of the rnn, of shape (samples, ...)\n-  outputs : tensor with shape (samples, time, ...) where each\nentry outputs[s, t] is the output of the step function\nat time t for sample s.\n-  new_states : list of tensors, latest states returned by\nthe step function, of shape (samples, ...).",
            "title": "rnn"
        },
        {
            "location": "/backend/#switch",
            "text": "keras.backend.switch(condition, then_expression, else_expression)  Switches between two operations depending on a scalar value.  Note that both  then_expression  and  else_expression \nshould be symbolic tensors of the  same shape .  Arguments   condition : scalar tensor ( int  or  bool ).  then_expression : either a tensor, or a callable that returns a tensor.  else_expression : either a tensor, or a callable that returns a tensor.   Returns  The selected tensor.",
            "title": "switch"
        },
        {
            "location": "/backend/#in_train_phase",
            "text": "keras.backend.in_train_phase(x, alt, training=None)  Selects  x  in train phase, and  alt  otherwise.  Note that  alt  should have the  same shape  as  x .  Returns  Either  x  or  alt  based on the  training  flag.\nthe  training  flag defaults to  K.learning_phase() .",
            "title": "in_train_phase"
        },
        {
            "location": "/backend/#in_test_phase",
            "text": "keras.backend.in_test_phase(x, alt, training=None)  Selects  x  in test phase, and  alt  otherwise.\nNote that  alt  should have the  same shape  as  x .  Returns  Either  x  or  alt  based on  K.learning_phase .",
            "title": "in_test_phase"
        },
        {
            "location": "/backend/#elu",
            "text": "keras.backend.elu(x, alpha=1.0)  Exponential linear unit  Arguments   x : Tensor to compute the activation function for.  alpha : scalar",
            "title": "elu"
        },
        {
            "location": "/backend/#relu",
            "text": "keras.backend.relu(x, alpha=0.0, max_value=None)",
            "title": "relu"
        },
        {
            "location": "/backend/#softmax",
            "text": "keras.backend.softmax(x)",
            "title": "softmax"
        },
        {
            "location": "/backend/#softplus",
            "text": "keras.backend.softplus(x)",
            "title": "softplus"
        },
        {
            "location": "/backend/#softsign",
            "text": "keras.backend.softsign(x)",
            "title": "softsign"
        },
        {
            "location": "/backend/#categorical_crossentropy",
            "text": "keras.backend.categorical_crossentropy(target, output, from_logits=False)",
            "title": "categorical_crossentropy"
        },
        {
            "location": "/backend/#sparse_categorical_crossentropy",
            "text": "keras.backend.sparse_categorical_crossentropy(target, output, from_logits=False)",
            "title": "sparse_categorical_crossentropy"
        },
        {
            "location": "/backend/#binary_crossentropy",
            "text": "keras.backend.binary_crossentropy(target, output, from_logits=False)",
            "title": "binary_crossentropy"
        },
        {
            "location": "/backend/#sigmoid",
            "text": "keras.backend.sigmoid(x)",
            "title": "sigmoid"
        },
        {
            "location": "/backend/#hard_sigmoid",
            "text": "keras.backend.hard_sigmoid(x)",
            "title": "hard_sigmoid"
        },
        {
            "location": "/backend/#tanh",
            "text": "keras.backend.tanh(x)",
            "title": "tanh"
        },
        {
            "location": "/backend/#dropout",
            "text": "keras.backend.dropout(x, level, noise_shape=None, seed=None)  Sets entries in  x  to zero at random,\nwhile scaling the entire tensor.  Arguments   x : tensor  level : fraction of the entries in the tensor\nthat will be set to 0.  noise_shape : shape for randomly generated keep/drop flags,\nmust be broadcastable to the shape of  x  seed : random seed to ensure determinism.",
            "title": "dropout"
        },
        {
            "location": "/backend/#l2_normalize",
            "text": "keras.backend.l2_normalize(x, axis=None)",
            "title": "l2_normalize"
        },
        {
            "location": "/backend/#in_top_k",
            "text": "keras.backend.in_top_k(predictions, targets, k)  Returns whether the  targets  are in the top  k   predictions .  Arguments   predictions : A tensor of shape  (batch_size, classes)  and type  float32 .  targets : A 1D tensor of length  batch_size  and type  int32  or  int64 .  k : An  int , number of top elements to consider.   Returns  A 1D tensor of length  batch_size  and type  bool . output[i]  is  True  if  predictions[i, targets[i]]  is within top- k \nvalues of  predictions[i] .",
            "title": "in_top_k"
        },
        {
            "location": "/backend/#conv1d",
            "text": "keras.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)  1D convolution.  Arguments   kernel : kernel tensor.  strides : stride integer.  padding : string,  \"same\" ,  \"causal\"  or  \"valid\" .  data_format : string, one of \"channels_last\", \"channels_first\"  dilation_rate : integer.",
            "title": "conv1d"
        },
        {
            "location": "/backend/#conv2d",
            "text": "keras.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))  2D convolution.  Arguments   kernel : kernel tensor.  strides : strides tuple.  padding : string, \"same\" or \"valid\".  data_format : \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.",
            "title": "conv2d"
        },
        {
            "location": "/backend/#conv2d_transpose",
            "text": "keras.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None)  2D deconvolution (transposed convolution).  Arguments   kernel : kernel tensor.  output_shape : desired dimensions of output.  strides : strides tuple.  padding : string, \"same\" or \"valid\".  data_format : \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.   Raises   ValueError : if using an even kernel size with padding 'same'.",
            "title": "conv2d_transpose"
        },
        {
            "location": "/backend/#separable_conv1d",
            "text": "keras.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
            "title": "separable_conv1d"
        },
        {
            "location": "/backend/#separable_conv2d",
            "text": "keras.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
            "title": "separable_conv2d"
        },
        {
            "location": "/backend/#depthwise_conv2d",
            "text": "keras.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))  2D convolution with separable filters.  Arguments   x : input tensor  depthwise_kernel : convolution kernel for the depthwise convolution.  strides : strides tuple (length 2).  padding : string,  \"same\"  or  \"valid\" .  data_format : string,  \"channels_last\"  or  \"channels_first\" .  dilation_rate : tuple of integers,\ndilation rates for the separable convolution.   Returns  Output tensor.  Raises   ValueError : if  data_format  is neither  \"channels_last\"  or  \"channels_first\" .",
            "title": "depthwise_conv2d"
        },
        {
            "location": "/backend/#conv3d",
            "text": "keras.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))  3D convolution.  Arguments   kernel : kernel tensor.  strides : strides tuple.  padding : string, \"same\" or \"valid\".  data_format : \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.",
            "title": "conv3d"
        },
        {
            "location": "/backend/#conv3d_transpose",
            "text": "keras.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)  3D deconvolution (transposed convolution).  Arguments   kernel : kernel tensor.  output_shape : desired dimensions of output.  strides : strides tuple.  padding : string, \"same\" or \"valid\".  data_format : \"channels_last\" or \"channels_first\".\nWhether to use Theano or TensorFlow data format\nin inputs/kernels/outputs.   Raises   ValueError : if using an even kernel size with padding 'same'.",
            "title": "conv3d_transpose"
        },
        {
            "location": "/backend/#pool2d",
            "text": "keras.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')",
            "title": "pool2d"
        },
        {
            "location": "/backend/#pool3d",
            "text": "keras.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')",
            "title": "pool3d"
        },
        {
            "location": "/backend/#bias_add",
            "text": "keras.backend.bias_add(x, bias, data_format=None)",
            "title": "bias_add"
        },
        {
            "location": "/backend/#random_normal",
            "text": "keras.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
            "title": "random_normal"
        },
        {
            "location": "/backend/#random_uniform",
            "text": "keras.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
            "title": "random_uniform"
        },
        {
            "location": "/backend/#random_binomial",
            "text": "keras.backend.random_binomial(shape, p=0.0, dtype=None, seed=None)",
            "title": "random_binomial"
        },
        {
            "location": "/backend/#truncated_normal",
            "text": "keras.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
            "title": "truncated_normal"
        },
        {
            "location": "/backend/#ctc_interleave_blanks",
            "text": "keras.backend.ctc_interleave_blanks(Y)",
            "title": "ctc_interleave_blanks"
        },
        {
            "location": "/backend/#ctc_create_skip_idxs",
            "text": "keras.backend.ctc_create_skip_idxs(Y)",
            "title": "ctc_create_skip_idxs"
        },
        {
            "location": "/backend/#ctc_update_log_p",
            "text": "keras.backend.ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev)",
            "title": "ctc_update_log_p"
        },
        {
            "location": "/backend/#ctc_path_probs",
            "text": "keras.backend.ctc_path_probs(predict, Y, alpha=0.0001)",
            "title": "ctc_path_probs"
        },
        {
            "location": "/backend/#ctc_cost",
            "text": "keras.backend.ctc_cost(predict, Y)",
            "title": "ctc_cost"
        },
        {
            "location": "/backend/#ctc_batch_cost",
            "text": "keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)  Runs CTC loss algorithm on each batch element.  Arguments   y_true : tensor (samples, max_string_length) containing the truth labels  y_pred : tensor (samples, time_steps, num_categories) containing the prediction,\nor output of the softmax  input_length : tensor (samples,1) containing the sequence length for\neach batch item in y_pred  label_length : tensor (samples,1) containing the sequence length for\neach batch item in y_true   Returns  Tensor with shape (samples,1) containing the\nCTC loss of each element",
            "title": "ctc_batch_cost"
        },
        {
            "location": "/backend/#map_fn",
            "text": "keras.backend.map_fn(fn, elems, name=None, dtype=None)  Map the function fn over the elements elems and return the outputs.  Arguments   fn : Callable that will be called upon each element in elems  elems : tensor, at least 2 dimensional  name : A string name for the map node in the graph   Returns  Tensor with first dimension equal to the elems and second depending on\nfn",
            "title": "map_fn"
        },
        {
            "location": "/backend/#foldl",
            "text": "keras.backend.foldl(fn, elems, initializer=None, name=None)  Reduce elems using fn to combine them from left to right.  Arguments   fn : Callable that will be called upon each element in elems and an\naccumulator, for instance lambda acc, x: acc + x  elems : tensor  initializer : The first value used (elems[0] in case of None)  name : A string name for the foldl node in the graph   Returns  Same type and shape as initializer",
            "title": "foldl"
        },
        {
            "location": "/backend/#foldr",
            "text": "keras.backend.foldr(fn, elems, initializer=None, name=None)  Reduce elems using fn to combine them from right to left.  Arguments   fn : Callable that will be called upon each element in elems and an\naccumulator, for instance lambda acc, x: acc + x  elems : tensor  initializer : The first value used (elems[-1] in case of None)  name : A string name for the foldr node in the graph   Returns  Same type and shape as initializer",
            "title": "foldr"
        },
        {
            "location": "/backend/#local_conv1d",
            "text": "keras.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)",
            "title": "local_conv1d"
        },
        {
            "location": "/backend/#local_conv2d",
            "text": "keras.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
            "title": "local_conv2d"
        },
        {
            "location": "/initializers/",
            "text": "Usage of initializers\n\n\nInitializations define the way to set the initial random weights of Keras layers.\n\n\nThe keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply \nkernel_initializer\n and \nbias_initializer\n:\n\n\nmodel.add(Dense(64,\n                kernel_initializer='random_uniform',\n                bias_initializer='zeros'))\n\n\n\n\nAvailable initializers\n\n\nThe following built-in initializers are available as part of the \nkeras.initializers\n module:\n\n\n[source]\n\n\nInitializer\n\n\nkeras.initializers.Initializer()\n\n\n\n\nInitializer base class: all initializers inherit from this class.\n\n\n\n\n[source]\n\n\nZeros\n\n\nkeras.initializers.Zeros()\n\n\n\n\nInitializer that generates tensors initialized to 0.\n\n\n\n\n[source]\n\n\nOnes\n\n\nkeras.initializers.Ones()\n\n\n\n\nInitializer that generates tensors initialized to 1.\n\n\n\n\n[source]\n\n\nConstant\n\n\nkeras.initializers.Constant(value=0)\n\n\n\n\nInitializer that generates tensors initialized to a constant value.\n\n\nArguments\n\n\n\n\nvalue\n: float; the value of the generator tensors.\n\n\n\n\n\n\n[source]\n\n\nRandomNormal\n\n\nkeras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n\n\n\n\nInitializer that generates tensors with a normal distribution.\n\n\nArguments\n\n\n\n\nmean\n: a python scalar or a scalar tensor. Mean of the random values\nto generate.\n\n\nstddev\n: a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate.\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\n\n\n[source]\n\n\nRandomUniform\n\n\nkeras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n\n\n\n\nInitializer that generates tensors with a uniform distribution.\n\n\nArguments\n\n\n\n\nminval\n: A python scalar or a scalar tensor. Lower bound of the range\nof random values to generate.\n\n\nmaxval\n: A python scalar or a scalar tensor. Upper bound of the range\nof random values to generate.  Defaults to 1 for float types.\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\n\n\n[source]\n\n\nTruncatedNormal\n\n\nkeras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n\n\n\n\nInitializer that generates a truncated normal distribution.\n\n\nThese values are similar to values from a \nRandomNormal\n\nexcept that values more than two standard deviations from the mean\nare discarded and re-drawn. This is the recommended initializer for\nneural network weights and filters.\n\n\nArguments\n\n\n\n\nmean\n: a python scalar or a scalar tensor. Mean of the random values\nto generate.\n\n\nstddev\n: a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate.\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\n\n\n[source]\n\n\nVarianceScaling\n\n\nkeras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)\n\n\n\n\nInitializer capable of adapting its scale to the shape of weights.\n\n\nWith \ndistribution=\"normal\"\n, samples are drawn from a truncated normal\ndistribution centered on zero, with \nstddev = sqrt(scale / n)\n where n is:\n\n\n\n\nnumber of input units in the weight tensor, if mode = \"fan_in\"\n\n\nnumber of output units, if mode = \"fan_out\"\n\n\naverage of the numbers of input and output units, if mode = \"fan_avg\"\n\n\n\n\nWith \ndistribution=\"uniform\"\n,\nsamples are drawn from a uniform distribution\nwithin [-limit, limit], with \nlimit = sqrt(3 * scale / n)\n.\n\n\nArguments\n\n\n\n\nscale\n: Scaling factor (positive float).\n\n\nmode\n: One of \"fan_in\", \"fan_out\", \"fan_avg\".\n\n\ndistribution\n: Random distribution to use. One of \"normal\", \"uniform\".\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments.\n\n\n\n\n\n\n[source]\n\n\nOrthogonal\n\n\nkeras.initializers.Orthogonal(gain=1.0, seed=None)\n\n\n\n\nInitializer that generates a random orthogonal matrix.\n\n\nArguments\n\n\n\n\ngain\n: Multiplicative factor to apply to the orthogonal matrix.\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReferences\n\n\nSaxe et al., http://arxiv.org/abs/1312.6120\n\n\n\n\n[source]\n\n\nIdentity\n\n\nkeras.initializers.Identity(gain=1.0)\n\n\n\n\nInitializer that generates the identity matrix.\n\n\nOnly use for square 2D matrices.\n\n\nArguments\n\n\n\n\ngain\n: Multiplicative factor to apply to the identity matrix.\n\n\n\n\n\n\nlecun_uniform\n\n\nlecun_uniform(seed=None)\n\n\n\n\nLeCun uniform initializer.\n\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere \nlimit\n is \nsqrt(3 / fan_in)\n\nwhere \nfan_in\n is the number of input units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\nLeCun 98, Efficient Backprop,\n- \nhttp\n://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n\n\n\n\nglorot_normal\n\n\nglorot_normal(seed=None)\n\n\n\n\nGlorot normal initializer, also called Xavier normal initializer.\n\n\nIt draws samples from a truncated normal distribution centered on 0\nwith \nstddev = sqrt(2 / (fan_in + fan_out))\n\nwhere \nfan_in\n is the number of input units in the weight tensor\nand \nfan_out\n is the number of output units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\nGlorot & Bengio, AISTATS 2010\n- \nhttp\n://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n\n\n\n\nglorot_uniform\n\n\nglorot_uniform(seed=None)\n\n\n\n\nGlorot uniform initializer, also called Xavier uniform initializer.\n\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere \nlimit\n is \nsqrt(6 / (fan_in + fan_out))\n\nwhere \nfan_in\n is the number of input units in the weight tensor\nand \nfan_out\n is the number of output units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\nGlorot & Bengio, AISTATS 2010\n- \nhttp\n://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n\n\n\n\nhe_normal\n\n\nhe_normal(seed=None)\n\n\n\n\nHe normal initializer.\n\n\nIt draws samples from a truncated normal distribution centered on 0\nwith \nstddev = sqrt(2 / fan_in)\n\nwhere \nfan_in\n is the number of input units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\nHe et al., http://arxiv.org/abs/1502.01852\n\n\n\n\nlecun_normal\n\n\nlecun_normal(seed=None)\n\n\n\n\nLeCun normal initializer.\n\n\nIt draws samples from a truncated normal distribution centered on 0\nwith \nstddev = sqrt(1 / fan_in)\n\nwhere \nfan_in\n is the number of input units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\n\n\nSelf-Normalizing Neural Networks\n\n\nEfficient Backprop\n\n\n\n\n\n\nhe_uniform\n\n\nhe_uniform(seed=None)\n\n\n\n\nHe uniform variance scaling initializer.\n\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere \nlimit\n is \nsqrt(6 / fan_in)\n\nwhere \nfan_in\n is the number of input units in the weight tensor.\n\n\nArguments\n\n\n\n\nseed\n: A Python integer. Used to seed the random generator.\n\n\n\n\nReturns\n\n\nAn initializer.\n\n\nReferences\n\n\nHe et al., http://arxiv.org/abs/1502.01852\n\n\nAn initializer may be passed as a string (must match one of the available initializers above), or as a callable:\n\n\nfrom keras import initializers\n\nmodel.add(Dense(64, kernel_initializer=initializers.random_normal(stddev=0.01)))\n\n# also works; will use the default parameters.\nmodel.add(Dense(64, kernel_initializer='random_normal'))\n\n\n\n\nUsing custom initializers\n\n\nIf passing a custom callable, then it must take the argument \nshape\n (shape of the variable to initialize) and \ndtype\n (dtype of generated values):\n\n\nfrom keras import backend as K\n\ndef my_init(shape, dtype=None):\n    return K.random_normal(shape, dtype=dtype)\n\nmodel.add(Dense(64, kernel_initializer=my_init))",
            "title": "Initializers"
        },
        {
            "location": "/initializers/#usage-of-initializers",
            "text": "Initializations define the way to set the initial random weights of Keras layers.  The keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply  kernel_initializer  and  bias_initializer :  model.add(Dense(64,\n                kernel_initializer='random_uniform',\n                bias_initializer='zeros'))",
            "title": "Usage of initializers"
        },
        {
            "location": "/initializers/#available-initializers",
            "text": "The following built-in initializers are available as part of the  keras.initializers  module:  [source]",
            "title": "Available initializers"
        },
        {
            "location": "/initializers/#initializer",
            "text": "keras.initializers.Initializer()  Initializer base class: all initializers inherit from this class.   [source]",
            "title": "Initializer"
        },
        {
            "location": "/initializers/#zeros",
            "text": "keras.initializers.Zeros()  Initializer that generates tensors initialized to 0.   [source]",
            "title": "Zeros"
        },
        {
            "location": "/initializers/#ones",
            "text": "keras.initializers.Ones()  Initializer that generates tensors initialized to 1.   [source]",
            "title": "Ones"
        },
        {
            "location": "/initializers/#constant",
            "text": "keras.initializers.Constant(value=0)  Initializer that generates tensors initialized to a constant value.  Arguments   value : float; the value of the generator tensors.    [source]",
            "title": "Constant"
        },
        {
            "location": "/initializers/#randomnormal",
            "text": "keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)  Initializer that generates tensors with a normal distribution.  Arguments   mean : a python scalar or a scalar tensor. Mean of the random values\nto generate.  stddev : a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate.  seed : A Python integer. Used to seed the random generator.    [source]",
            "title": "RandomNormal"
        },
        {
            "location": "/initializers/#randomuniform",
            "text": "keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)  Initializer that generates tensors with a uniform distribution.  Arguments   minval : A python scalar or a scalar tensor. Lower bound of the range\nof random values to generate.  maxval : A python scalar or a scalar tensor. Upper bound of the range\nof random values to generate.  Defaults to 1 for float types.  seed : A Python integer. Used to seed the random generator.    [source]",
            "title": "RandomUniform"
        },
        {
            "location": "/initializers/#truncatednormal",
            "text": "keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)  Initializer that generates a truncated normal distribution.  These values are similar to values from a  RandomNormal \nexcept that values more than two standard deviations from the mean\nare discarded and re-drawn. This is the recommended initializer for\nneural network weights and filters.  Arguments   mean : a python scalar or a scalar tensor. Mean of the random values\nto generate.  stddev : a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate.  seed : A Python integer. Used to seed the random generator.    [source]",
            "title": "TruncatedNormal"
        },
        {
            "location": "/initializers/#variancescaling",
            "text": "keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)  Initializer capable of adapting its scale to the shape of weights.  With  distribution=\"normal\" , samples are drawn from a truncated normal\ndistribution centered on zero, with  stddev = sqrt(scale / n)  where n is:   number of input units in the weight tensor, if mode = \"fan_in\"  number of output units, if mode = \"fan_out\"  average of the numbers of input and output units, if mode = \"fan_avg\"   With  distribution=\"uniform\" ,\nsamples are drawn from a uniform distribution\nwithin [-limit, limit], with  limit = sqrt(3 * scale / n) .  Arguments   scale : Scaling factor (positive float).  mode : One of \"fan_in\", \"fan_out\", \"fan_avg\".  distribution : Random distribution to use. One of \"normal\", \"uniform\".  seed : A Python integer. Used to seed the random generator.   Raises   ValueError : In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments.    [source]",
            "title": "VarianceScaling"
        },
        {
            "location": "/initializers/#orthogonal",
            "text": "keras.initializers.Orthogonal(gain=1.0, seed=None)  Initializer that generates a random orthogonal matrix.  Arguments   gain : Multiplicative factor to apply to the orthogonal matrix.  seed : A Python integer. Used to seed the random generator.   References  Saxe et al., http://arxiv.org/abs/1312.6120   [source]",
            "title": "Orthogonal"
        },
        {
            "location": "/initializers/#identity",
            "text": "keras.initializers.Identity(gain=1.0)  Initializer that generates the identity matrix.  Only use for square 2D matrices.  Arguments   gain : Multiplicative factor to apply to the identity matrix.",
            "title": "Identity"
        },
        {
            "location": "/initializers/#lecun_uniform",
            "text": "lecun_uniform(seed=None)  LeCun uniform initializer.  It draws samples from a uniform distribution within [-limit, limit]\nwhere  limit  is  sqrt(3 / fan_in) \nwhere  fan_in  is the number of input units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References  LeCun 98, Efficient Backprop,\n-  http ://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf",
            "title": "lecun_uniform"
        },
        {
            "location": "/initializers/#glorot_normal",
            "text": "glorot_normal(seed=None)  Glorot normal initializer, also called Xavier normal initializer.  It draws samples from a truncated normal distribution centered on 0\nwith  stddev = sqrt(2 / (fan_in + fan_out)) \nwhere  fan_in  is the number of input units in the weight tensor\nand  fan_out  is the number of output units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References  Glorot & Bengio, AISTATS 2010\n-  http ://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf",
            "title": "glorot_normal"
        },
        {
            "location": "/initializers/#glorot_uniform",
            "text": "glorot_uniform(seed=None)  Glorot uniform initializer, also called Xavier uniform initializer.  It draws samples from a uniform distribution within [-limit, limit]\nwhere  limit  is  sqrt(6 / (fan_in + fan_out)) \nwhere  fan_in  is the number of input units in the weight tensor\nand  fan_out  is the number of output units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References  Glorot & Bengio, AISTATS 2010\n-  http ://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf",
            "title": "glorot_uniform"
        },
        {
            "location": "/initializers/#he_normal",
            "text": "he_normal(seed=None)  He normal initializer.  It draws samples from a truncated normal distribution centered on 0\nwith  stddev = sqrt(2 / fan_in) \nwhere  fan_in  is the number of input units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References  He et al., http://arxiv.org/abs/1502.01852",
            "title": "he_normal"
        },
        {
            "location": "/initializers/#lecun_normal",
            "text": "lecun_normal(seed=None)  LeCun normal initializer.  It draws samples from a truncated normal distribution centered on 0\nwith  stddev = sqrt(1 / fan_in) \nwhere  fan_in  is the number of input units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References   Self-Normalizing Neural Networks  Efficient Backprop",
            "title": "lecun_normal"
        },
        {
            "location": "/initializers/#he_uniform",
            "text": "he_uniform(seed=None)  He uniform variance scaling initializer.  It draws samples from a uniform distribution within [-limit, limit]\nwhere  limit  is  sqrt(6 / fan_in) \nwhere  fan_in  is the number of input units in the weight tensor.  Arguments   seed : A Python integer. Used to seed the random generator.   Returns  An initializer.  References  He et al., http://arxiv.org/abs/1502.01852  An initializer may be passed as a string (must match one of the available initializers above), or as a callable:  from keras import initializers\n\nmodel.add(Dense(64, kernel_initializer=initializers.random_normal(stddev=0.01)))\n\n# also works; will use the default parameters.\nmodel.add(Dense(64, kernel_initializer='random_normal'))",
            "title": "he_uniform"
        },
        {
            "location": "/initializers/#using-custom-initializers",
            "text": "If passing a custom callable, then it must take the argument  shape  (shape of the variable to initialize) and  dtype  (dtype of generated values):  from keras import backend as K\n\ndef my_init(shape, dtype=None):\n    return K.random_normal(shape, dtype=dtype)\n\nmodel.add(Dense(64, kernel_initializer=my_init))",
            "title": "Using custom initializers"
        },
        {
            "location": "/regularizers/",
            "text": "Usage of regularizers\n\n\nRegularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n\n\nThe penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers \nDense\n, \nConv1D\n, \nConv2D\n and \nConv3D\n have a unified API.\n\n\nThese layers expose 3 keyword arguments:\n\n\n\n\nkernel_regularizer\n: instance of \nkeras.regularizers.Regularizer\n\n\nbias_regularizer\n: instance of \nkeras.regularizers.Regularizer\n\n\nactivity_regularizer\n: instance of \nkeras.regularizers.Regularizer\n\n\n\n\nExample\n\n\nfrom keras import regularizers\nmodel.add(Dense(64, input_dim=64,\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\n\n\n\n\nAvailable penalties\n\n\nkeras.regularizers.l1(0.)\nkeras.regularizers.l2(0.)\nkeras.regularizers.l1_l2(0.)\n\n\n\n\nDeveloping new regularizers\n\n\nAny function that takes in a weight matrix and returns a loss contribution tensor can be used as a regularizer, e.g.:\n\n\nfrom keras import backend as K\n\ndef l1_reg(weight_matrix):\n    return 0.01 * K.sum(K.abs(weight_matrix))\n\nmodel.add(Dense(64, input_dim=64,\n                kernel_regularizer=l1_reg))\n\n\n\n\nAlternatively, you can write your regularizers in an object-oriented way;\nsee the \nkeras/regularizers.py\n module for examples.",
            "title": "Regularizers"
        },
        {
            "location": "/regularizers/#usage-of-regularizers",
            "text": "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.  The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers  Dense ,  Conv1D ,  Conv2D  and  Conv3D  have a unified API.  These layers expose 3 keyword arguments:   kernel_regularizer : instance of  keras.regularizers.Regularizer  bias_regularizer : instance of  keras.regularizers.Regularizer  activity_regularizer : instance of  keras.regularizers.Regularizer",
            "title": "Usage of regularizers"
        },
        {
            "location": "/regularizers/#example",
            "text": "from keras import regularizers\nmodel.add(Dense(64, input_dim=64,\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))",
            "title": "Example"
        },
        {
            "location": "/regularizers/#available-penalties",
            "text": "keras.regularizers.l1(0.)\nkeras.regularizers.l2(0.)\nkeras.regularizers.l1_l2(0.)",
            "title": "Available penalties"
        },
        {
            "location": "/regularizers/#developing-new-regularizers",
            "text": "Any function that takes in a weight matrix and returns a loss contribution tensor can be used as a regularizer, e.g.:  from keras import backend as K\n\ndef l1_reg(weight_matrix):\n    return 0.01 * K.sum(K.abs(weight_matrix))\n\nmodel.add(Dense(64, input_dim=64,\n                kernel_regularizer=l1_reg))  Alternatively, you can write your regularizers in an object-oriented way;\nsee the  keras/regularizers.py  module for examples.",
            "title": "Developing new regularizers"
        },
        {
            "location": "/constraints/",
            "text": "Usage of constraints\n\n\nFunctions from the \nconstraints\n module allow setting constraints (eg. non-negativity) on network parameters during optimization.\n\n\nThe penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers \nDense\n, \nConv1D\n, \nConv2D\n and \nConv3D\n have a unified API.\n\n\nThese layers expose 2 keyword arguments:\n\n\n\n\nkernel_constraint\n for the main weights matrix\n\n\nbias_constraint\n for the bias.\n\n\n\n\nfrom keras.constraints import max_norm\nmodel.add(Dense(64, kernel_constraint=max_norm(2.)))\n\n\n\n\nAvailable constraints\n\n\n\n\nmax_norm(max_value=2, axis=0)\n: maximum-norm constraint\n\n\nnon_neg()\n: non-negativity constraint\n\n\nunit_norm(axis=0)\n: unit-norm constraint\n\n\nmin_max_norm(min_value=0.0, max_value=1.0, rate=1.0, axis=0)\n:  minimum/maximum-norm constraint",
            "title": "Constraints"
        },
        {
            "location": "/constraints/#usage-of-constraints",
            "text": "Functions from the  constraints  module allow setting constraints (eg. non-negativity) on network parameters during optimization.  The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers  Dense ,  Conv1D ,  Conv2D  and  Conv3D  have a unified API.  These layers expose 2 keyword arguments:   kernel_constraint  for the main weights matrix  bias_constraint  for the bias.   from keras.constraints import max_norm\nmodel.add(Dense(64, kernel_constraint=max_norm(2.)))",
            "title": "Usage of constraints"
        },
        {
            "location": "/constraints/#available-constraints",
            "text": "max_norm(max_value=2, axis=0) : maximum-norm constraint  non_neg() : non-negativity constraint  unit_norm(axis=0) : unit-norm constraint  min_max_norm(min_value=0.0, max_value=1.0, rate=1.0, axis=0) :  minimum/maximum-norm constraint",
            "title": "Available constraints"
        },
        {
            "location": "/visualization/",
            "text": "Model visualization\n\n\nThe \nkeras.utils.vis_utils\n module provides utility functions to plot\na Keras model (using \ngraphviz\n).\n\n\nThis will plot a graph of the model and save it to a file:\n\n\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')\n\n\n\n\nplot_model\n takes two optional arguments:\n\n\n\n\nshow_shapes\n (defaults to False) controls whether output shapes are shown in the graph.\n\n\nshow_layer_names\n (defaults to True) controls whether layer names are shown in the graph.\n\n\n\n\nYou can also directly obtain the \npydot.Graph\n object and render it yourself,\nfor example to show it in an ipython notebook :\n\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))",
            "title": "Visualization"
        },
        {
            "location": "/visualization/#model-visualization",
            "text": "The  keras.utils.vis_utils  module provides utility functions to plot\na Keras model (using  graphviz ).  This will plot a graph of the model and save it to a file:  from keras.utils import plot_model\nplot_model(model, to_file='model.png')  plot_model  takes two optional arguments:   show_shapes  (defaults to False) controls whether output shapes are shown in the graph.  show_layer_names  (defaults to True) controls whether layer names are shown in the graph.   You can also directly obtain the  pydot.Graph  object and render it yourself,\nfor example to show it in an ipython notebook :  from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))",
            "title": "Model visualization"
        },
        {
            "location": "/scikit-learn-api/",
            "text": "Wrappers for the Scikit-Learn API\n\n\nYou can use \nSequential\n Keras models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at \nkeras.wrappers.scikit_learn.py\n.\n\n\nThere are two wrappers available:\n\n\nkeras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)\n, which implements the Scikit-Learn classifier interface,\n\n\nkeras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params)\n, which implements the Scikit-Learn regressor interface.\n\n\nArguments\n\n\n\n\nbuild_fn\n: callable function or class instance\n\n\nsk_params\n: model parameters & fitting parameters\n\n\n\n\nbuild_fn\n should construct, compile and return a Keras model, which\nwill then be used to fit/predict. One of the following\nthree values could be passed to \nbuild_fn\n:\n\n\n\n\nA function\n\n\nAn instance of a class that implements the \n__call__\n method\n\n\nNone. This means you implement a class that inherits from either\n\nKerasClassifier\n or \nKerasRegressor\n. The \n__call__\n method of the\npresent class will then be treated as the default \nbuild_fn\n.\n\n\n\n\nsk_params\n takes both model parameters and fitting parameters. Legal model\nparameters are the arguments of \nbuild_fn\n. Note that like all other\nestimators in scikit-learn, \nbuild_fn\n should provide default values for\nits arguments, so that you could create the estimator without passing any\nvalues to \nsk_params\n.\n\n\nsk_params\n could also accept parameters for calling \nfit\n, \npredict\n,\n\npredict_proba\n, and \nscore\n methods (e.g., \nepochs\n, \nbatch_size\n).\nfitting (predicting) parameters are selected in the following order:\n\n\n\n\nValues passed to the dictionary arguments of\n\nfit\n, \npredict\n, \npredict_proba\n, and \nscore\n methods\n\n\nValues passed to \nsk_params\n\n\nThe default values of the \nkeras.models.Sequential\n\n\nfit\n, \npredict\n, \npredict_proba\n and \nscore\n methods\n\n\n\n\nWhen using scikit-learn's \ngrid_search\n API, legal tunable parameters are\nthose you could pass to \nsk_params\n, including fitting parameters.\nIn other words, you could use \ngrid_search\n to search for the best\n\nbatch_size\n or \nepochs\n as well as the model parameters.",
            "title": "Scikit-learn API"
        },
        {
            "location": "/scikit-learn-api/#wrappers-for-the-scikit-learn-api",
            "text": "You can use  Sequential  Keras models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at  keras.wrappers.scikit_learn.py .  There are two wrappers available:  keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface,  keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface.",
            "title": "Wrappers for the Scikit-Learn API"
        },
        {
            "location": "/scikit-learn-api/#arguments",
            "text": "build_fn : callable function or class instance  sk_params : model parameters & fitting parameters   build_fn  should construct, compile and return a Keras model, which\nwill then be used to fit/predict. One of the following\nthree values could be passed to  build_fn :   A function  An instance of a class that implements the  __call__  method  None. This means you implement a class that inherits from either KerasClassifier  or  KerasRegressor . The  __call__  method of the\npresent class will then be treated as the default  build_fn .   sk_params  takes both model parameters and fitting parameters. Legal model\nparameters are the arguments of  build_fn . Note that like all other\nestimators in scikit-learn,  build_fn  should provide default values for\nits arguments, so that you could create the estimator without passing any\nvalues to  sk_params .  sk_params  could also accept parameters for calling  fit ,  predict , predict_proba , and  score  methods (e.g.,  epochs ,  batch_size ).\nfitting (predicting) parameters are selected in the following order:   Values passed to the dictionary arguments of fit ,  predict ,  predict_proba , and  score  methods  Values passed to  sk_params  The default values of the  keras.models.Sequential  fit ,  predict ,  predict_proba  and  score  methods   When using scikit-learn's  grid_search  API, legal tunable parameters are\nthose you could pass to  sk_params , including fitting parameters.\nIn other words, you could use  grid_search  to search for the best batch_size  or  epochs  as well as the model parameters.",
            "title": "Arguments"
        },
        {
            "location": "/utils/",
            "text": "[source]\n\n\nCustomObjectScope\n\n\nkeras.utils.CustomObjectScope()\n\n\n\n\nProvides a scope that changes to \n_GLOBAL_CUSTOM_OBJECTS\n cannot escape.\n\n\nCode within a \nwith\n statement will be able to access custom objects\nby name. Changes to global custom objects persist\nwithin the enclosing \nwith\n statement. At end of the \nwith\n statement,\nglobal custom objects are reverted to state\nat beginning of the \nwith\n statement.\n\n\nExample\n\n\nConsider a custom object \nMyObject\n (e.g. a class):\n\n\nwith CustomObjectScope({'MyObject':MyObject}):\n    layer = Dense(..., kernel_regularizer='MyObject')\n    # save, load, etc. will recognize custom object by name\n\n\n\n\n\n\n[source]\n\n\nHDF5Matrix\n\n\nkeras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)\n\n\n\n\nRepresentation of HDF5 dataset to be used instead of a Numpy array.\n\n\nExample\n\n\nx_data = HDF5Matrix('input/file.hdf5', 'data')\nmodel.predict(x_data)\n\n\n\n\nProviding \nstart\n and \nend\n allows use of a slice of the dataset.\n\n\nOptionally, a normalizer function (or lambda) can be given. This will\nbe called on every slice of data retrieved.\n\n\nArguments\n\n\n\n\ndatapath\n: string, path to a HDF5 file\n\n\ndataset\n: string, name of the HDF5 dataset in the file specified\nin datapath\n\n\nstart\n: int, start of desired slice of the specified dataset\n\n\nend\n: int, end of desired slice of the specified dataset\n\n\nnormalizer\n: function to be called on data when retrieved\n\n\n\n\nReturns\n\n\nAn array-like HDF5 dataset.\n\n\n\n\n[source]\n\n\nSequence\n\n\nkeras.utils.Sequence()\n\n\n\n\nBase object for fitting to a sequence of data, such as a dataset.\n\n\nEvery \nSequence\n must implements the \n__getitem__\n and the \n__len__\n methods.\nIf you want to modify your dataset between epochs you may implement \non_epoch_end\n.\nThe method \n__getitem__\n should return a complete batch.\n\n\nNotes\n\n\nSequence\n are a safer way to do multiprocessing. This structure guarantees that the network will only train once\non each sample per epoch which is not the case with generators.\n\n\nExamples\n\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\n\n# Here, `x_set` is list of path to the images\n# and `y_set` are the associated classes.\n\nclass CIFAR10Sequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return np.ceil(len(self.x) / float(self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.array([\n            resize(imread(file_name), (200, 200))\n               for file_name in batch_x]), np.array(batch_y)\n\n\n\n\n\n\nto_categorical\n\n\nkeras.utils.to_categorical(y, num_classes=None)\n\n\n\n\nConverts a class vector (integers) to binary class matrix.\n\n\nE.g. for use with categorical_crossentropy.\n\n\nArguments\n\n\n\n\ny\n: class vector to be converted into a matrix\n(integers from 0 to num_classes).\n\n\nnum_classes\n: total number of classes.\n\n\n\n\nReturns\n\n\nA binary matrix representation of the input.\n\n\n\n\nnormalize\n\n\nkeras.utils.normalize(x, axis=-1, order=2)\n\n\n\n\nNormalizes a Numpy array.\n\n\nArguments\n\n\n\n\nx\n: Numpy array to normalize.\n\n\naxis\n: axis along which to normalize.\n\n\norder\n: Normalization order (e.g. 2 for L2 norm).\n\n\n\n\nReturns\n\n\nA normalized copy of the array.\n\n\n\n\nget_file\n\n\nkeras.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None)\n\n\n\n\nDownloads a file from a URL if it not already in the cache.\n\n\nBy default the file at the url \norigin\n is downloaded to the\ncache_dir \n~/.keras\n, placed in the cache_subdir \ndatasets\n,\nand given the filename \nfname\n. The final location of a file\n\nexample.txt\n would therefore be \n~/.keras/datasets/example.txt\n.\n\n\nFiles in tar, tar.gz, tar.bz, and zip formats can also be extracted.\nPassing a hash will verify the file after download. The command line\nprograms \nshasum\n and \nsha256sum\n can compute the hash.\n\n\nArguments\n\n\n\n\nfname\n: Name of the file. If an absolute path \n/path/to/file.txt\n is\nspecified the file will be saved at that location.\n\n\norigin\n: Original URL of the file.\n\n\nuntar\n: Deprecated in favor of 'extract'.\nboolean, whether the file should be decompressed\n\n\nmd5_hash\n: Deprecated in favor of 'file_hash'.\nmd5 hash of the file for verification\n\n\nfile_hash\n: The expected hash string of the file after download.\nThe sha256 and md5 hash algorithms are both supported.\n\n\ncache_subdir\n: Subdirectory under the Keras cache dir where the file is\nsaved. If an absolute path \n/path/to/folder\n is\nspecified the file will be saved at that location.\n\n\nhash_algorithm\n: Select the hash algorithm to verify the file.\noptions are 'md5', 'sha256', and 'auto'.\nThe default 'auto' detects the hash algorithm in use.\n\n\nextract\n: True tries extracting the file as an Archive, like tar or zip.\n\n\narchive_format\n: Archive format to try for extracting the file.\nOptions are 'auto', 'tar', 'zip', and None.\n'tar' includes tar, tar.gz, and tar.bz files.\nThe default 'auto' is ['tar', 'zip'].\nNone or an empty list will return no matches found.\n\n\ncache_dir\n: Location to store cached files, when None it\ndefaults to the \nKeras Directory\n.\n\n\n\n\nReturns\n\n\nPath to the downloaded file\n\n\n\n\nprint_summary\n\n\nkeras.utils.print_summary(model, line_length=None, positions=None, print_fn=None)\n\n\n\n\nPrints a summary of a model.\n\n\nArguments\n\n\n\n\nmodel\n: Keras model instance.\n\n\nline_length\n: Total length of printed lines\n(e.g. set this to adapt the display to different\nterminal window sizes).\n\n\npositions\n: Relative or absolute positions of log elements in each line.\nIf not provided, defaults to \n[.33, .55, .67, 1.]\n.\n\n\nprint_fn\n: Print function to use.\nIt will be called on each line of the summary.\nYou can set it to a custom function\nin order to capture the string summary.\nIt defaults to \nprint\n (prints to stdout).\n\n\n\n\n\n\nplot_model\n\n\nkeras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')\n\n\n\n\nConverts a Keras model to dot format and save to a file.\n\n\nArguments\n\n\n\n\nmodel\n: A Keras model instance\n\n\nto_file\n: File name of the plot image.\n\n\nshow_shapes\n: whether to display shape information.\n\n\nshow_layer_names\n: whether to display layer names.\n\n\nrankdir\n: \nrankdir\n argument passed to PyDot,\na string specifying the format of the plot:\n'TB' creates a vertical plot;\n'LR' creates a horizontal plot.\n\n\n\n\n\n\nmulti_gpu_model\n\n\nkeras.utils.multi_gpu_model(model, gpus=None)\n\n\n\n\nReplicates a model on different GPUs.\n\n\nSpecifically, this function implements single-machine\nmulti-GPU data parallelism. It works in the following way:\n\n\n\n\nDivide the model's input(s) into multiple sub-batches.\n\n\nApply a model copy on each sub-batch. Every model copy\nis executed on a dedicated GPU.\n\n\nConcatenate the results (on CPU) into one big batch.\n\n\n\n\nE.g. if your \nbatch_size\n is 64 and you use \ngpus=2\n,\nthen we will divide the input into 2 sub-batches of 32 samples,\nprocess each sub-batch on one GPU, then return the full\nbatch of 64 processed samples.\n\n\nThis induces quasi-linear speedup on up to 8 GPUs.\n\n\nThis function is only available with the TensorFlow backend\nfor the time being.\n\n\nArguments\n\n\n\n\nmodel\n: A Keras model instance. To avoid OOM errors,\nthis model could have been built on CPU, for instance\n(see usage example below).\n\n\ngpus\n: Integer >= 2 or list of integers, number of GPUs or\nlist of GPU IDs on which to create model replicas.\n\n\n\n\nReturns\n\n\nA Keras \nModel\n instance which can be used just like the initial\n\nmodel\n argument, but which distributes its workload on multiple GPUs.\n\n\nExample\n\n\nimport tensorflow as tf\nfrom keras.applications import Xception\nfrom keras.utils import multi_gpu_model\nimport numpy as np\n\nnum_samples = 1000\nheight = 224\nwidth = 224\nnum_classes = 1000\n\n# Instantiate the base model (or \"template\" model).\n# We recommend doing this with under a CPU device scope,\n# so that the model's weights are hosted on CPU memory.\n# Otherwise they may end up hosted on a GPU, which would\n# complicate weight sharing.\nwith tf.device('/cpu:0'):\n    model = Xception(weights=None,\n                     input_shape=(height, width, 3),\n                     classes=num_classes)\n\n# Replicates the model on 8 GPUs.\n# This assumes that your machine has 8 available GPUs.\nparallel_model = multi_gpu_model(model, gpus=8)\nparallel_model.compile(loss='categorical_crossentropy',\n                       optimizer='rmsprop')\n\n# Generate dummy data.\nx = np.random.random((num_samples, height, width, 3))\ny = np.random.random((num_samples, num_classes))\n\n# This `fit` call will be distributed on 8 GPUs.\n# Since the batch size is 256, each GPU will process 32 samples.\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n\n# Save model via the template model (which shares the same weights):\nmodel.save('my_model.h5')\n\n\n\n\nOn model saving\n\n\nTo save the multi-gpu model, use \n.save(fname)\n or \n.save_weights(fname)\n\nwith the template model (the argument you passed to \nmulti_gpu_model\n),\nrather than the model returned by \nmulti_gpu_model\n.",
            "title": "Utils"
        },
        {
            "location": "/utils/#customobjectscope",
            "text": "keras.utils.CustomObjectScope()  Provides a scope that changes to  _GLOBAL_CUSTOM_OBJECTS  cannot escape.  Code within a  with  statement will be able to access custom objects\nby name. Changes to global custom objects persist\nwithin the enclosing  with  statement. At end of the  with  statement,\nglobal custom objects are reverted to state\nat beginning of the  with  statement.  Example  Consider a custom object  MyObject  (e.g. a class):  with CustomObjectScope({'MyObject':MyObject}):\n    layer = Dense(..., kernel_regularizer='MyObject')\n    # save, load, etc. will recognize custom object by name   [source]",
            "title": "CustomObjectScope"
        },
        {
            "location": "/utils/#hdf5matrix",
            "text": "keras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)  Representation of HDF5 dataset to be used instead of a Numpy array.  Example  x_data = HDF5Matrix('input/file.hdf5', 'data')\nmodel.predict(x_data)  Providing  start  and  end  allows use of a slice of the dataset.  Optionally, a normalizer function (or lambda) can be given. This will\nbe called on every slice of data retrieved.  Arguments   datapath : string, path to a HDF5 file  dataset : string, name of the HDF5 dataset in the file specified\nin datapath  start : int, start of desired slice of the specified dataset  end : int, end of desired slice of the specified dataset  normalizer : function to be called on data when retrieved   Returns  An array-like HDF5 dataset.   [source]",
            "title": "HDF5Matrix"
        },
        {
            "location": "/utils/#sequence",
            "text": "keras.utils.Sequence()  Base object for fitting to a sequence of data, such as a dataset.  Every  Sequence  must implements the  __getitem__  and the  __len__  methods.\nIf you want to modify your dataset between epochs you may implement  on_epoch_end .\nThe method  __getitem__  should return a complete batch.  Notes  Sequence  are a safer way to do multiprocessing. This structure guarantees that the network will only train once\non each sample per epoch which is not the case with generators.  Examples  from skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\n\n# Here, `x_set` is list of path to the images\n# and `y_set` are the associated classes.\n\nclass CIFAR10Sequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return np.ceil(len(self.x) / float(self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.array([\n            resize(imread(file_name), (200, 200))\n               for file_name in batch_x]), np.array(batch_y)",
            "title": "Sequence"
        },
        {
            "location": "/utils/#to_categorical",
            "text": "keras.utils.to_categorical(y, num_classes=None)  Converts a class vector (integers) to binary class matrix.  E.g. for use with categorical_crossentropy.  Arguments   y : class vector to be converted into a matrix\n(integers from 0 to num_classes).  num_classes : total number of classes.   Returns  A binary matrix representation of the input.",
            "title": "to_categorical"
        },
        {
            "location": "/utils/#normalize",
            "text": "keras.utils.normalize(x, axis=-1, order=2)  Normalizes a Numpy array.  Arguments   x : Numpy array to normalize.  axis : axis along which to normalize.  order : Normalization order (e.g. 2 for L2 norm).   Returns  A normalized copy of the array.",
            "title": "normalize"
        },
        {
            "location": "/utils/#get_file",
            "text": "keras.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None)  Downloads a file from a URL if it not already in the cache.  By default the file at the url  origin  is downloaded to the\ncache_dir  ~/.keras , placed in the cache_subdir  datasets ,\nand given the filename  fname . The final location of a file example.txt  would therefore be  ~/.keras/datasets/example.txt .  Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.\nPassing a hash will verify the file after download. The command line\nprograms  shasum  and  sha256sum  can compute the hash.  Arguments   fname : Name of the file. If an absolute path  /path/to/file.txt  is\nspecified the file will be saved at that location.  origin : Original URL of the file.  untar : Deprecated in favor of 'extract'.\nboolean, whether the file should be decompressed  md5_hash : Deprecated in favor of 'file_hash'.\nmd5 hash of the file for verification  file_hash : The expected hash string of the file after download.\nThe sha256 and md5 hash algorithms are both supported.  cache_subdir : Subdirectory under the Keras cache dir where the file is\nsaved. If an absolute path  /path/to/folder  is\nspecified the file will be saved at that location.  hash_algorithm : Select the hash algorithm to verify the file.\noptions are 'md5', 'sha256', and 'auto'.\nThe default 'auto' detects the hash algorithm in use.  extract : True tries extracting the file as an Archive, like tar or zip.  archive_format : Archive format to try for extracting the file.\nOptions are 'auto', 'tar', 'zip', and None.\n'tar' includes tar, tar.gz, and tar.bz files.\nThe default 'auto' is ['tar', 'zip'].\nNone or an empty list will return no matches found.  cache_dir : Location to store cached files, when None it\ndefaults to the  Keras Directory .   Returns  Path to the downloaded file",
            "title": "get_file"
        },
        {
            "location": "/utils/#print_summary",
            "text": "keras.utils.print_summary(model, line_length=None, positions=None, print_fn=None)  Prints a summary of a model.  Arguments   model : Keras model instance.  line_length : Total length of printed lines\n(e.g. set this to adapt the display to different\nterminal window sizes).  positions : Relative or absolute positions of log elements in each line.\nIf not provided, defaults to  [.33, .55, .67, 1.] .  print_fn : Print function to use.\nIt will be called on each line of the summary.\nYou can set it to a custom function\nin order to capture the string summary.\nIt defaults to  print  (prints to stdout).",
            "title": "print_summary"
        },
        {
            "location": "/utils/#plot_model",
            "text": "keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')  Converts a Keras model to dot format and save to a file.  Arguments   model : A Keras model instance  to_file : File name of the plot image.  show_shapes : whether to display shape information.  show_layer_names : whether to display layer names.  rankdir :  rankdir  argument passed to PyDot,\na string specifying the format of the plot:\n'TB' creates a vertical plot;\n'LR' creates a horizontal plot.",
            "title": "plot_model"
        },
        {
            "location": "/utils/#multi_gpu_model",
            "text": "keras.utils.multi_gpu_model(model, gpus=None)  Replicates a model on different GPUs.  Specifically, this function implements single-machine\nmulti-GPU data parallelism. It works in the following way:   Divide the model's input(s) into multiple sub-batches.  Apply a model copy on each sub-batch. Every model copy\nis executed on a dedicated GPU.  Concatenate the results (on CPU) into one big batch.   E.g. if your  batch_size  is 64 and you use  gpus=2 ,\nthen we will divide the input into 2 sub-batches of 32 samples,\nprocess each sub-batch on one GPU, then return the full\nbatch of 64 processed samples.  This induces quasi-linear speedup on up to 8 GPUs.  This function is only available with the TensorFlow backend\nfor the time being.  Arguments   model : A Keras model instance. To avoid OOM errors,\nthis model could have been built on CPU, for instance\n(see usage example below).  gpus : Integer >= 2 or list of integers, number of GPUs or\nlist of GPU IDs on which to create model replicas.   Returns  A Keras  Model  instance which can be used just like the initial model  argument, but which distributes its workload on multiple GPUs.  Example  import tensorflow as tf\nfrom keras.applications import Xception\nfrom keras.utils import multi_gpu_model\nimport numpy as np\n\nnum_samples = 1000\nheight = 224\nwidth = 224\nnum_classes = 1000\n\n# Instantiate the base model (or \"template\" model).\n# We recommend doing this with under a CPU device scope,\n# so that the model's weights are hosted on CPU memory.\n# Otherwise they may end up hosted on a GPU, which would\n# complicate weight sharing.\nwith tf.device('/cpu:0'):\n    model = Xception(weights=None,\n                     input_shape=(height, width, 3),\n                     classes=num_classes)\n\n# Replicates the model on 8 GPUs.\n# This assumes that your machine has 8 available GPUs.\nparallel_model = multi_gpu_model(model, gpus=8)\nparallel_model.compile(loss='categorical_crossentropy',\n                       optimizer='rmsprop')\n\n# Generate dummy data.\nx = np.random.random((num_samples, height, width, 3))\ny = np.random.random((num_samples, num_classes))\n\n# This `fit` call will be distributed on 8 GPUs.\n# Since the batch size is 256, each GPU will process 32 samples.\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n\n# Save model via the template model (which shares the same weights):\nmodel.save('my_model.h5')  On model saving  To save the multi-gpu model, use  .save(fname)  or  .save_weights(fname) \nwith the template model (the argument you passed to  multi_gpu_model ),\nrather than the model returned by  multi_gpu_model .",
            "title": "multi_gpu_model"
        },
        {
            "location": "/contributing/",
            "text": "On Github Issues and Pull Requests\n\n\nFound a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.\n\n\nBug reporting\n\n\nYour code doesn't work, and you have determined that the issue lies with Keras? Follow these steps to report a bug.\n\n\n\n\n\n\nYour bug may already be fixed. Make sure to update to the current Keras master branch, as well as the latest Theano/TensorFlow/CNTK master branch.\nTo easily update Theano: \npip install git+git://github.com/Theano/Theano.git --upgrade\n\n\n\n\n\n\nSearch for similar issues. Make sure to delete \nis:open\n on the issue search to find solved tickets as well. It's possible somebody has encountered this bug already. Also remember to check out Keras' \nFAQ\n. Still having a problem? Open an issue on Github to let us know.\n\n\n\n\n\n\nMake sure you provide us with useful information about your configuration: what OS are you using? What Keras backend are you using? Are you running on GPU? If so, what is your version of Cuda, of cuDNN? What is your GPU?\n\n\n\n\n\n\nProvide us with a script to reproduce the issue. This script should be runnable as-is and should not require external data download (use randomly generated data if you need to run a model on some test data). We recommend that you use Github Gists to post your code. Any issue that cannot be reproduced is likely to be closed.\n\n\n\n\n\n\nIf possible, take a stab at fixing the bug yourself --if you can!\n\n\n\n\n\n\nThe more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial.\n\n\n\n\nRequesting a Feature\n\n\nYou can also use Github issues to request features you would like to see in Keras, or changes in the Keras API.\n\n\n\n\n\n\nProvide a clear and detailed explanation of the feature you want and why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library for Keras. It is crucial for Keras to avoid bloating the API and codebase.\n\n\n\n\n\n\nProvide code snippets demonstrating the API you have in mind and illustrating the use cases of your feature. Of course, you don't need to write any real code at this point!\n\n\n\n\n\n\nAfter discussing the feature you may choose to attempt a Pull Request. If you're at all able, start writing some code. We always have more work to do than time to do it. If you can write some code then that will speed the process along.\n\n\n\n\n\n\n\n\nRequests for Contributions\n\n\nThis is the board\n where we list current outstanding issues and features to be added. If you want to start contributing to Keras, this is the place to start.\n\n\n\n\nPull Requests\n\n\nWhere should I submit my pull request?\n\n\n\n\nKeras improvements and bugfixes\n go to the \nKeras \nmaster\n branch\n.\n\n\nExperimental new features\n such as layers and datasets go to \nkeras-contrib\n. Unless it is a new feature listed in \nRequests for Contributions\n, in which case it belongs in core Keras. If you think your feature belongs in core Keras, you can submit a design doc to explain your feature and argue for it (see explanations below).\n\n\n\n\nPlease note that PRs that are primarily about \ncode style\n (as opposed to fixing bugs, improving docs, or adding new functionality) will likely be rejected.\n\n\nHere's a quick guide to submitting your improvements:\n\n\n\n\n\n\nIf your PR introduces a change in functionality, make sure you start by writing a design doc and sending it to the Keras mailing list to discuss whether the change should be made, and how to handle it. This will save you from having your PR closed down the road! Of course, if your PR is a simple bug fix, you don't need to do that. The process for writing and submitting design docs is as follow:\n\n\n\n\nStart from \nthis Google Doc template\n, and copy it to new Google doc.\n\n\nFill in the content. Note that you will need to insert code examples. To insert code, use a Google Doc extension such as \nCodePretty\n (there are several such extensions available).\n\n\nSet sharing settings to \"everyone with the link is allowed to comment\"\n\n\nSend the document to \nkeras-users@googlegroups.com\n with a subject that starts with \n[API DESIGN REVIEW]\n (all caps) so that we notice it.\n\n\nWait for comments, and answer them as they come. Edit the proposal as necessary.\n\n\nThe proposal will finally be approved or rejected. Once approved, you can send out Pull Requests or ask others to write Pull Requests.\n\n\n\n\n\n\n\n\nWrite the code (or get others to write it). This is the hard part!\n\n\n\n\n\n\nMake sure any new function or class you introduce has proper docstrings. Make sure any code you touch still has up-to-date docstrings and documentation. \nDocstring style should be respected.\n In particular, they should be formatted in MarkDown, and there should be sections for \nArguments\n, \nReturns\n, \nRaises\n (if applicable). Look at other docstrings in the codebase for examples.\n\n\n\n\n\n\nWrite tests. Your code should have full unit test coverage. If you want to see your PR merged promptly, this is crucial.\n\n\n\n\n\n\nRun our test suite locally. It's easy: from the Keras folder, simply run: \npy.test tests/\n.\n\n\n\n\nYou will need to install the test requirements as well: \npip install -e .[tests]\n.\n\n\n\n\n\n\n\n\nMake sure all tests are passing:\n\n\n\n\nwith the Theano backend, on Python 2.7 and Python 3.6. Make sure you have the development version of Theano.\n\n\nwith the TensorFlow backend, on Python 2.7 and Python 3.6. Make sure you have the development version of TensorFlow.\n\n\nwith the CNTK backend, on Python 2.7 and Python 3.6. Make sure you have the development version of CNTK.\n\n\n\n\n\n\n\n\nWe use PEP8 syntax conventions, but we aren't dogmatic when it comes to line length. Make sure your lines stay reasonably sized, though. To make your life easier, we recommend running a PEP8 linter:\n\n\n\n\nInstall PEP8 packages: \npip install pep8 pytest-pep8 autopep8\n\n\nRun a standalone PEP8 check: \npy.test --pep8 -m pep8\n\n\nYou can automatically fix some PEP8 error by running: \nautopep8 -i --select <errors> <FILENAME>\n for example: \nautopep8 -i --select E128 tests/keras/backend/test_backends.py\n\n\n\n\n\n\n\n\nWhen committing, use appropriate, descriptive commit messages.\n\n\n\n\n\n\nUpdate the documentation. If introducing new functionality, make sure you include code snippets demonstrating the usage of your new feature.\n\n\n\n\n\n\nSubmit your PR. If your changes have been approved in a previous discussion, and if you have complete (and passing) unit tests as well as proper docstrings/documentation, your PR is likely to be merged promptly.\n\n\n\n\n\n\n\n\nAdding new examples\n\n\nEven if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples. \nExisting examples\n show idiomatic Keras code: make sure to keep your own script in the same spirit.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#on-github-issues-and-pull-requests",
            "text": "Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.",
            "title": "On Github Issues and Pull Requests"
        },
        {
            "location": "/contributing/#bug-reporting",
            "text": "Your code doesn't work, and you have determined that the issue lies with Keras? Follow these steps to report a bug.    Your bug may already be fixed. Make sure to update to the current Keras master branch, as well as the latest Theano/TensorFlow/CNTK master branch.\nTo easily update Theano:  pip install git+git://github.com/Theano/Theano.git --upgrade    Search for similar issues. Make sure to delete  is:open  on the issue search to find solved tickets as well. It's possible somebody has encountered this bug already. Also remember to check out Keras'  FAQ . Still having a problem? Open an issue on Github to let us know.    Make sure you provide us with useful information about your configuration: what OS are you using? What Keras backend are you using? Are you running on GPU? If so, what is your version of Cuda, of cuDNN? What is your GPU?    Provide us with a script to reproduce the issue. This script should be runnable as-is and should not require external data download (use randomly generated data if you need to run a model on some test data). We recommend that you use Github Gists to post your code. Any issue that cannot be reproduced is likely to be closed.    If possible, take a stab at fixing the bug yourself --if you can!    The more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial.",
            "title": "Bug reporting"
        },
        {
            "location": "/contributing/#requesting-a-feature",
            "text": "You can also use Github issues to request features you would like to see in Keras, or changes in the Keras API.    Provide a clear and detailed explanation of the feature you want and why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library for Keras. It is crucial for Keras to avoid bloating the API and codebase.    Provide code snippets demonstrating the API you have in mind and illustrating the use cases of your feature. Of course, you don't need to write any real code at this point!    After discussing the feature you may choose to attempt a Pull Request. If you're at all able, start writing some code. We always have more work to do than time to do it. If you can write some code then that will speed the process along.",
            "title": "Requesting a Feature"
        },
        {
            "location": "/contributing/#requests-for-contributions",
            "text": "This is the board  where we list current outstanding issues and features to be added. If you want to start contributing to Keras, this is the place to start.",
            "title": "Requests for Contributions"
        },
        {
            "location": "/contributing/#pull-requests",
            "text": "Where should I submit my pull request?   Keras improvements and bugfixes  go to the  Keras  master  branch .  Experimental new features  such as layers and datasets go to  keras-contrib . Unless it is a new feature listed in  Requests for Contributions , in which case it belongs in core Keras. If you think your feature belongs in core Keras, you can submit a design doc to explain your feature and argue for it (see explanations below).   Please note that PRs that are primarily about  code style  (as opposed to fixing bugs, improving docs, or adding new functionality) will likely be rejected.  Here's a quick guide to submitting your improvements:    If your PR introduces a change in functionality, make sure you start by writing a design doc and sending it to the Keras mailing list to discuss whether the change should be made, and how to handle it. This will save you from having your PR closed down the road! Of course, if your PR is a simple bug fix, you don't need to do that. The process for writing and submitting design docs is as follow:   Start from  this Google Doc template , and copy it to new Google doc.  Fill in the content. Note that you will need to insert code examples. To insert code, use a Google Doc extension such as  CodePretty  (there are several such extensions available).  Set sharing settings to \"everyone with the link is allowed to comment\"  Send the document to  keras-users@googlegroups.com  with a subject that starts with  [API DESIGN REVIEW]  (all caps) so that we notice it.  Wait for comments, and answer them as they come. Edit the proposal as necessary.  The proposal will finally be approved or rejected. Once approved, you can send out Pull Requests or ask others to write Pull Requests.     Write the code (or get others to write it). This is the hard part!    Make sure any new function or class you introduce has proper docstrings. Make sure any code you touch still has up-to-date docstrings and documentation.  Docstring style should be respected.  In particular, they should be formatted in MarkDown, and there should be sections for  Arguments ,  Returns ,  Raises  (if applicable). Look at other docstrings in the codebase for examples.    Write tests. Your code should have full unit test coverage. If you want to see your PR merged promptly, this is crucial.    Run our test suite locally. It's easy: from the Keras folder, simply run:  py.test tests/ .   You will need to install the test requirements as well:  pip install -e .[tests] .     Make sure all tests are passing:   with the Theano backend, on Python 2.7 and Python 3.6. Make sure you have the development version of Theano.  with the TensorFlow backend, on Python 2.7 and Python 3.6. Make sure you have the development version of TensorFlow.  with the CNTK backend, on Python 2.7 and Python 3.6. Make sure you have the development version of CNTK.     We use PEP8 syntax conventions, but we aren't dogmatic when it comes to line length. Make sure your lines stay reasonably sized, though. To make your life easier, we recommend running a PEP8 linter:   Install PEP8 packages:  pip install pep8 pytest-pep8 autopep8  Run a standalone PEP8 check:  py.test --pep8 -m pep8  You can automatically fix some PEP8 error by running:  autopep8 -i --select <errors> <FILENAME>  for example:  autopep8 -i --select E128 tests/keras/backend/test_backends.py     When committing, use appropriate, descriptive commit messages.    Update the documentation. If introducing new functionality, make sure you include code snippets demonstrating the usage of your new feature.    Submit your PR. If your changes have been approved in a previous discussion, and if you have complete (and passing) unit tests as well as proper docstrings/documentation, your PR is likely to be merged promptly.",
            "title": "Pull Requests"
        },
        {
            "location": "/contributing/#adding-new-examples",
            "text": "Even if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples.  Existing examples  show idiomatic Keras code: make sure to keep your own script in the same spirit.",
            "title": "Adding new examples"
        }
    ]
}